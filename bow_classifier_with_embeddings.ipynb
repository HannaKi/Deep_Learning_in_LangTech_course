{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Bag-of-words classifier with pretrained word embeddings\n",
    "\n",
    "- If we have a trained word embeddings model, we can transfer that knowledge into a new task and model\n",
    "- Initialize the weights in the classifier with pretrained word embeddings\n",
    "- Here we will use embeddings trained with word2vec on news data: GoogleNews-vectors-negative300.bin from https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': 'pos', 'text': 'Her Deadly Rival (1995): Starring Harry Hamlin, Annie Potts, Lisa Zane, Tommy Hinkley, Susan Diol, Roma Maffia, Robert C. Treveiler, D. L. Anderson, William Blair, Sean Bridges, Robin Dallenbach, Wilbur Fitzgerald, Dale Frye, Stan Kelly, Deborah Hobart, David Lenthall, Lorri Lindberg, Chuck Kinlaw, Amy Parrish, Melissa Suzanne McBride, Ralph Wilcox, Al Wiggins, Jeff Sumerel, Daria Sanford....Director James Hayman, Screenplay Dan Vining.  Actor Harry Hamlin (of LA Law fame, Clash of The Titans and other films) seems perfectly cast in this \\\\Lifetime\\\\\" type film directed by James Hayman and released in 1995. He and his wife Lisa Rinna would later work on a film about sex addiction. \\\\\"Her Deadly Rival\\\\\" is, at first glance, similar to the better known Hollywood box-office hit \\\\\"Fatal Attraction\\\\\". In \\\\\"Rival\\\\\", happily married couple Jim and Kris Lanford move into a new home in the typically beautiful suburbs. They have the seemingly perfect marriage- they are deeply in love, despite a routine lifestyle. But then a mysterious admirer sets her eyes on Jim. Her identity is never revealed, despite an attempt by Jim and even investigators to discover who she is. She constantly harasses Jim through phone calls and letters. His marriage nearly flounders as his wife begins to think he\\'s having an affair and trying to cover it up. While Harry Hamlin, Annie Potts and the rest of the cast - Lisa Zane, Tommy Hinkley, Susan Diol, Roma Maffia, Robert C. Treveiler, D. L. Anderson, William Blair- each seem to be straight out of a soap opera. But this is a very suspense-filled drama and has its good moments. There is a twist at the end. Spoiler Alert. All I have to say is \\\\\"her deadly rival\\\\\" was only herself. Based on a supposedly actual case, Jim\\'s wife Kris suffered from multiple personality disorder and that was what ruined her marriage. Even if the story is not terribly impressive, even if the acting is only a step above soap opera acting, this film has its moments. Especially moving are the intimate scenes between Jim and his wife and the final scene in which, when Jim learns the truth, he can\\'t believe what he has just heard. The movie is probably a little too long and boring in some parts but it\\'s the kind of TV movie that usually does well, especially on Lifetime, which continues to produce films of this kind, of the \\\\\"domestic thriller\\\\\" type, or seduction stories. Trashy but everyone likes trash.\"'}\n",
      "['Her Deadly Rival (1995): Starring Harry Hamlin, Annie Potts, Lisa Zane, Tommy Hinkley, Susan Diol, Roma Maffia, Robert C. Treveiler, D. L. Anderson, William Blair, Sean Bridges, Robin Dallenbach, Wilbur Fitzgerald, Dale Frye, Stan Kelly, Deborah Hobart, David Lenthall, Lorri Lindberg, Chuck Kinlaw, Amy Parrish, Melissa Suzanne McBride, Ralph Wilcox, Al Wiggins, Jeff Sumerel, Daria Sanford....Director James Hayman, Screenplay Dan Vining.  Actor Harry Hamlin (of LA Law fame, Clash of The Titans and other films) seems perfectly cast in this \\\\Lifetime\\\\\" type film directed by James Hayman and released in 1995. He and his wife Lisa Rinna would later work on a film about sex addiction. \\\\\"Her Deadly Rival\\\\\" is, at first glance, similar to the better known Hollywood box-office hit \\\\\"Fatal Attraction\\\\\". In \\\\\"Rival\\\\\", happily married couple Jim and Kris Lanford move into a new home in the typically beautiful suburbs. They have the seemingly perfect marriage- they are deeply in love, despite a routine lifestyle. But then a mysterious admirer sets her eyes on Jim. Her identity is never revealed, despite an attempt by Jim and even investigators to discover who she is. She constantly harasses Jim through phone calls and letters. His marriage nearly flounders as his wife begins to think he\\'s having an affair and trying to cover it up. While Harry Hamlin, Annie Potts and the rest of the cast - Lisa Zane, Tommy Hinkley, Susan Diol, Roma Maffia, Robert C. Treveiler, D. L. Anderson, William Blair- each seem to be straight out of a soap opera. But this is a very suspense-filled drama and has its good moments. There is a twist at the end. Spoiler Alert. All I have to say is \\\\\"her deadly rival\\\\\" was only herself. Based on a supposedly actual case, Jim\\'s wife Kris suffered from multiple personality disorder and that was what ruined her marriage. Even if the story is not terribly impressive, even if the acting is only a step above soap opera acting, this film has its moments. Especially moving are the intimate scenes between Jim and his wife and the final scene in which, when Jim learns the truth, he can\\'t believe what he has just heard. The movie is probably a little too long and boring in some parts but it\\'s the kind of TV movie that usually does well, especially on Lifetime, which continues to produce films of this kind, of the \\\\\"domestic thriller\\\\\" type, or seduction stories. Trashy but everyone likes trash.\"', \"DANIEL DAY-LEWIS does a remarkable job of playing Christy Brown, the artist who grew up with cerebral palsy but managed to have a productive life, dealing successfully with his handicap and becoming a respected artist and writer.  The film, however, is a very difficult one to review--or even watch. Fortunately, I had the caption feature on to catch every spoken word which would have been impossible if I saw the film in a theater. While I respect it as a brave piece of work dealing with difficult subject matter, I can't say it's the sort of film I'd want to view more than once.  Nevertheless, my attention was held by the story-telling device, a flashback framed by the present, in which we see Christy being honored for his achievements before we see the flashback to his youth and his struggles to communicate with those around him, who certainly gave him loving care.  DANIEL DAY-LEWIS certainly is remarkable as the troubled man who falls in love with a therapist (FIONA SHAW), much to his mother's fear that when the love is not reciprocated his heart will be broken. There's a painfully long scene in a restaurant where he confesses his love to her before others and then goes into a frenzied rage after drinking too much.  BRENDA FRICKER does a brilliant job as the mother taking care of him, his father and a brood of siblings while struggling to keep a roof over their heads until Day-Lewis begins to have success with his work. She complements Day-Lewis' performance as the warm-hearted mother and shares many poignant moments with him.  Richly detailed story of a family that stayed together under the most unusual of circumstances with attention to period detail in every frame of the film. Both Fricker and Day-Lewis won Oscars, but HUGH O'CONOR and RAY McANALLY are also excellent. O'Conor is Christy as a boy and McAnally is the father who spends too much time at the local pub but loves the boy.  Summing up: Elmer Bernstein's music is an added plus factor. Well worthwhile, but definitely not a film for everyone.\"]\n",
      "['pos', 'pos']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"data/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) \n",
    "print(data[0])\n",
    "\n",
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the data\n",
    "\n",
    "- When we use an embedding layer (keras.layers.Embedding) the input data must be a sequence, not a bag-of-words vector\n",
    "- You can use CountVectorizer only as an analyzer without building the feature matrix\n",
    "- We will then build the vectorizer part later ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Her', 'Deadly', 'Rival', '1995', 'Starring', 'Harry', 'Hamlin', 'Annie', 'Potts', 'Lisa', 'Zane', 'Tommy', 'Hinkley', 'Susan', 'Diol', 'Roma', 'Maffia', 'Robert', 'Treveiler', 'Anderson', 'William', 'Blair', 'Sean', 'Bridges', 'Robin', 'Dallenbach', 'Wilbur', 'Fitzgerald', 'Dale', 'Frye', 'Stan', 'Kelly', 'Deborah', 'Hobart', 'David', 'Lenthall', 'Lorri', 'Lindberg', 'Chuck', 'Kinlaw', 'Amy', 'Parrish', 'Melissa', 'Suzanne', 'McBride', 'Ralph', 'Wilcox', 'Al', 'Wiggins', 'Jeff', 'Sumerel', 'Daria', 'Sanford', 'Director', 'James', 'Hayman', 'Screenplay', 'Dan', 'Vining', 'Actor', 'Harry', 'Hamlin', 'of', 'LA', 'Law', 'fame', 'Clash', 'of', 'The', 'Titans', 'and', 'other', 'films', 'seems', 'perfectly', 'cast', 'in', 'this', 'Lifetime', 'type', 'film', 'directed', 'by', 'James', 'Hayman', 'and', 'released', 'in', '1995', 'He', 'and', 'his', 'wife', 'Lisa', 'Rinna', 'would', 'later', 'work', 'on', 'film', 'about', 'sex', 'addiction', 'Her', 'Deadly', 'Rival', 'is', 'at', 'first', 'glance', 'similar', 'to', 'the', 'better', 'known', 'Hollywood', 'box', 'office', 'hit', 'Fatal', 'Attraction', 'In', 'Rival', 'happily', 'married', 'couple', 'Jim', 'and', 'Kris', 'Lanford', 'move', 'into', 'new', 'home', 'in', 'the', 'typically', 'beautiful', 'suburbs', 'They', 'have', 'the', 'seemingly', 'perfect', 'marriage', 'they', 'are', 'deeply', 'in', 'love', 'despite', 'routine', 'lifestyle', 'But', 'then', 'mysterious', 'admirer', 'sets', 'her', 'eyes', 'on', 'Jim', 'Her', 'identity', 'is', 'never', 'revealed', 'despite', 'an', 'attempt', 'by', 'Jim', 'and', 'even', 'investigators', 'to', 'discover', 'who', 'she', 'is', 'She', 'constantly', 'harasses', 'Jim', 'through', 'phone', 'calls', 'and', 'letters', 'His', 'marriage', 'nearly', 'flounders', 'as', 'his', 'wife', 'begins', 'to', 'think', 'he', 'having', 'an', 'affair', 'and', 'trying', 'to', 'cover', 'it', 'up', 'While', 'Harry', 'Hamlin', 'Annie', 'Potts', 'and', 'the', 'rest', 'of', 'the', 'cast', 'Lisa', 'Zane', 'Tommy', 'Hinkley', 'Susan', 'Diol', 'Roma', 'Maffia', 'Robert', 'Treveiler', 'Anderson', 'William', 'Blair', 'each', 'seem', 'to', 'be', 'straight', 'out', 'of', 'soap', 'opera', 'But', 'this', 'is', 'very', 'suspense', 'filled', 'drama', 'and', 'has', 'its', 'good', 'moments', 'There', 'is', 'twist', 'at', 'the', 'end', 'Spoiler', 'Alert', 'All', 'have', 'to', 'say', 'is', 'her', 'deadly', 'rival', 'was', 'only', 'herself', 'Based', 'on', 'supposedly', 'actual', 'case', 'Jim', 'wife', 'Kris', 'suffered', 'from', 'multiple', 'personality', 'disorder', 'and', 'that', 'was', 'what', 'ruined', 'her', 'marriage', 'Even', 'if', 'the', 'story', 'is', 'not', 'terribly', 'impressive', 'even', 'if', 'the', 'acting', 'is', 'only', 'step', 'above', 'soap', 'opera', 'acting', 'this', 'film', 'has', 'its', 'moments', 'Especially', 'moving', 'are', 'the', 'intimate', 'scenes', 'between', 'Jim', 'and', 'his', 'wife', 'and', 'the', 'final', 'scene', 'in', 'which', 'when', 'Jim', 'learns', 'the', 'truth', 'he', 'can', 'believe', 'what', 'he', 'has', 'just', 'heard', 'The', 'movie', 'is', 'probably', 'little', 'too', 'long', 'and', 'boring', 'in', 'some', 'parts', 'but', 'it', 'the', 'kind', 'of', 'TV', 'movie', 'that', 'usually', 'does', 'well', 'especially', 'on', 'Lifetime', 'which', 'continues', 'to', 'produce', 'films', 'of', 'this', 'kind', 'of', 'the', 'domestic', 'thriller', 'type', 'or', 'seduction', 'stories', 'Trashy', 'but', 'everyone', 'likes', 'trash']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy\n",
    "analyzer=CountVectorizer(lowercase=False).build_analyzer() # includes tokenizer and preprocessing\n",
    "print(analyzer(texts[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use gensim to read the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 1000000\n",
      "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model=KeyedVectors.load_word2vec_format(\"data/wiki-news-300d-1M.vec\", binary=False, limit=1000000)\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words=[k for k,v in sorted(vector_model.vocab.items(), key=lambda x:x[1].index)]\n",
    "print(\"Words from embedding model:\",len(words))\n",
    "print(\"First 50 words:\",words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the vectors\n",
    "\n",
    "- Easier to learn on top of these vectors when the magnitude does not vary much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
      " -0.0063]\n",
      "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before normalization:\",vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\",vector_model.get_vector(\"in\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand the vocabulary using words from the embedding model\n",
    "\n",
    "- The embedding model usually knows more words than the task specific model, because it has seen a lot more data\n",
    "- If you wish, you can use the embedding model vocabulary to expand the task specific one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 47944\n"
     ]
    }
   ],
   "source": [
    "# init the vectorizer vocabulary using words from the embedding model\n",
    "def init_vocabulary(vocab, text, text_analyzer):\n",
    "    for word in analyzer(text):\n",
    "        vocab.setdefault(word, len(vocab))\n",
    "    return vocab\n",
    "\n",
    "words_from_model=\" \".join(words[:50000]) # use 50K words from the embedding model to initialize the vocabulary --> expands the learned vocabulary\n",
    "vocabulary={\"<SPECIAL>\": 0} # zero has a special meaning in sequence models, prevent using it for a normal word\n",
    "vocabulary=init_vocabulary(vocabulary, words_from_model, analyzer)\n",
    "print(\"Words from embedding model:\",len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer\n",
    "\n",
    "- Build a dictionary to turn words into numbers, here we use the one which we initialized with the embedding model\n",
    "- Vectorizing a sequence: In our data each example is a list of words, we need to turn each example into list of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in vocabulary: 105571\n",
      "Vectorized data shape: (25000,)\n",
      "First example vectorized: [2153, 25519, 34132, 1537, 31813, 2955, 31591, 9649, 29157, 5760, 36714, 7137, 41480, 5357, 47944, 8077, 47945, 1416, 47946, 3558, 1268, 3629, 6299, 14666, 5868, 47947, 27319, 12643, 9201, 35157, 9965, 3626, 13102, 16718, 800, 47948, 47949, 47389, 8303, 47950, 6556, 34510, 11279, 14335, 20383, 7601, 24518, 3318, 20463, 4334, 47951, 47952, 15117, 2578, 943, 47953, 38277, 3502, 47954, 9492, 2955, 31591, 3, 6034, 1772, 4870, 27232, 3, 13, 13942, 2, 53, 1402, 724, 4468, 1789, 5, 19, 20417, 853, 272, 2255, 16, 943, 47953, 2, 752, 5, 1537, 140, 2, 27, 1238, 5760, 47955, 63, 205, 80, 9, 272, 31, 1746, 7479, 2153, 25519, 34132, 7, 18, 90, 13606, 700, 4, 1, 225, 413, 2763, 1246, 530, 625, 35418, 47956, 65, 34132, 9115, 1519, 1966, 2703, 2, 15839, 47957, 415, 70, 88, 241, 5, 1, 1994, 2615, 7806, 314, 23, 1, 4559, 2012, 1521, 54, 21, 5326, 5, 602, 951, 4005, 4819, 103, 69, 7561, 44858, 2146, 66, 1890, 9, 2703, 2153, 2016, 7, 377, 2051, 951, 33, 2216, 16, 2703, 2, 110, 4484, 4, 6200, 51, 131, 7, 593, 3739, 47958, 2703, 97, 1655, 1375, 2, 1891, 753, 1521, 1226, 47959, 11, 27, 1238, 2359, 4, 132, 37, 244, 33, 5122, 2, 1225, 4, 924, 12, 71, 554, 2955, 31591, 9649, 29157, 2, 1, 1911, 3, 1, 1789, 5760, 36714, 7137, 41480, 5357, 47944, 8077, 47945, 1416, 47946, 3558, 1268, 3629, 170, 1290, 4, 24, 2297, 75, 3, 9370, 4802, 103, 19, 7, 183, 21060, 3970, 2665, 2, 28, 40, 142, 4517, 374, 7, 8476, 18, 1, 406, 47960, 22706, 408, 23, 4, 157, 7, 66, 6771, 3543, 15, 45, 1991, 8790, 9, 4716, 1239, 127, 2703, 1238, 15839, 3135, 17, 1201, 3908, 4653, 2, 6, 15, 58, 9921, 66, 1521, 1325, 46, 1, 372, 7, 22, 12780, 4790, 110, 46, 1, 2343, 7, 45, 1638, 255, 9370, 4802, 2343, 19, 272, 28, 40, 4517, 10818, 1176, 21, 1, 10340, 3310, 130, 2703, 2, 27, 1238, 2, 1, 837, 1601, 5, 25, 55, 2703, 12695, 1, 1609, 37, 64, 401, 58, 37, 28, 59, 1196, 13, 1056, 7, 378, 411, 180, 347, 2, 6268, 5, 48, 1590, 32, 12, 1, 1244, 3, 1073, 1056, 6, 686, 193, 189, 889, 9, 20417, 25, 2138, 4, 1093, 1402, 3, 19, 1244, 3, 1, 2102, 13288, 853, 14, 34147, 1421, 47961, 32, 537, 4591, 7856]\n",
      "First example text: ['Her', 'Deadly', 'Rival', '1995', 'Starring', 'Harry', 'Hamlin', 'Annie', 'Potts', 'Lisa', 'Zane', 'Tommy', 'Hinkley', 'Susan', 'Diol', 'Roma', 'Maffia', 'Robert', 'Treveiler', 'Anderson', 'William', 'Blair', 'Sean', 'Bridges', 'Robin', 'Dallenbach', 'Wilbur', 'Fitzgerald', 'Dale', 'Frye', 'Stan', 'Kelly', 'Deborah', 'Hobart', 'David', 'Lenthall', 'Lorri', 'Lindberg', 'Chuck', 'Kinlaw', 'Amy', 'Parrish', 'Melissa', 'Suzanne', 'McBride', 'Ralph', 'Wilcox', 'Al', 'Wiggins', 'Jeff', 'Sumerel', 'Daria', 'Sanford', 'Director', 'James', 'Hayman', 'Screenplay', 'Dan', 'Vining', 'Actor', 'Harry', 'Hamlin', 'of', 'LA', 'Law', 'fame', 'Clash', 'of', 'The', 'Titans', 'and', 'other', 'films', 'seems', 'perfectly', 'cast', 'in', 'this', 'Lifetime', 'type', 'film', 'directed', 'by', 'James', 'Hayman', 'and', 'released', 'in', '1995', 'He', 'and', 'his', 'wife', 'Lisa', 'Rinna', 'would', 'later', 'work', 'on', 'film', 'about', 'sex', 'addiction', 'Her', 'Deadly', 'Rival', 'is', 'at', 'first', 'glance', 'similar', 'to', 'the', 'better', 'known', 'Hollywood', 'box', 'office', 'hit', 'Fatal', 'Attraction', 'In', 'Rival', 'happily', 'married', 'couple', 'Jim', 'and', 'Kris', 'Lanford', 'move', 'into', 'new', 'home', 'in', 'the', 'typically', 'beautiful', 'suburbs', 'They', 'have', 'the', 'seemingly', 'perfect', 'marriage', 'they', 'are', 'deeply', 'in', 'love', 'despite', 'routine', 'lifestyle', 'But', 'then', 'mysterious', 'admirer', 'sets', 'her', 'eyes', 'on', 'Jim', 'Her', 'identity', 'is', 'never', 'revealed', 'despite', 'an', 'attempt', 'by', 'Jim', 'and', 'even', 'investigators', 'to', 'discover', 'who', 'she', 'is', 'She', 'constantly', 'harasses', 'Jim', 'through', 'phone', 'calls', 'and', 'letters', 'His', 'marriage', 'nearly', 'flounders', 'as', 'his', 'wife', 'begins', 'to', 'think', 'he', 'having', 'an', 'affair', 'and', 'trying', 'to', 'cover', 'it', 'up', 'While', 'Harry', 'Hamlin', 'Annie', 'Potts', 'and', 'the', 'rest', 'of', 'the', 'cast', 'Lisa', 'Zane', 'Tommy', 'Hinkley', 'Susan', 'Diol', 'Roma', 'Maffia', 'Robert', 'Treveiler', 'Anderson', 'William', 'Blair', 'each', 'seem', 'to', 'be', 'straight', 'out', 'of', 'soap', 'opera', 'But', 'this', 'is', 'very', 'suspense', 'filled', 'drama', 'and', 'has', 'its', 'good', 'moments', 'There', 'is', 'twist', 'at', 'the', 'end', 'Spoiler', 'Alert', 'All', 'have', 'to', 'say', 'is', 'her', 'deadly', 'rival', 'was', 'only', 'herself', 'Based', 'on', 'supposedly', 'actual', 'case', 'Jim', 'wife', 'Kris', 'suffered', 'from', 'multiple', 'personality', 'disorder', 'and', 'that', 'was', 'what', 'ruined', 'her', 'marriage', 'Even', 'if', 'the', 'story', 'is', 'not', 'terribly', 'impressive', 'even', 'if', 'the', 'acting', 'is', 'only', 'step', 'above', 'soap', 'opera', 'acting', 'this', 'film', 'has', 'its', 'moments', 'Especially', 'moving', 'are', 'the', 'intimate', 'scenes', 'between', 'Jim', 'and', 'his', 'wife', 'and', 'the', 'final', 'scene', 'in', 'which', 'when', 'Jim', 'learns', 'the', 'truth', 'he', 'can', 'believe', 'what', 'he', 'has', 'just', 'heard', 'The', 'movie', 'is', 'probably', 'little', 'too', 'long', 'and', 'boring', 'in', 'some', 'parts', 'but', 'it', 'the', 'kind', 'of', 'TV', 'movie', 'that', 'usually', 'does', 'well', 'especially', 'on', 'Lifetime', 'which', 'continues', 'to', 'produce', 'films', 'of', 'this', 'kind', 'of', 'the', 'domestic', 'thriller', 'type', 'or', 'seduction', 'stories', 'Trashy', 'but', 'everyone', 'likes', 'trash']\n"
     ]
    }
   ],
   "source": [
    "def vectorizer(vocab, texts):\n",
    "    vectorized_data=[] # turn text into numbers based on our vocabulary mapping\n",
    "    for one_example in texts:\n",
    "        vectorized_example=[]\n",
    "        for word in analyzer(one_example):\n",
    "            vocab.setdefault(word, len(vocab)) # add word to our vocabulary if it does not exist\n",
    "            vectorized_example.append(vocab[word])\n",
    "        vectorized_data.append(vectorized_example)\n",
    "    \n",
    "    vectorized_data=numpy.array(vectorized_data) # turn python list into numpy matrix\n",
    "    return vectorized_data, vocab\n",
    "\n",
    "vectorized_data, vocabulary=vectorizer(vocabulary, texts)\n",
    "\n",
    "# now vectorized data is the same as feature_matrix, but in different format\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "print(\"Vectorized data shape:\",vectorized_data.shape)\n",
    "print(\"First example vectorized:\",vectorized_data[0])\n",
    "inversed_vocabulary={value:key for key, value in vocabulary.items()} # inverse the dictionary\n",
    "print(\"First example text:\",[inversed_vocabulary[idx] for idx in vectorized_data[0]])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels into onehot vectors\n",
    "\n",
    "- Same as in the original BOW classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_numbers shape= (25000,)\n",
      "class_numbers [1 1 1 ... 1 0 1]\n",
      "class labels ['neg' 'pos']\n",
      "classes_1hot [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "one_hot_encoder=OneHotEncoder(sparse=False) #Turns class integers into one-hot encoding\n",
    "class_numbers=label_encoder.fit_transform(labels)\n",
    "print(\"class_numbers shape=\",class_numbers.shape)\n",
    "print(\"class_numbers\",class_numbers)\n",
    "print(\"class labels\",label_encoder.classes_)\n",
    "#And now yet the one-hot encoding\n",
    "classes_1hot=one_hot_encoder.fit_transform(class_numbers.reshape(-1,1))\n",
    "print(\"classes_1hot\",classes_1hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "- First we need to create an embedding matrix which we can then plug into the neural network\n",
    "- The embedding matrix must follow the order from the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained vectors for 93284 words.\n",
      "Shape of pretrained embeddings: (105571, 300)\n",
      "Vector for the word 'in': [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
     ]
    }
   ],
   "source": [
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings=numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab),embedding_model.vectors.shape[1])) # initialize new matrix (words x embedding dim)\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)\n",
    "print(\"Shape of pretrained embeddings:\",pretrained.shape)\n",
    "print(\"Vector for the word 'in':\",pretrained[vocabulary[\"in\"]][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "### Sequential input\n",
    "\n",
    "- Remember how the shape of the input data matrix had undefined number of columns\n",
    "- Now we must make it into fixed size (same for each example)\n",
    "- Padding: include zeros until you reach the correct size\n",
    "- You will hear more about this next week!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (25000,)\n",
      "New shape: (25000, 2366)\n",
      "First example: [ 2153 25519 34132 ...     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='post')\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\", vectorized_data_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "class MaskedAveragePooling(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(MaskedAveragePooling, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            # mask (batch, time)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            # mask (batch, x_dim, time)\n",
    "            mask = K.repeat(mask, x.shape[-1])\n",
    "            # mask (batch, time, x_dim)\n",
    "            mask = tf.transpose(mask, perm=[0,2,1])\n",
    "            x = x * mask\n",
    "        return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # remove temporal dimension\n",
    "        return input_shape[0], input_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2366)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 2366, 300)         31671300  \n",
      "_________________________________________________________________\n",
      "masked_average_pooling_1 (Ma (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 31,671,902\n",
      "Trainable params: 31,671,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/100\n",
      " 3400/22500 [===>..........................] - ETA: 5:12 - loss: 0.6914 - acc: 0.5421"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "\n",
    "example_count,sequence_len=vectorized_data_padded.shape\n",
    "example_count,class_count=classes_1hot.shape\n",
    "\n",
    "vector_size=pretrained.shape[1] # embedding dim (\"hidden layer\") must be the same as in the pretrained model\n",
    "\n",
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, weights=[pretrained])(inp)\n",
    "average_embeddings=MaskedAveragePooling()(embeddings)\n",
    "#sums=Lambda(lambda s: K.sum(s, axis=1), output_shape=lambda s: (s[0],s[2]))(embeddings) # custom layer to sum all embeddings\n",
    "tanh=Activation(\"tanh\")(average_embeddings)\n",
    "outp=Dense(class_count, activation=\"softmax\")(tanh)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.0001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "hist=model.fit(vectorized_data_padded,classes_1hot,batch_size=100,verbose=1,epochs=100,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"History:\",hist.history[\"val_acc\"])\n",
    "print(\"Max accuracy:\",numpy.max(hist.history[\"val_acc\"]))\n",
    "plt.ylim(0.85,1.0)\n",
    "plt.plot(hist.history[\"val_acc\"],label=\"Validation set accuracy\")\n",
    "plt.plot(hist.history[\"acc\"],label=\"Training set accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
