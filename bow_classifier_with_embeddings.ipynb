{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"I started to watch this movie expecting nothing, just another movie to watch, but since the first twenty minutes, the artwork and main character, who is enigmatic, doesn't talk much, really got me in this movie.  I really liked this movie, it was dark, beautifully acted and really touching. It's a bit slow but the immersion was complete. The directing was awesome by letting us know bits by bits the story leading to the conviction of Joey and his life behind bars. The music was really great and very well incorporated into the scenes. The ending was unexpected with a twist I didn't see coming. It's not the kind of movie we see often.\", 'class': 'pos'}\n",
      "[\"I started to watch this movie expecting nothing, just another movie to watch, but since the first twenty minutes, the artwork and main character, who is enigmatic, doesn't talk much, really got me in this movie.  I really liked this movie, it was dark, beautifully acted and really touching. It's a bit slow but the immersion was complete. The directing was awesome by letting us know bits by bits the story leading to the conviction of Joey and his life behind bars. The music was really great and very well incorporated into the scenes. The ending was unexpected with a twist I didn't see coming. It's not the kind of movie we see often.\", 'I firstly and completely and confidently disagree with the user who calls this a \\\\spoof\\\\\". Crispin Glover is very serious about his film. He personally introduced the film at the screening I saw in Chicago. He had worked on the film for years and it is the first in an intended trilogy. \\\\\"What is it?\\\\\" is Crispin Glover\\'s attempt at an art film in the vein of those he idolizes by Herzog, Lynch etc.  I had heard rumor of this film years ago \\\\\"epic porno movie with all down-syndrome cast directed by crispin glover\\\\\". When it finally came out i watched the trailer on-line and read the synopsis and i was foaming at the mouth with anticipation. ...I went to chicago to see it and it was a major disappointment. If he took out the goofy sh*t, such as the pot-smoking grandma, and the dancing dolls, he would be left with something much better, but only about 10 minutes long.  In other words just watch the trailer, be entertained, and leave it at that. There are some striking images and fantastic juxtapositions and phrases, but its lack of focus amounts to disappointment.\"']\n",
      "['pos', 'neg']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"data/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) \n",
    "print(data[0])\n",
    "\n",
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['started', 'to', 'watch', 'this', 'movie', 'expecting', 'nothing', 'just', 'another', 'movie', 'to', 'watch', 'but', 'since', 'the', 'first', 'twenty', 'minutes', 'the', 'artwork', 'and', 'main', 'character', 'who', 'is', 'enigmatic', 'doesn', 'talk', 'much', 'really', 'got', 'me', 'in', 'this', 'movie', 'really', 'liked', 'this', 'movie', 'it', 'was', 'dark', 'beautifully', 'acted', 'and', 'really', 'touching', 'It', 'bit', 'slow', 'but', 'the', 'immersion', 'was', 'complete', 'The', 'directing', 'was', 'awesome', 'by', 'letting', 'us', 'know', 'bits', 'by', 'bits', 'the', 'story', 'leading', 'to', 'the', 'conviction', 'of', 'Joey', 'and', 'his', 'life', 'behind', 'bars', 'The', 'music', 'was', 'really', 'great', 'and', 'very', 'well', 'incorporated', 'into', 'the', 'scenes', 'The', 'ending', 'was', 'unexpected', 'with', 'twist', 'didn', 'see', 'coming', 'It', 'not', 'the', 'kind', 'of', 'movie', 'we', 'see', 'often']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy\n",
    "analyzer=CountVectorizer(lowercase=False).build_analyzer() # includes tokenizer and preprocessing\n",
    "print(analyzer(texts[0]))\n",
    "\n",
    "\n",
    "#vectorizer=vectorizer.fit(texts) # learns a vocabulary dictionary\n",
    "#print(\"Vocabulary size:\",len(vectorizer.vocabulary_))\n",
    "#print(\"First 5 items in the vocabulary:\",list(vectorizer.vocabulary_.keys())[:5])\n",
    "#print(\"How many words are recognized from the data:\",numpy.count_nonzero(vectorizer.transform([\"i went today to new_york\"]).todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 50000\n",
      "First 50 words: ['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said', 'was', 'the', 'at', 'not', 'as', 'it', 'be', 'from', 'by', 'are', 'I', 'have', 'he', 'will', 'has', '####', 'his', 'an', 'this', 'or', 'their', 'who', 'they', 'but', '$', 'had', 'year', 'were', 'we', 'more', '###', 'up', 'been', 'you', 'its', 'one', 'about', 'would', 'which', 'out']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model=KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin\", binary=True, limit=50000)\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "# lowercase everything because that is the dafeault setting in CountVectorizer\n",
    "words=[k for k,v in sorted(vector_model.vocab.items(), key=lambda x:x[1].index)]\n",
    "print(\"Words from embedding model:\",len(words))\n",
    "print(\"First 50 words:\",words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 49412\n"
     ]
    }
   ],
   "source": [
    "# init the vectorizer vocabulary using words from the embedding model\n",
    "\n",
    "def init_vocabulary(vocab, text, text_analyzer):\n",
    "    for word in analyzer(text):\n",
    "        vocab.setdefault(word, len(vocab))\n",
    "    return vocab\n",
    "\n",
    "words_txt=\" \".join(words)\n",
    "vocabulary={\"<SPECIAL>\": 0} # zero has a special meaning, prevent using it for a normal word\n",
    "vocabulary=init_vocabulary(vocabulary,words_txt,analyzer)\n",
    "print(\"Words from embedding model:\",len(vocabulary))\n",
    "#for word in words:\n",
    "#    if word not in vocab:\n",
    "#        print(word)\n",
    "#    vocab.setdefault(word, len(vectorizer.vocabulary_)) # setdefault adds the word if it does not already exist\n",
    "#vectorizer.vocabulary_=vocab\n",
    "#print(\"Vocabulary size:\",len(vectorizer.vocabulary_))\n",
    "#print(\"First 5 items in the vocabulary:\",list(vectorizer.vocabulary_.keys())[:5])\n",
    "#print(\"How many words are recognized from the data:\",numpy.count_nonzero(vectorizer.transform([\"i went today to new_york\"]).todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in vocabulary: 111151\n",
      "Vectorized data shape: (25000,)\n",
      "Firs example vectorized: [416, 49412, 1187, 25, 1081, 3339, 740, 70, 183, 1081, 49412, 1187, 30, 131, 10, 51, 7392, 424, 10, 9830, 49413, 795, 1929, 28, 4, 30515, 49414, 808, 139, 213, 179, 157, 1, 25, 1081, 213, 3903, 25, 1081, 14, 9, 2789, 13220, 5281, 49413, 213, 8267, 46, 705, 1754, 30, 10, 27730, 9, 889, 6, 7864, 9, 5981, 17, 4750, 152, 163, 9896, 17, 9896, 10, 520, 430, 49412, 10, 4291, 49415, 11044, 49413, 23, 239, 446, 4921, 6, 603, 9, 213, 247, 49413, 129, 104, 8078, 64, 10, 3887, 6, 1966, 9, 5137, 7, 7977, 49416, 146, 443, 46, 12, 10, 593, 49415, 1081, 34, 146, 606]\n"
     ]
    }
   ],
   "source": [
    "def vectorizer(vocab, texts):\n",
    "    vectorized_data=[] # turn text into numbers based on our vocabulary mapping\n",
    "    for one_example in texts:\n",
    "        vectorized_example=[]\n",
    "        for word in analyzer(one_example):\n",
    "            vocab.setdefault(word, len(vocab)) # add word to out vocabulary if it does not exist\n",
    "            vectorized_example.append(vocab[word])\n",
    "        vectorized_data.append(vectorized_example)\n",
    "    \n",
    "    vectorized_data=numpy.array(vectorized_data) # turn python list into numpy matrix\n",
    "    return vectorized_data\n",
    "\n",
    "vectorized_data=vectorizer(vocabulary, texts)\n",
    "\n",
    "# now vectorized data is in the same as feature_matrix, but in slightly different format\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "print(\"Vectorized data shape:\",vectorized_data.shape)\n",
    "print(\"Firs example vectorized:\",vectorized_data[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_numbers shape= (25000,)\n",
      "class_numbers [1 0 0 ... 1 1 1]\n",
      "class labels ['neg' 'pos']\n",
      "classes_1hot [[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#feature_matrix=vectorizer.transform(texts)\n",
    "#print(feature_matrix.shape)\n",
    "\n",
    "# labels\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "one_hot_encoder=OneHotEncoder(sparse=False) #Turns class integers into one-hot encoding\n",
    "class_numbers=label_encoder.fit_transform(labels)\n",
    "print(\"class_numbers shape=\",class_numbers.shape)\n",
    "print(\"class_numbers\",class_numbers)\n",
    "print(\"class labels\",label_encoder.classes_)\n",
    "#And now yet the one-hot encoding\n",
    "classes_1hot=one_hot_encoder.fit_transform(class_numbers.reshape(-1,1))\n",
    "print(\"classes_1hot\",classes_1hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained vectors for 48923 words.\n",
      "Shape of pretrained embeddings: (111151, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    import string\n",
    "    pretrained_embeddings=numpy.zeros((len(vocab),embedding_model.vectors.shape[1])) # initialize new matrix (words x embedding dim)\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)\n",
    "print(\"Shape of pretrained embeddings:\",pretrained.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape: (25000, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 200)          22230200  \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 200)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 22,230,602\n",
      "Trainable params: 22,230,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "22500/22500 [==============================] - 78s 3ms/step - loss: 0.5756 - acc: 0.7451 - val_loss: 0.4519 - val_acc: 0.8092\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Flatten\n",
    "from keras.layers.pooling import AveragePooling1D\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, maxlen=100, padding='post', truncating='post')\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "example_count,sequence_len=vectorized_data_padded.shape\n",
    "example_count,class_count=classes_1hot.shape\n",
    "\n",
    "#example_count,feature_count=feature_matrix.shape\n",
    "\n",
    "#example_count,class_count=classes_1hot.shape\n",
    "\n",
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary),200)(inp)\n",
    "pooling=AveragePooling1D(pool_size=sequence_len)(embeddings)\n",
    "flattened=Flatten()(pooling) # removes extra dimension\n",
    "#hidden=Dense(200, activation=\"tanh\")(flattened)\n",
    "outp=Dense(class_count,activation=\"softmax\")(flattened)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "hist=model.fit(vectorized_data_padded,classes_1hot,batch_size=100,verbose=1,epochs=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
