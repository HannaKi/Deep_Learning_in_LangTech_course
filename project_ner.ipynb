{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition project\n",
    "In this project the goal is to detect persons, organizations and locations mentioned in the text.\n",
    "Thus, we are doing predictions for each word/token.\n",
    "The data looks something like this:\n",
    "\n",
    "| Word           | Label |\n",
    "|----------------|-------|\n",
    "| Germany        | I-LOC |\n",
    "| 's             | O     |\n",
    "| representative | O     |\n",
    "| to             | O     |\n",
    "| the            | O     |\n",
    "| European       | I-ORG |\n",
    "| Union          | I-ORG |\n",
    "| 's             | O     |\n",
    "| veterinary     | O     |\n",
    "| committee      | O     |\n",
    "| Werner         | I-PER |\n",
    "| Zwingmann      | I-PER |\n",
    "| said           | O     |\n",
    "| on             | O     |\n",
    "| Wednesday      | O     |\n",
    "| consumers      | O     |\n",
    "| should         | O     |\n",
    "| buy            | O     |\n",
    "| sheepmeat      | O     |\n",
    "| from           | O     |\n",
    "| countries      | O     |\n",
    "| other          | O     |\n",
    "| than           | O     |\n",
    "| Britain        | I-LOC |\n",
    "| until          | O     |\n",
    "| the            | O     |\n",
    "| scientific     | O     |\n",
    "| advice         | O     |\n",
    "| was            | O     |\n",
    "| clearer        | O     |\n",
    "| .              | O     |\n",
    "\n",
    "\n",
    "Here label I-LOC refers to locations, I-ORG to organizations and I-PER to persons. All other words are labeled with O tags. Having multiple consecutive words with the same label are considered to be part of the same entity, e.g. _Werner Zwingmann_ is a single person entity. This topic will be covered in detail during the upcoming lectures, but you can already start planning how you would solve the problem and you should be able to also build a simple model for the task.\n",
    "\n",
    "## Data\n",
    "Training data can be downloaded from https://github.com/glample/tagger .\n",
    "The data can be converted to a json format with read_ner.ipynb .\n",
    "\n",
    "The data is divided into separate sentences and has also been tokenized already. You don't have to do any preprocessing. The produced json files contain a single dictionary for each sentence. The dictionary has a list of tokens and the corresponding list of labels, e.g.:\n",
    "  {\n",
    "    \"text\": [\n",
    "      \"EU\",\n",
    "      \"rejects\",\n",
    "      \"German\",\n",
    "      \"call\",\n",
    "      \"to\",\n",
    "      \"boycott\",\n",
    "      \"British\",\n",
    "      \"lamb\",\n",
    "      \".\"\n",
    "    ],\n",
    "    \"tags\": [\n",
    "      \"I-ORG\",\n",
    "      \"O\",\n",
    "      \"O\",\n",
    "      \"O\",\n",
    "      \"O\",\n",
    "      \"O\",\n",
    "      \"O\",\n",
    "      \"O\",\n",
    "      \"O\"\n",
    "    ]\n",
    "  },\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestones\n",
    "## 1.1 Predicting word labels independently\n",
    "The first part is to train a classifier which assigns a label for each given input word independently. Evaluate the results on token level and entity level.\n",
    "Report your results with different network hyperparameters.\n",
    "Also discuss whether the token level accuracy is a reasonable metric. How do the pretrained word embeddings influence your predictions?\n",
    "## 1.2 Expand context\n",
    "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer (recurrent neural networks will be discussed later).\n",
    "Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions.\n",
    "## 2.1 Analyze false positive predictions\n",
    "Look at the entities your model is predicting, but which are not present in the gold standard data. Can you see any patterns in the misclassifications? Do the predicted entities share some similarities with the real entities?\n",
    "\n",
    "## 2.2 Analyze the convolutional kernels\n",
    "Using the example codes shown during the lectures, analyze where your convolutional kernels are activating. Are the any clear person name related kernels etc.?\n",
    "\n",
    "## 3. Add character level information to your model\n",
    "Add a convolutional layer which reads words as character sequences. Concatenate this information with the word embeddings before feeding them into the other existing layers you have created in your model architecture. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
