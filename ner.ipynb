{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data, vocabulary and pretrained embeddings\n",
    "These parts are similar to the previous examples. Things to note though:\n",
    "* Our data has already been tokenized and divided into sentences\n",
    "* We _cannot_ skip tokens\n",
    "* We are using a specific OOV (out-of-vocabulary) embedding for all words which are not present in our vocab\n",
    "* We now have one label for each token, not for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'tags': ['I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn']]\n",
      "[['I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-PER', 'I-PER']]\n",
      "Words from embedding model: 50000\n",
      "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
      "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
      " -0.0063]\n",
      "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n",
      "Words in vocabulary: 50002\n",
      "Found pretrained vectors for 50000 words.\n"
     ]
    }
   ],
   "source": [
    "# Load our training data\n",
    "import json\n",
    "import random\n",
    "import numpy\n",
    "with open(\"data/ner_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "print(data[0])\n",
    "\n",
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"tags\"] for one_example in data] # This is now a list of lists just like the texts variable\n",
    "print(texts[:2])\n",
    "print(labels[:2])\n",
    "\n",
    "# Lets do the same thing for the validation data\n",
    "# We use a separate validation set, since generally using sentences from the same documents as train/validation results in overly optimistic scores\n",
    "with open(\"data/ner_test.json\") as f:\n",
    "    validation_data=json.load(f)\n",
    "validation_texts=[one_example[\"text\"] for one_example in data]\n",
    "validation_labels=[one_example[\"tags\"] for one_example in data]\n",
    "\n",
    "# Use gensim to read the embedding model\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model=KeyedVectors.load_word2vec_format(\"data/wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words=[k for k,v in sorted(vector_model.vocab.items(), key=lambda x:x[1].index)]\n",
    "print(\"Words from embedding model:\",len(words))\n",
    "print(\"First 50 words:\",words[:50])\n",
    "\n",
    "# Normalize the vectors\n",
    "\n",
    "print(\"Before normalization:\",vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\",vector_model.get_vector(\"in\")[:10])\n",
    "\n",
    "# Build vocabulary mappings\n",
    "\n",
    "vocabulary={\"<SPECIAL>\": 0, \"<OOV>\": 1} # zero has a special meaning in sequence models, prevent using it for a normal word\n",
    "for word in words:\n",
    "    vocabulary.setdefault(word, len(vocabulary))\n",
    "\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "inversed_vocabulary={value:key for key, value in vocabulary.items()} # inverse the dictionary\n",
    "\n",
    "# Label mappings\n",
    "label_set = set([label for sentence_labels in labels for label in sentence_labels])\n",
    "label_map = {label: index for index, label in enumerate(label_set)}\n",
    "                \n",
    "# Embedding matrix\n",
    "\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings=numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
    "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing data\n",
    "If we want to consider the task as sequence labeling, we should feed the input data as word sequences and outputs as label sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def vectorizer(vocab, texts, label_map, labels):\n",
    "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
    "    vectorized_labels = [] # same thing for the labels\n",
    "    sentence_lengths = [] # Number of tokens in each sentence\n",
    "    \n",
    "    for i, one_example in enumerate(texts):\n",
    "        vectorized_example = []\n",
    "        vectorized_example_labels = []\n",
    "        for word in one_example:\n",
    "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
    "        \n",
    "        for label in labels[i]:\n",
    "            vectorized_example_labels.append(label_map[label])\n",
    "\n",
    "        vectorized_data.append(vectorized_example)\n",
    "        vectorized_labels.append(vectorized_example_labels)\n",
    "        \n",
    "        sentence_lengths.append(len(one_example))\n",
    "        \n",
    "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy matrix\n",
    "    vectorized_labels = numpy.array(vectorized_labels)\n",
    "    \n",
    "    return vectorized_data, vectorized_labels, sentence_lengths\n",
    "\n",
    "vectorized_data, vectorized_labels, lengths=vectorizer(vocabulary, texts, label_map, labels)\n",
    "validation_vectorized_data, validation_vectorized_labels, validation_lengths=vectorizer(vocabulary, validation_texts, label_map, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "We add padding to the label sequences as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (14041,)\n",
      "New shape: (14041, 113)\n",
      "First example: [ 1587 11424   718   537     7 10975   379 14078     4     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "Padded labels shape: (14041, 113, 1)\n",
      "{'I-ORG': 0, 'I-LOC': 1, 'O': 2, 'I-PER': 3}\n",
      "First example labels: [[0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "First weight vector: [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='post')\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\", vectorized_data_padded[0])\n",
    "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
    "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='post'), -1)\n",
    "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
    "print(label_map)\n",
    "print(\"First example labels:\", vectorized_labels_padded[0])\n",
    "\n",
    "weights = numpy.copy(vectorized_data_padded)\n",
    "weights[weights > 0] = 1\n",
    "print(\"First weight vector:\", weights[0])\n",
    "\n",
    "# Same stuff for the validation data\n",
    "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='post')\n",
    "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='post'), -1)\n",
    "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
    "validation_weights[weights > 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating named entities\n",
    "Two things to consider:\n",
    "\n",
    "1. Keras does not use sample weighting in metrics (only for losses) (corect me if I'm wrong), so we have to create our own evaluation if we want to ignore padding in models which do not support masking (e.g. convolution)\n",
    "2. NER is usually evaluated on entity level, i.e. the full entity spans are compared instead of single tokens. The most common metric used is micro averaged F-score. This again is not something Keras has, so we have to code it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def evaluate(predictions, gold, lengths):\n",
    "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
    "    \n",
    "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
    "    \n",
    "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
    "    pred_count = sum([len(e) for e in pred_entities])\n",
    "    try:\n",
    "        precision = tp / pred_count\n",
    "        recall = tp / sum([len(e) for e in gold_entities])\n",
    "        fscore = 2 * precision * recall / (precision + recall)\n",
    "    except Exception as e:\n",
    "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
    "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
    "\n",
    "\n",
    "def _convert_to_entities(input_sequence):\n",
    "    \"\"\"\n",
    "    Reads a sequence of tags and converts them into a set of entities.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    previous_tag = label_map['O']\n",
    "    for i, tag in enumerate(input_sequence):\n",
    "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "            current_entity.append((tag, i))\n",
    "        elif tag == label_map['O']: # Entity has ended\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "        elif tag == previous_tag: # Current entity continues\n",
    "            current_entity.append((tag, i))\n",
    "        previous_tag = tag\n",
    "    \n",
    "    # Add the last entity to our entity list if the sentences ends with an entity\n",
    "    if len(current_entity) > 0:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    entity_offsets = set()\n",
    "    \n",
    "    for e in entities:\n",
    "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
    "    \n",
    "    return entity_offsets\n",
    "\n",
    "class EvaluateEntities(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
    "        evaluate(pred, validation_vectorized_labels_padded, validation_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent classification\n",
    "Time-distributed means that the same dense layer is applied to each time step. This means that we are now simply using a normal feedforward network to classify each word/token separately.\n",
    "\n",
    "Why didn't we one-hot encode our labels? :S\n",
    "\n",
    "It's because the sparse loss is doing it for us implicitly! Neat, right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, Conv1D, TimeDistributed, LSTM, Bidirectional\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "\n",
    "vector_size= pretrained.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 113, 100)          30100     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 113, 4)            404       \n",
      "=================================================================\n",
      "Total params: 15,031,104\n",
      "Trainable params: 30,504\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 9.5562\n",
      "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
      "14041/14041 [==============================] - 6s 445us/step - loss: 9.5545\n",
      "Epoch 2/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 7.2648\n",
      "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
      "14041/14041 [==============================] - 4s 296us/step - loss: 7.2330\n",
      "Epoch 3/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 5.6878\n",
      "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
      "14041/14041 [==============================] - 5s 336us/step - loss: 5.6862\n",
      "Epoch 4/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 4.7273\n",
      "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
      "14041/14041 [==============================] - 5s 338us/step - loss: 4.7176\n",
      "Epoch 5/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 3.9975\n",
      "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
      "14041/14041 [==============================] - 5s 346us/step - loss: 3.9714\n",
      "Epoch 6/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 3.3825\n",
      "Precision/Recall/F-score: 0.895774647887324 / 0.12703485468890444 / 0.22251377591183416\n",
      "14041/14041 [==============================] - 4s 290us/step - loss: 3.3794\n",
      "Epoch 7/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 2.9877\n",
      "Precision/Recall/F-score: 0.8008822939643072 / 0.1994407270548287 / 0.3193539359533043\n",
      "14041/14041 [==============================] - 5s 322us/step - loss: 2.9864\n",
      "Epoch 8/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 2.6897\n",
      "Precision/Recall/F-score: 0.7347107438016529 / 0.2219614501148507 / 0.3409265224727719\n",
      "14041/14041 [==============================] - 4s 286us/step - loss: 2.6829\n",
      "Epoch 9/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 2.3981\n",
      "Precision/Recall/F-score: 0.5893956889915319 / 0.3058523918905423 / 0.40272207245709785\n",
      "14041/14041 [==============================] - 5s 323us/step - loss: 2.3944\n",
      "Epoch 10/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 2.2077\n",
      "Precision/Recall/F-score: 0.5545862013766608 / 0.3460001997403376 / 0.42613776137761383\n",
      "14041/14041 [==============================] - 5s 363us/step - loss: 2.2058\n",
      "Epoch 11/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 2.0641\n",
      "Precision/Recall/F-score: 0.5264112610651575 / 0.36227903725157296 / 0.4291883577851396\n",
      "14041/14041 [==============================] - 4s 297us/step - loss: 2.0665\n",
      "Epoch 12/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 1.9843\n",
      "Precision/Recall/F-score: 0.5304882307746036 / 0.37925696594427244 / 0.4423026526511953\n",
      "14041/14041 [==============================] - 5s 337us/step - loss: 1.9808\n",
      "Epoch 13/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 1.8830\n",
      "Precision/Recall/F-score: 0.5403384244882439 / 0.4257465295116349 / 0.4762463343108504\n",
      "14041/14041 [==============================] - 4s 270us/step - loss: 1.8863\n",
      "Epoch 14/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.8036\n",
      "Precision/Recall/F-score: 0.5439677793372796 / 0.4451213422550684 / 0.4896053607228188\n",
      "14041/14041 [==============================] - 5s 335us/step - loss: 1.8053\n",
      "Epoch 15/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 1.7467\n",
      "Precision/Recall/F-score: 0.5479090418191637 / 0.45600719065215223 / 0.4977516147494073\n",
      "14041/14041 [==============================] - 4s 278us/step - loss: 1.7427\n",
      "Epoch 16/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 1.6983\n",
      "Precision/Recall/F-score: 0.5457562825983878 / 0.45980225706581446 / 0.49910564258225365\n",
      "14041/14041 [==============================] - 4s 313us/step - loss: 1.6976\n",
      "Epoch 17/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 1.6714\n",
      "Precision/Recall/F-score: 0.546360153256705 / 0.4628482972136223 / 0.5011489281176501\n",
      "14041/14041 [==============================] - 5s 343us/step - loss: 1.6728\n",
      "Epoch 18/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.6525\n",
      "Precision/Recall/F-score: 0.5478363261701501 / 0.4646459602516728 / 0.5028234849098916\n",
      "14041/14041 [==============================] - 5s 348us/step - loss: 1.6530\n",
      "Epoch 19/100\n",
      "13200/14041 [===========================>..] - ETA: 0s - loss: 1.6403\n",
      "Precision/Recall/F-score: 0.5500618046971569 / 0.46664336362728454 / 0.5049304336079967\n",
      "14041/14041 [==============================] - 6s 413us/step - loss: 1.6345\n",
      "Epoch 20/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 1.6192\n",
      "Precision/Recall/F-score: 0.551920927222451 / 0.46844102666533505 / 0.5067660643383842\n",
      "14041/14041 [==============================] - 5s 376us/step - loss: 1.6184\n",
      "Epoch 21/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.5953\n",
      "Precision/Recall/F-score: 0.5554512381175918 / 0.47268550883850996 / 0.510737023848063\n",
      "14041/14041 [==============================] - 5s 333us/step - loss: 1.5975\n",
      "Epoch 22/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.5928\n",
      "Precision/Recall/F-score: 0.555007342143906 / 0.471836612403875 / 0.5100537097514237\n",
      "14041/14041 [==============================] - 5s 348us/step - loss: 1.5901\n",
      "Epoch 23/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.5884\n",
      "Precision/Recall/F-score: 0.5569189220924089 / 0.47368421052631576 / 0.5119404193313365\n",
      "14041/14041 [==============================] - 4s 312us/step - loss: 1.5800\n",
      "Epoch 24/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.5707\n",
      "Precision/Recall/F-score: 0.5595720919297008 / 0.47538200339558573 / 0.5140527552039742\n",
      "14041/14041 [==============================] - 4s 293us/step - loss: 1.5703\n",
      "Epoch 25/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 1.5716\n",
      "Precision/Recall/F-score: 0.560016410737311 / 0.47712973134924597 / 0.5152610008628128\n",
      "14041/14041 [==============================] - 4s 294us/step - loss: 1.5674\n",
      "Epoch 26/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 1.5554\n",
      "Precision/Recall/F-score: 0.5614354516621638 / 0.47732947168680717 / 0.5159775450717909\n",
      "14041/14041 [==============================] - 5s 374us/step - loss: 1.5558\n",
      "Epoch 27/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.5482\n",
      "Precision/Recall/F-score: 0.5644838937220785 / 0.47952661539998004 / 0.5185485177385388\n",
      "14041/14041 [==============================] - 4s 288us/step - loss: 1.5460\n",
      "Epoch 28/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.5459\n",
      "Precision/Recall/F-score: 0.5653504384158182 / 0.4797263557375412 / 0.5190307679840082\n",
      "14041/14041 [==============================] - 4s 276us/step - loss: 1.5475\n",
      "Epoch 29/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 1.5382\n",
      "Precision/Recall/F-score: 0.5659544411089529 / 0.48012583641266354 / 0.5195191138727543\n",
      "14041/14041 [==============================] - 5s 329us/step - loss: 1.5379\n",
      "Epoch 30/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.5335\n",
      "Precision/Recall/F-score: 0.5702518239585785 / 0.48397083791071605 / 0.5235805737129274\n",
      "14041/14041 [==============================] - 4s 307us/step - loss: 1.5324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 1.5312\n",
      "Precision/Recall/F-score: 0.571184945604234 / 0.48501947468291223 / 0.5245875077637655\n",
      "14041/14041 [==============================] - 4s 305us/step - loss: 1.5256\n",
      "Epoch 32/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 1.5269\n",
      "Precision/Recall/F-score: 0.5718236921085152 / 0.4852192150204734 / 0.5249736621734785\n",
      "14041/14041 [==============================] - 4s 276us/step - loss: 1.5233\n",
      "Epoch 33/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.5290\n",
      "Precision/Recall/F-score: 0.5730171805130619 / 0.48631778687705984 / 0.5261196045594512\n",
      "14041/14041 [==============================] - 5s 321us/step - loss: 1.5259\n",
      "Epoch 34/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 1.5130\n",
      "Precision/Recall/F-score: 0.5739411764705883 / 0.4872166183960851 / 0.5270350564468211\n",
      "14041/14041 [==============================] - 4s 267us/step - loss: 1.5159\n",
      "Epoch 35/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 1.5162\n",
      "Precision/Recall/F-score: 0.5718397556390977 / 0.48611804653949864 / 0.5255060728744938\n",
      "14041/14041 [==============================] - 5s 329us/step - loss: 1.5137\n",
      "Epoch 36/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.5098\n",
      "Precision/Recall/F-score: 0.5719745972009879 / 0.4857185658643763 / 0.5253294448044935\n",
      "14041/14041 [==============================] - 5s 325us/step - loss: 1.5096\n",
      "Epoch 37/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.4987\n",
      "Precision/Recall/F-score: 0.5726711724948574 / 0.48656746229901127 / 0.5261197051915445\n",
      "14041/14041 [==============================] - 4s 290us/step - loss: 1.4999\n",
      "Epoch 38/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.4978\n",
      "Precision/Recall/F-score: 0.5732555398396982 / 0.4857185658643763 / 0.5258690598475428\n",
      "14041/14041 [==============================] - 4s 300us/step - loss: 1.4998\n",
      "Epoch 39/100\n",
      "13200/14041 [===========================>..] - ETA: 0s - loss: 1.4985\n",
      "Precision/Recall/F-score: 0.5722696496909038 / 0.48536902027364426 / 0.5252492502229066\n",
      "14041/14041 [==============================] - 4s 294us/step - loss: 1.4983\n",
      "Epoch 40/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.4961\n",
      "Precision/Recall/F-score: 0.5756787129591293 / 0.4860181763707181 / 0.5270625186147889\n",
      "14041/14041 [==============================] - 4s 307us/step - loss: 1.4965\n",
      "Epoch 41/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 1.4852\n",
      "Precision/Recall/F-score: 0.5778552849373374 / 0.48811544991511036 / 0.5292079475935249\n",
      "14041/14041 [==============================] - 4s 303us/step - loss: 1.4897\n",
      "Epoch 42/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 1.4866\n",
      "Precision/Recall/F-score: 0.578155657292348 / 0.4855687606112054 / 0.5278328131361106\n",
      "14041/14041 [==============================] - 5s 326us/step - loss: 1.4862\n",
      "Epoch 43/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.4790\n",
      "Precision/Recall/F-score: 0.5786245906519798 / 0.48526915010486366 / 0.5278509546182885\n",
      "14041/14041 [==============================] - 5s 356us/step - loss: 1.4799\n",
      "Epoch 44/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.4777\n",
      "Precision/Recall/F-score: 0.5804437042764528 / 0.4873164885648657 / 0.5298189418822444\n",
      "14041/14041 [==============================] - 4s 318us/step - loss: 1.4776\n",
      "Epoch 45/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 1.4689\n",
      "Precision/Recall/F-score: 0.5833482222619261 / 0.4891141516029162 / 0.5320911535432002\n",
      "14041/14041 [==============================] - 5s 327us/step - loss: 1.4717\n",
      "Epoch 46/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.4619\n",
      "Precision/Recall/F-score: 0.582251340083383 / 0.48816538499950063 / 0.5310734463276836\n",
      "14041/14041 [==============================] - 4s 294us/step - loss: 1.4731\n",
      "Epoch 47/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.4670\n",
      "Precision/Recall/F-score: 0.5793603614314589 / 0.48666733246779187 / 0.5289839339991316\n",
      "14041/14041 [==============================] - 5s 322us/step - loss: 1.4667\n",
      "Epoch 48/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.4667\n",
      "Precision/Recall/F-score: 0.5810102454133905 / 0.4870668131429142 / 0.5299071005595697\n",
      "14041/14041 [==============================] - 5s 337us/step - loss: 1.4617\n",
      "Epoch 49/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.4595\n",
      "Precision/Recall/F-score: 0.5793820558526441 / 0.48691700788974335 / 0.5291404384632082\n",
      "14041/14041 [==============================] - 4s 286us/step - loss: 1.4623\n",
      "Epoch 50/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.4432\n",
      "Precision/Recall/F-score: 0.583422778771616 / 0.488564865674623 / 0.5317969344493966\n",
      "14041/14041 [==============================] - 4s 295us/step - loss: 1.4487\n",
      "Epoch 51/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 1.4459\n",
      "Precision/Recall/F-score: 0.5846034770183377 / 0.4903125936282832 / 0.5333224702623431\n",
      "14041/14041 [==============================] - 5s 350us/step - loss: 1.4477\n",
      "Epoch 52/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.4522\n",
      "Precision/Recall/F-score: 0.5835864451194092 / 0.48931389194047736 / 0.5323084444685879\n",
      "14041/14041 [==============================] - 4s 297us/step - loss: 1.4519\n",
      "Epoch 53/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.4430\n",
      "Precision/Recall/F-score: 0.585349862915723 / 0.4904124637970638 / 0.5336919900010869\n",
      "14041/14041 [==============================] - 4s 280us/step - loss: 1.4486\n",
      "Epoch 54/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 1.4422\n",
      "Precision/Recall/F-score: 0.5859519408502772 / 0.4907120743034056 / 0.5341196293176075\n",
      "14041/14041 [==============================] - 4s 303us/step - loss: 1.4429\n",
      "Epoch 55/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 1.4438\n",
      "Precision/Recall/F-score: 0.5854848304580607 / 0.49146110056925996 / 0.5343685525029862\n",
      "14041/14041 [==============================] - 4s 259us/step - loss: 1.4410\n",
      "Epoch 56/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 1.4323\n",
      "Precision/Recall/F-score: 0.5925215723873442 / 0.4937581144512134 / 0.5386501062265076\n",
      "14041/14041 [==============================] - 4s 272us/step - loss: 1.4335\n",
      "Epoch 57/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.4301\n",
      "Precision/Recall/F-score: 0.5856340384845226 / 0.4893638270248677 / 0.5331882480957563\n",
      "14041/14041 [==============================] - 4s 274us/step - loss: 1.4294\n",
      "Epoch 58/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.4322\n",
      "Precision/Recall/F-score: 0.5871450635618392 / 0.4935583741136523 / 0.5362995116657624\n",
      "14041/14041 [==============================] - 4s 304us/step - loss: 1.4313\n",
      "Epoch 59/100\n",
      "13100/14041 [==========================>...] - ETA: 0s - loss: 1.4366\n",
      "Precision/Recall/F-score: 0.5861616462869073 / 0.4907120743034056 / 0.5342067353428829\n",
      "14041/14041 [==============================] - 4s 288us/step - loss: 1.4328\n",
      "Epoch 60/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 1.4320\n",
      "Precision/Recall/F-score: 0.5888476726097882 / 0.49146110056925996 / 0.5357648339684268\n",
      "14041/14041 [==============================] - 4s 304us/step - loss: 1.4312\n",
      "Epoch 61/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.4217\n",
      "Precision/Recall/F-score: 0.589894031012393 / 0.4920103864975532 / 0.536524272373329\n",
      "14041/14041 [==============================] - 4s 276us/step - loss: 1.4238\n",
      "Epoch 62/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 1.4150\n",
      "Precision/Recall/F-score: 0.5896714738794806 / 0.49206032158194346 / 0.5364618776710128\n",
      "14041/14041 [==============================] - 3s 235us/step - loss: 1.4184\n",
      "Epoch 63/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 1.4204\n",
      "Precision/Recall/F-score: 0.5907208450873297 / 0.49146110056925996 / 0.5365388284678496\n",
      "14041/14041 [==============================] - 3s 233us/step - loss: 1.4217\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.4251\n",
      "Precision/Recall/F-score: 0.5898388353004613 / 0.49161090582243083 / 0.5362638559795191\n",
      "14041/14041 [==============================] - 3s 227us/step - loss: 1.4186\n",
      "Epoch 65/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.4148\n",
      "Precision/Recall/F-score: 0.5949557575392764 / 0.4935583741136523 / 0.5395343759382079\n",
      "14041/14041 [==============================] - 3s 245us/step - loss: 1.4170\n",
      "Epoch 66/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 1.4107\n",
      "Precision/Recall/F-score: 0.5928262436914203 / 0.4927094776790173 / 0.5381510771748023\n",
      "14041/14041 [==============================] - 4s 257us/step - loss: 1.4127\n",
      "Epoch 67/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 1.4133\n",
      "Precision/Recall/F-score: 0.5950403274346936 / 0.4936582442824328 / 0.5396288209606986\n",
      "14041/14041 [==============================] - 4s 267us/step - loss: 1.4126\n",
      "Epoch 68/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.4115\n",
      "Precision/Recall/F-score: 0.5912933980931823 / 0.49240986717267554 / 0.5373402719123778\n",
      "14041/14041 [==============================] - 4s 252us/step - loss: 1.4127\n",
      "Epoch 69/100\n",
      "13100/14041 [==========================>...] - ETA: 0s - loss: 1.3991\n",
      "Precision/Recall/F-score: 0.5921368547418968 / 0.4926096075102367 / 0.5378073379490814\n",
      "14041/14041 [==============================] - 4s 268us/step - loss: 1.4086\n",
      "Epoch 70/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 1.4019\n",
      "Precision/Recall/F-score: 0.5951365956169319 / 0.49495655647658043 / 0.540443281262779\n",
      "14041/14041 [==============================] - 4s 264us/step - loss: 1.4087\n",
      "Epoch 71/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.4106\n",
      "Precision/Recall/F-score: 0.5917742419693786 / 0.49216019175072406 / 0.537389929391238\n",
      "14041/14041 [==============================] - 4s 270us/step - loss: 1.4075\n",
      "Epoch 72/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.4104\n",
      "Precision/Recall/F-score: 0.5954271961492178 / 0.4941575951263358 / 0.5400862304207826\n",
      "14041/14041 [==============================] - 4s 266us/step - loss: 1.4037\n",
      "Epoch 73/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 1.4079\n",
      "Precision/Recall/F-score: 0.5928986288246213 / 0.49445720563267753 / 0.5392218259046478\n",
      "14041/14041 [==============================] - 4s 286us/step - loss: 1.4046\n",
      "Epoch 74/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.4043\n",
      "Precision/Recall/F-score: 0.5933729515577165 / 0.49360830919804255 / 0.5389123620008178\n",
      "14041/14041 [==============================] - 3s 244us/step - loss: 1.4004\n",
      "Epoch 75/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.3984\n",
      "Precision/Recall/F-score: 0.5968129885748648 / 0.49560571257365427 / 0.5415211697948494\n",
      "14041/14041 [==============================] - 4s 286us/step - loss: 1.4015\n",
      "Epoch 76/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 1.3972\n",
      "Precision/Recall/F-score: 0.5974119771291002 / 0.49565564765804454 / 0.5417974400262002\n",
      "14041/14041 [==============================] - 4s 308us/step - loss: 1.4048\n",
      "Epoch 77/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.3980\n",
      "Precision/Recall/F-score: 0.601715908404326 / 0.4973035054429242 / 0.5445498537332203\n",
      "14041/14041 [==============================] - 4s 300us/step - loss: 1.3976\n",
      "Epoch 78/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 1.3995\n",
      "Precision/Recall/F-score: 0.594706517824991 / 0.49480675122340956 / 0.5401766245093762\n",
      "14041/14041 [==============================] - 4s 264us/step - loss: 1.3958\n",
      "Epoch 79/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 1.3911\n",
      "Precision/Recall/F-score: 0.5956513904739024 / 0.4952062318985319 / 0.5408043626448534\n",
      "14041/14041 [==============================] - 4s 272us/step - loss: 1.3963\n",
      "Epoch 80/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.3939\n",
      "Precision/Recall/F-score: 0.5961561561561561 / 0.49565564765804454 / 0.5412804013523831\n",
      "14041/14041 [==============================] - 4s 271us/step - loss: 1.3923\n",
      "Epoch 81/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.3961\n",
      "Precision/Recall/F-score: 0.596705344796489 / 0.49560571257365427 / 0.5414768542513435\n",
      "14041/14041 [==============================] - 4s 270us/step - loss: 1.3967\n",
      "Epoch 82/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 1.3890\n",
      "Precision/Recall/F-score: 0.598015037593985 / 0.49645460900828925 / 0.5425227142506344\n",
      "14041/14041 [==============================] - 4s 269us/step - loss: 1.3940\n",
      "Epoch 83/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.3956\n",
      "Precision/Recall/F-score: 0.5999157083508941 / 0.49755318086487565 / 0.5439606933260543\n",
      "14041/14041 [==============================] - 4s 262us/step - loss: 1.3981\n",
      "Epoch 84/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 1.3968\n",
      "Precision/Recall/F-score: 0.5996389891696751 / 0.49765305103365626 / 0.5439065655187469\n",
      "14041/14041 [==============================] - 4s 271us/step - loss: 1.3933\n",
      "Epoch 85/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.3879\n",
      "Precision/Recall/F-score: 0.6053779069767442 / 0.499151103565365 / 0.5471563851332859\n",
      "14041/14041 [==============================] - 4s 289us/step - loss: 1.3856\n",
      "Epoch 86/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.3858\n",
      "Precision/Recall/F-score: 0.5981976569540403 / 0.49720363527414363 / 0.543044912873933\n",
      "14041/14041 [==============================] - 4s 282us/step - loss: 1.3847\n",
      "Epoch 87/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.3839\n",
      "Precision/Recall/F-score: 0.6006979962693303 / 0.49850194746829124 / 0.5448492290899168\n",
      "14041/14041 [==============================] - 4s 307us/step - loss: 1.3840\n",
      "Epoch 88/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.3874\n",
      "Precision/Recall/F-score: 0.5996380090497737 / 0.4963048037551184 / 0.5430999153028606\n",
      "14041/14041 [==============================] - 4s 294us/step - loss: 1.3881\n",
      "Epoch 89/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 1.3835\n",
      "Precision/Recall/F-score: 0.5998316599531053 / 0.4982023369619495 / 0.5443138110695872\n",
      "14041/14041 [==============================] - 4s 298us/step - loss: 1.3816\n",
      "Epoch 90/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 1.3769\n",
      "Precision/Recall/F-score: 0.6003845922720991 / 0.49890142814341354 / 0.5449586821937982\n",
      "14041/14041 [==============================] - 4s 276us/step - loss: 1.3814\n",
      "Epoch 91/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.3755\n",
      "Precision/Recall/F-score: 0.6029838125151002 / 0.49850194746829124 / 0.5457875457875457\n",
      "14041/14041 [==============================] - 4s 291us/step - loss: 1.3737\n",
      "Epoch 92/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 1.3821\n",
      "Precision/Recall/F-score: 0.6011057027822847 / 0.4995006491560971 / 0.5456132216979845\n",
      "14041/14041 [==============================] - 4s 257us/step - loss: 1.3831\n",
      "Epoch 93/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 1.3813\n",
      "Precision/Recall/F-score: 0.6032973005616281 / 0.49885149305902327 / 0.5461254612546125\n",
      "14041/14041 [==============================] - 3s 239us/step - loss: 1.3763\n",
      "Epoch 94/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.3850\n",
      "Precision/Recall/F-score: 0.5985471573512637 / 0.4978527913712174 / 0.5435760433988496\n",
      "14041/14041 [==============================] - 4s 263us/step - loss: 1.3822\n",
      "Epoch 95/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 1.3796\n",
      "Precision/Recall/F-score: 0.604776299879081 / 0.4995006491560971 / 0.5471202756659193\n",
      "14041/14041 [==============================] - 4s 300us/step - loss: 1.3790\n",
      "Epoch 96/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.3703\n",
      "Precision/Recall/F-score: 0.6042195623262 / 0.49910116848097474 / 0.5466528112010501\n",
      "14041/14041 [==============================] - 4s 257us/step - loss: 1.3703\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13500/14041 [===========================>..] - ETA: 0s - loss: 1.3841\n",
      "Precision/Recall/F-score: 0.6041403382078594 / 0.5012983121941476 / 0.547935485631635\n",
      "14041/14041 [==============================] - 4s 284us/step - loss: 1.3785\n",
      "Epoch 98/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 1.3682\n",
      "Precision/Recall/F-score: 0.6077992744860943 / 0.5019974033756117 / 0.5498550566099655\n",
      "14041/14041 [==============================] - 4s 269us/step - loss: 1.3697\n",
      "Epoch 99/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.3716\n",
      "Precision/Recall/F-score: 0.6040725344900295 / 0.500699091181464 / 0.5475494880546075\n",
      "14041/14041 [==============================] - 4s 264us/step - loss: 1.3729\n",
      "Epoch 100/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.3675\n",
      "Precision/Recall/F-score: 0.6045405275201734 / 0.5012983121941476 / 0.5481000218388293\n",
      "14041/14041 [==============================] - 3s 246us/step - loss: 1.3662\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "hidden = TimeDistributed(Dense(100, activation=\"softmax\"))(embeddings)\n",
    "outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=1,epochs=100, callbacks=[EvaluateEntities()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding context with convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 113, 100)          90100     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 113, 4)            404       \n",
      "=================================================================\n",
      "Total params: 15,091,104\n",
      "Trainable params: 90,504\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.4250\n",
      "Precision/Recall/F-score: 0.6337763075343638 / 0.5548786577449316 / 0.591709044436753\n",
      "14041/14041 [==============================] - 8s 590us/step - loss: 0.4213\n",
      "Epoch 2/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.1614\n",
      "Precision/Recall/F-score: 0.7273012552301256 / 0.6943972835314092 / 0.7104685025289941\n",
      "14041/14041 [==============================] - 4s 313us/step - loss: 0.1605\n",
      "Epoch 3/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.1283\n",
      "Precision/Recall/F-score: 0.7467646754322393 / 0.7203635274143614 / 0.7333265555103702\n",
      "14041/14041 [==============================] - 4s 293us/step - loss: 0.1280\n",
      "Epoch 4/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.1161\n",
      "Precision/Recall/F-score: 0.7585977822065512 / 0.7412863277738939 / 0.7498421517868418\n",
      "14041/14041 [==============================] - 4s 289us/step - loss: 0.1160\n",
      "Epoch 5/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.1099\n",
      "Precision/Recall/F-score: 0.7705428731934018 / 0.7534205532807351 / 0.7618855252859343\n",
      "14041/14041 [==============================] - 4s 294us/step - loss: 0.1098\n",
      "Epoch 6/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.1046\n",
      "Precision/Recall/F-score: 0.7863476572747783 / 0.7483771097573155 / 0.7668926698221824\n",
      "14041/14041 [==============================] - 4s 269us/step - loss: 0.1046\n",
      "Epoch 7/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.1007\n",
      "Precision/Recall/F-score: 0.7557491786887588 / 0.7926195945271147 / 0.7737453995954081\n",
      "14041/14041 [==============================] - 4s 289us/step - loss: 0.1002\n",
      "Epoch 8/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0969\n",
      "Precision/Recall/F-score: 0.786902495240004 / 0.7842305003495456 / 0.7855642256902762\n",
      "14041/14041 [==============================] - 4s 274us/step - loss: 0.0972\n",
      "Epoch 9/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0941\n",
      "Precision/Recall/F-score: 0.8076535215752879 / 0.7598621791670828 / 0.7830293050659943\n",
      "14041/14041 [==============================] - 4s 271us/step - loss: 0.0942\n",
      "Epoch 10/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0914\n",
      "Precision/Recall/F-score: 0.8045163290615956 / 0.7774393288724658 / 0.790746101884301\n",
      "14041/14041 [==============================] - 4s 250us/step - loss: 0.0914\n",
      "Epoch 11/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0894\n",
      "Precision/Recall/F-score: 0.8064713687863632 / 0.789074203535404 / 0.7976779404341241\n",
      "14041/14041 [==============================] - 4s 278us/step - loss: 0.0892\n",
      "Epoch 12/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0867\n",
      "Precision/Recall/F-score: 0.7801290567470108 / 0.8210326575451913 / 0.8000583913191572\n",
      "14041/14041 [==============================] - 4s 255us/step - loss: 0.0867\n",
      "Epoch 13/100\n",
      "13200/14041 [===========================>..] - ETA: 0s - loss: 0.0845\n",
      "Precision/Recall/F-score: 0.7828568710459972 / 0.8218316188954359 / 0.8018709347365344\n",
      "14041/14041 [==============================] - 4s 268us/step - loss: 0.0851\n",
      "Epoch 14/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0821\n",
      "Precision/Recall/F-score: 0.7980834711547816 / 0.819284929591531 / 0.8085452395032525\n",
      "14041/14041 [==============================] - 4s 309us/step - loss: 0.0824\n",
      "Epoch 15/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 0.0809\n",
      "Precision/Recall/F-score: 0.826640447112399 / 0.7976630380505343 / 0.8118932655654384\n",
      "14041/14041 [==============================] - 4s 303us/step - loss: 0.0809\n",
      "Epoch 16/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0795\n",
      "Precision/Recall/F-score: 0.8025378989683731 / 0.8274243483471487 / 0.8147911390848966\n",
      "14041/14041 [==============================] - 4s 276us/step - loss: 0.0795\n",
      "Epoch 17/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0776\n",
      "Precision/Recall/F-score: 0.837376879212027 / 0.8066014181563966 / 0.821701088615322\n",
      "14041/14041 [==============================] - 4s 289us/step - loss: 0.0776\n",
      "Epoch 18/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0760\n",
      "Precision/Recall/F-score: 0.8032786885245902 / 0.8368121442125237 / 0.8197026022304834\n",
      "14041/14041 [==============================] - 4s 292us/step - loss: 0.0762\n",
      "Epoch 19/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 0.0745\n",
      "Precision/Recall/F-score: 0.8048827098561941 / 0.8412563667232598 / 0.8226676758551652\n",
      "14041/14041 [==============================] - 4s 303us/step - loss: 0.0748\n",
      "Epoch 20/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 0.0735\n",
      "Precision/Recall/F-score: 0.8012252591894439 / 0.8489963048037551 / 0.8244193376327402\n",
      "14041/14041 [==============================] - 4s 291us/step - loss: 0.0734\n",
      "Epoch 21/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0716\n",
      "Precision/Recall/F-score: 0.8084883166428231 / 0.8465994207530211 / 0.827105083422773\n",
      "14041/14041 [==============================] - 5s 347us/step - loss: 0.0718\n",
      "Epoch 22/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0708\n",
      "Precision/Recall/F-score: 0.813953488372093 / 0.845900329571557 / 0.829619472060336\n",
      "14041/14041 [==============================] - 4s 317us/step - loss: 0.0709\n",
      "Epoch 23/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0695\n",
      "Precision/Recall/F-score: 0.8213090097520741 / 0.8453011085588734 / 0.8331323670546545\n",
      "14041/14041 [==============================] - 4s 288us/step - loss: 0.0697\n",
      "Epoch 24/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0682\n",
      "Precision/Recall/F-score: 0.8245333333333333 / 0.8491960451413163 / 0.8366829844284274\n",
      "14041/14041 [==============================] - 5s 335us/step - loss: 0.0682\n",
      "Epoch 25/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0675\n",
      "Precision/Recall/F-score: 0.8252230528092597 / 0.854439229002297 / 0.8395770466867839\n",
      "14041/14041 [==============================] - 4s 307us/step - loss: 0.0674\n",
      "Epoch 26/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0664\n",
      "Precision/Recall/F-score: 0.8280088794517904 / 0.8567861779686408 / 0.8421517620496712\n",
      "14041/14041 [==============================] - 4s 284us/step - loss: 0.0663\n",
      "Epoch 27/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0654\n",
      "Precision/Recall/F-score: 0.8290895061728395 / 0.8584839708379107 / 0.8435307394141602\n",
      "14041/14041 [==============================] - 5s 348us/step - loss: 0.0655\n",
      "Epoch 28/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0643\n",
      "Precision/Recall/F-score: 0.8235992402659069 / 0.8661240387496255 / 0.8443265345859904\n",
      "14041/14041 [==============================] - 4s 317us/step - loss: 0.0644\n",
      "Epoch 29/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0632\n",
      "Precision/Recall/F-score: 0.8285042694042023 / 0.8624288425047438 / 0.8451262477980035\n",
      "14041/14041 [==============================] - 4s 310us/step - loss: 0.0634\n",
      "Epoch 30/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0628\n",
      "Precision/Recall/F-score: 0.8351195383347073 / 0.8599320882852292 / 0.8473442074445838\n",
      "14041/14041 [==============================] - 5s 341us/step - loss: 0.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0622\n",
      "Precision/Recall/F-score: 0.8357740080228119 / 0.8635274143613303 / 0.8494240734828205\n",
      "14041/14041 [==============================] - 4s 304us/step - loss: 0.0622\n",
      "Epoch 32/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0610\n",
      "Precision/Recall/F-score: 0.8401530192242507 / 0.8663737141715769 / 0.8530619268874302\n",
      "14041/14041 [==============================] - 5s 362us/step - loss: 0.0610\n",
      "Epoch 33/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0605\n",
      "Precision/Recall/F-score: 0.829796325309785 / 0.8727654049735344 / 0.8507386405120592\n",
      "14041/14041 [==============================] - 4s 305us/step - loss: 0.0604\n",
      "Epoch 34/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0597\n",
      "Precision/Recall/F-score: 0.8773286285091588 / 0.8442524717866773 / 0.8604728096292338\n",
      "14041/14041 [==============================] - 4s 288us/step - loss: 0.0596\n",
      "Epoch 35/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0591\n",
      "Precision/Recall/F-score: 0.8282866277976303 / 0.8796564466193948 / 0.853199011962997\n",
      "14041/14041 [==============================] - 4s 309us/step - loss: 0.0591\n",
      "Epoch 36/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0587\n",
      "Precision/Recall/F-score: 0.8709660947712419 / 0.8517427344452212 / 0.8612471598081293\n",
      "14041/14041 [==============================] - 4s 314us/step - loss: 0.0585\n",
      "Epoch 37/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0575\n",
      "Precision/Recall/F-score: 0.8760606839804577 / 0.8506441625886347 / 0.8631653619112766\n",
      "14041/14041 [==============================] - 5s 343us/step - loss: 0.0578\n",
      "Epoch 38/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0571\n",
      "Precision/Recall/F-score: 0.8406619610835558 / 0.880205732547688 / 0.8599795091964678\n",
      "14041/14041 [==============================] - 5s 370us/step - loss: 0.0572\n",
      "Epoch 39/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0563\n",
      "Precision/Recall/F-score: 0.8481731282449415 / 0.872865275142315 / 0.8603420696443952\n",
      "14041/14041 [==============================] - 4s 310us/step - loss: 0.0563\n",
      "Epoch 40/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0555\n",
      "Precision/Recall/F-score: 0.8333802552552553 / 0.8868970338559872 / 0.8593062073636848\n",
      "14041/14041 [==============================] - 5s 359us/step - loss: 0.0559\n",
      "Epoch 41/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0554\n",
      "Precision/Recall/F-score: 0.8502910472891712 / 0.8826026165984221 / 0.8661455908656557\n",
      "14041/14041 [==============================] - 4s 290us/step - loss: 0.0554\n",
      "Epoch 42/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0545\n",
      "Precision/Recall/F-score: 0.8561724171635227 / 0.8748127434335364 / 0.8653922149772771\n",
      "14041/14041 [==============================] - 5s 323us/step - loss: 0.0546\n",
      "Epoch 43/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0540\n",
      "Precision/Recall/F-score: 0.8420778482810872 / 0.8879956057125736 / 0.8644273770173051\n",
      "14041/14041 [==============================] - 4s 295us/step - loss: 0.0540\n",
      "Epoch 44/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0535\n",
      "Precision/Recall/F-score: 0.8907943187795897 / 0.8456007190652152 / 0.8676093862076031\n",
      "14041/14041 [==============================] - 4s 319us/step - loss: 0.0534\n",
      "Epoch 45/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 0.0532\n",
      "Precision/Recall/F-score: 0.8597742707773489 / 0.8787076800159792 / 0.8691378756822166\n",
      "14041/14041 [==============================] - 5s 322us/step - loss: 0.0532\n",
      "Epoch 46/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0523\n",
      "Precision/Recall/F-score: 0.857702918542181 / 0.8848996304803756 / 0.8710890456411139\n",
      "14041/14041 [==============================] - 6s 427us/step - loss: 0.0524\n",
      "Epoch 47/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0520\n",
      "Precision/Recall/F-score: 0.8637675487942083 / 0.8817537201637871 / 0.8726679680743286\n",
      "14041/14041 [==============================] - 5s 375us/step - loss: 0.0519\n",
      "Epoch 48/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0518\n",
      "Precision/Recall/F-score: 0.8575984537327856 / 0.8862478777589134 / 0.8716878269197711\n",
      "14041/14041 [==============================] - 5s 349us/step - loss: 0.0517\n",
      "Epoch 49/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0511\n",
      "Precision/Recall/F-score: 0.8698025205914459 / 0.8753620293618296 / 0.8725734196117472\n",
      "14041/14041 [==============================] - 4s 316us/step - loss: 0.0511\n",
      "Epoch 50/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0509\n",
      "Precision/Recall/F-score: 0.8583413693346191 / 0.8889443723159892 / 0.8733748712162097\n",
      "14041/14041 [==============================] - 5s 361us/step - loss: 0.0507\n",
      "Epoch 51/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0503\n",
      "Precision/Recall/F-score: 0.8678059320369279 / 0.8824528113452512 / 0.8750680861599406\n",
      "14041/14041 [==============================] - 5s 369us/step - loss: 0.0501\n",
      "Epoch 52/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0497\n",
      "Precision/Recall/F-score: 0.8689301416707377 / 0.8881953460501348 / 0.8784571315685501\n",
      "14041/14041 [==============================] - 4s 311us/step - loss: 0.0498\n",
      "Epoch 53/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0495\n",
      "Precision/Recall/F-score: 0.8693737769080235 / 0.8873464496154998 / 0.8782681757524836\n",
      "14041/14041 [==============================] - 5s 374us/step - loss: 0.0495\n",
      "Epoch 54/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0489\n",
      "Precision/Recall/F-score: 0.8672889018521219 / 0.8908918406072106 / 0.8789319407838018\n",
      "14041/14041 [==============================] - 4s 305us/step - loss: 0.0488\n",
      "Epoch 55/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0484\n",
      "Precision/Recall/F-score: 0.8529844024083819 / 0.8984320383501448 / 0.8751185583306986\n",
      "14041/14041 [==============================] - 4s 315us/step - loss: 0.0485\n",
      "Epoch 56/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0487\n",
      "Precision/Recall/F-score: 0.8682271978288262 / 0.8945870368520923 / 0.8812100344318741\n",
      "14041/14041 [==============================] - 4s 311us/step - loss: 0.0486\n",
      "Epoch 57/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0476\n",
      "Precision/Recall/F-score: 0.8919591608675775 / 0.8768600818935384 / 0.8843451766424093\n",
      "14041/14041 [==============================] - 5s 329us/step - loss: 0.0476\n",
      "Epoch 58/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0472\n",
      "Precision/Recall/F-score: 0.8685667119829094 / 0.8932887246579446 / 0.880754271084634\n",
      "14041/14041 [==============================] - 5s 388us/step - loss: 0.0474\n",
      "Epoch 59/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0469\n",
      "Precision/Recall/F-score: 0.9070902107339355 / 0.8726655348047538 / 0.8895449455359871\n",
      "14041/14041 [==============================] - 5s 324us/step - loss: 0.0471\n",
      "Epoch 60/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0470\n",
      "Precision/Recall/F-score: 0.9064578641386896 / 0.8642265055427943 / 0.8848385694930853\n",
      "14041/14041 [==============================] - 5s 365us/step - loss: 0.0471\n",
      "Epoch 61/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0465\n",
      "Precision/Recall/F-score: 0.878441275500099 / 0.8858983321681814 / 0.8821540450499727\n",
      "14041/14041 [==============================] - 5s 370us/step - loss: 0.0463\n",
      "Epoch 62/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0460\n",
      "Precision/Recall/F-score: 0.8698027460839296 / 0.8983821032657545 / 0.8838614591009579\n",
      "14041/14041 [==============================] - 5s 335us/step - loss: 0.0462\n",
      "Epoch 63/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0459\n",
      "Precision/Recall/F-score: 0.8735855470836773 / 0.8982322980125836 / 0.8857374990767413\n",
      "14041/14041 [==============================] - 4s 311us/step - loss: 0.0458\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0457\n",
      "Precision/Recall/F-score: 0.844181826641232 / 0.9060221711774693 / 0.8740094896312531\n",
      "14041/14041 [==============================] - 5s 380us/step - loss: 0.0455\n",
      "Epoch 65/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0455\n",
      "Precision/Recall/F-score: 0.8750365247881562 / 0.8972335963247778 / 0.8859960552268243\n",
      "14041/14041 [==============================] - 4s 311us/step - loss: 0.0455\n",
      "Epoch 66/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0454\n",
      "Precision/Recall/F-score: 0.8611637604245641 / 0.9075202237091781 / 0.8837345003646973\n",
      "14041/14041 [==============================] - 5s 350us/step - loss: 0.0453\n",
      "Epoch 67/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0449\n",
      "Precision/Recall/F-score: 0.9146175578110286 / 0.8729651453110956 / 0.8933060807358202\n",
      "14041/14041 [==============================] - 5s 360us/step - loss: 0.0449\n",
      "Epoch 68/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0445\n",
      "Precision/Recall/F-score: 0.8735632183908046 / 0.9032258064516129 / 0.8881469115191987\n",
      "14041/14041 [==============================] - 5s 324us/step - loss: 0.0444\n",
      "Epoch 69/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0443\n",
      "Precision/Recall/F-score: 0.9152206763587099 / 0.8770598222310996 / 0.8957339929112376\n",
      "14041/14041 [==============================] - 5s 329us/step - loss: 0.0441\n",
      "Epoch 70/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0439\n",
      "Precision/Recall/F-score: 0.8783269961977186 / 0.8997303505442924 / 0.8888998519980265\n",
      "14041/14041 [==============================] - 5s 340us/step - loss: 0.0441\n",
      "Epoch 71/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0437\n",
      "Precision/Recall/F-score: 0.882970889990673 / 0.8981823629281933 / 0.8905116716587864\n",
      "14041/14041 [==============================] - 4s 313us/step - loss: 0.0438\n",
      "Epoch 72/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0434\n",
      "Precision/Recall/F-score: 0.8859090235590458 / 0.8956856087086787 / 0.8907704913962208\n",
      "14041/14041 [==============================] - 4s 311us/step - loss: 0.0434\n",
      "Epoch 73/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0430\n",
      "Precision/Recall/F-score: 0.9142042509072058 / 0.8806052132228104 / 0.8970902431580019\n",
      "14041/14041 [==============================] - 5s 323us/step - loss: 0.0431\n",
      "Epoch 74/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0424\n",
      "Precision/Recall/F-score: 0.9134655101197852 / 0.883451513033057 / 0.8982078489110017\n",
      "14041/14041 [==============================] - 4s 308us/step - loss: 0.0427\n",
      "Epoch 75/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0426\n",
      "Precision/Recall/F-score: 0.8857508995021933 / 0.8973834015779487 / 0.8915292074910083\n",
      "14041/14041 [==============================] - 4s 319us/step - loss: 0.0429\n",
      "Epoch 76/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0423\n",
      "Precision/Recall/F-score: 0.8787425149700598 / 0.9086687306501547 / 0.893455098934551\n",
      "14041/14041 [==============================] - 6s 423us/step - loss: 0.0425\n",
      "Epoch 77/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 0.0424\n",
      "Precision/Recall/F-score: 0.882682112703711 / 0.8979326875062419 / 0.8902420911926333\n",
      "14041/14041 [==============================] - 4s 316us/step - loss: 0.0424\n",
      "Epoch 78/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0423\n",
      "Precision/Recall/F-score: 0.882464685825621 / 0.9046739238989314 / 0.8934313048624125\n",
      "14041/14041 [==============================] - 5s 337us/step - loss: 0.0423\n",
      "Epoch 79/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0419\n",
      "Precision/Recall/F-score: 0.8813962520633071 / 0.9065714571057625 / 0.8938066167782591\n",
      "14041/14041 [==============================] - 5s 328us/step - loss: 0.0419\n",
      "Epoch 80/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0415\n",
      "Precision/Recall/F-score: 0.8775097501083345 / 0.910066913013083 / 0.8934918494913593\n",
      "14041/14041 [==============================] - 4s 310us/step - loss: 0.0418\n",
      "Epoch 81/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0417\n",
      "Precision/Recall/F-score: 0.8733234690468236 / 0.9136622390891841 / 0.8930375576542939\n",
      "14041/14041 [==============================] - 4s 313us/step - loss: 0.0417\n",
      "Epoch 82/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0415\n",
      "Precision/Recall/F-score: 0.8781158092666699 / 0.9094676920003995 / 0.8935168150710133\n",
      "14041/14041 [==============================] - 4s 316us/step - loss: 0.0417\n",
      "Epoch 83/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0415\n",
      "Precision/Recall/F-score: 0.8822528914374574 / 0.9065714571057625 / 0.894246872229337\n",
      "14041/14041 [==============================] - 4s 312us/step - loss: 0.0415\n",
      "Epoch 84/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0414\n",
      "Precision/Recall/F-score: 0.8785425101214575 / 0.9102167182662538 / 0.8940991808505421\n",
      "14041/14041 [==============================] - 5s 324us/step - loss: 0.0413\n",
      "Epoch 85/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0410\n",
      "Precision/Recall/F-score: 0.8798586572438163 / 0.9076700289623489 / 0.8935479906599484\n",
      "14041/14041 [==============================] - 4s 308us/step - loss: 0.0410\n",
      "Epoch 86/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0412\n",
      "Precision/Recall/F-score: 0.9084209991360471 / 0.8925896334764806 / 0.9004357353348613\n",
      "14041/14041 [==============================] - 5s 333us/step - loss: 0.0412\n",
      "Epoch 87/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0410\n",
      "Precision/Recall/F-score: 0.9065847591003706 / 0.8916908019574553 / 0.8990761020063943\n",
      "14041/14041 [==============================] - 4s 303us/step - loss: 0.0409\n",
      "Epoch 88/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0409\n",
      "Precision/Recall/F-score: 0.8845854796384488 / 0.9089683411564966 / 0.8966111713131711\n",
      "14041/14041 [==============================] - 4s 308us/step - loss: 0.0410\n",
      "Epoch 89/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0405\n",
      "Precision/Recall/F-score: 0.9119815668202765 / 0.8893937880755018 / 0.9005460612802104\n",
      "14041/14041 [==============================] - 4s 301us/step - loss: 0.0405\n",
      "Epoch 90/100\n",
      "13200/14041 [===========================>..] - ETA: 0s - loss: 0.0402\n",
      "Precision/Recall/F-score: 0.9261170212765958 / 0.8694197543193848 / 0.8968732292793489\n",
      "14041/14041 [==============================] - 4s 312us/step - loss: 0.0404\n",
      "Epoch 91/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0404\n",
      "Precision/Recall/F-score: 0.8894630641271716 / 0.9024767801857585 / 0.8959226669971496\n",
      "14041/14041 [==============================] - 5s 348us/step - loss: 0.0403\n",
      "Epoch 92/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0403\n",
      "Precision/Recall/F-score: 0.9033896600281633 / 0.8969839209028263 / 0.9001753946379354\n",
      "14041/14041 [==============================] - 4s 275us/step - loss: 0.0403\n",
      "Epoch 93/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0402\n",
      "Precision/Recall/F-score: 0.8888943272478097 / 0.9068710676121042 / 0.8977927181946265\n",
      "14041/14041 [==============================] - 4s 295us/step - loss: 0.0402\n",
      "Epoch 94/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0402\n",
      "Precision/Recall/F-score: 0.8813878299474014 / 0.9120643163886947 / 0.8964637169010283\n",
      "14041/14041 [==============================] - 5s 338us/step - loss: 0.0401\n",
      "Epoch 95/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0395\n",
      "Precision/Recall/F-score: 0.9161170651277823 / 0.8878458004594028 / 0.9017599026221028\n",
      "14041/14041 [==============================] - 4s 292us/step - loss: 0.0395\n",
      "Epoch 96/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0396\n",
      "Precision/Recall/F-score: 0.8811842993538432 / 0.9125137321482073 / 0.8965754096752037\n",
      "14041/14041 [==============================] - 4s 302us/step - loss: 0.0397\n",
      "Epoch 97/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0395\n",
      "Precision/Recall/F-score: 0.887058250060931 / 0.9087186657345451 / 0.897757825411312\n",
      "14041/14041 [==============================] - 5s 367us/step - loss: 0.0395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0395\n",
      "Precision/Recall/F-score: 0.8865501074428599 / 0.9064715869369819 / 0.896400177768999\n",
      "14041/14041 [==============================] - 5s 337us/step - loss: 0.0396\n",
      "Epoch 99/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0396\n",
      "Precision/Recall/F-score: 0.8894181193223668 / 0.9044741835613702 / 0.8968829689782378\n",
      "14041/14041 [==============================] - 4s 317us/step - loss: 0.0396\n",
      "Epoch 100/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0395\n",
      "Precision/Recall/F-score: 0.9203599317370843 / 0.8886946968940378 / 0.9042501841831162\n",
      "14041/14041 [==============================] - 4s 308us/step - loss: 0.0393\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "cnn = Conv1D(100,3, activation='relu', padding='same')(embeddings)\n",
    "outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(cnn)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=1,epochs=100, callbacks=[EvaluateEntities()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 113, 100)          160400    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 113, 4)            404       \n",
      "=================================================================\n",
      "Total params: 15,161,404\n",
      "Trainable params: 160,804\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 4.3386\n",
      "Precision/Recall/F-score: 0.3169291338582677 / 0.03215819434734845 / 0.058391513283162576\n",
      "14041/14041 [==============================] - 155s 11ms/step - loss: 4.3347\n",
      "Epoch 2/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 2.1341\n",
      "Precision/Recall/F-score: 0.6756756756756757 / 0.49435733546389693 / 0.5709671838052945\n",
      "14041/14041 [==============================] - 156s 11ms/step - loss: 2.1341\n",
      "Epoch 3/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.4414\n",
      "Precision/Recall/F-score: 0.7088869205786832 / 0.5994706881054629 / 0.6496036362652525\n",
      "14041/14041 [==============================] - 155s 11ms/step - loss: 1.4415\n",
      "Epoch 4/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.2324\n",
      "Precision/Recall/F-score: 0.7116390584132519 / 0.6521522021372216 / 0.680598259419459\n",
      "14041/14041 [==============================] - 157s 11ms/step - loss: 1.2321\n",
      "Epoch 5/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.1217\n",
      "Precision/Recall/F-score: 0.7215482070002147 / 0.6711774692899231 / 0.6954519584001656\n",
      "14041/14041 [==============================] - 156s 11ms/step - loss: 1.1214\n",
      "Epoch 6/100\n",
      " 5300/14041 [==========>...................] - ETA: 46s - loss: 1.0870"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "rnn = LSTM(100, activation='tanh', return_sequences=True)(embeddings)\n",
    "outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(rnn)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=1,epochs=100, callbacks=[EvaluateEntities()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
