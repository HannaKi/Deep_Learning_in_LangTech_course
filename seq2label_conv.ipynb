{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Label with conv-nets\n",
    "\n",
    "## Goals of the lecture\n",
    "\n",
    "1. Understand seq2label problem setting\n",
    "2. Understanding how 1d-convolution and its practicalities work\n",
    "3. Acquiring capability of implementing seq2label models with conv-nets on keras\n",
    "\n",
    "## Outline\n",
    "\n",
    "0. Intro\n",
    "1. Few words about seq2label, data and its form\n",
    "2. Detailed look into a simple example model\n",
    "3. Look into a more complicated model\n",
    "4. Look into what might be happening here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2label\n",
    "\n",
    "\n",
    "### Recap on Bag-Of-Words\n",
    "\n",
    "\n",
    "#### Input\n",
    "\n",
    "As was previously discussed, when we are bulding a bag of words - classifier, our input is a set of unordered words and the output is a single label. \n",
    "\n",
    "The list of words is given to the model as a feature vector, a single numerical vector. So, to repeat its input is a single vector.\n",
    "\n",
    "To do this transformation, all tokens need to have a numerical identification. That is, each word is associated with a specific number. In practice this is retrieved from a vocabulary as was shown before.\n",
    "\n",
    "### Output\n",
    "\n",
    "Its output is a label. As such it is a function from a vector into a label.\n",
    "\n",
    "Labels are typically represented as a vector, which has a value for all possible labels, or a single value if the task is binary.\n",
    "\n",
    "### How Seq2Label differs from this?\n",
    "\n",
    "The only difference from the bow is the form of the input.\n",
    "\n",
    "Like in the bow example, the input is a single vector, but instead of representing a set, it represents a sequence. This is achieved simply by adding the token identifiers after each other.\n",
    "\n",
    "The output is identical.\n",
    "\n",
    "### A Simple Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : essential\n",
      "1 : it\n",
      "2 : mat\n",
      "3 : on\n",
      "4 : the\n",
      "5 : is\n",
      "6 : cat\n",
      "7 : sat\n"
     ]
    }
   ],
   "source": [
    "#Let's define a simple and short vocabulary\n",
    "tokens = list(set('it is essential the cat sat on the mat'.split()))\n",
    "token_ids = {}\n",
    "for i,t in enumerate(tokens):\n",
    "    print (i,':',t)\n",
    "    token_ids[t] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'sat', 'on', 'the', 'mat']\n"
     ]
    }
   ],
   "source": [
    "#Let's define a sentence we want to turn into a vector form\n",
    "\n",
    "sentence = 'cat sat on the mat'.split()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Bag of words vector\n",
    "import numpy as np\n",
    "\n",
    "#Vector is as long as the vocabulary, initial values are zeros\n",
    "vector = np.zeros((len(token_ids),))\n",
    "for token in sentence:\n",
    "    vector[token_ids[token]] = 1.0\n",
    "print (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 3, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "#Sequential input vector\n",
    "\n",
    "vector = [token_ids[t] for t in sentence]\n",
    "print (vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding of the data\n",
    "\n",
    "Some deep-learning frameworks, such as keras, require the input to have a preset shape which is the same for all examples. This is not the case with all frameworks, but its good to cover. If for nothing else, for the sake of this tutorial.\n",
    "\n",
    "Since these input sequences are not the same length, we need to make them so. This is accomplished by padding, which in practice means adding zeros to the input vectors to make them uniform in size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 7, 3, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Padding can easily be achieved with keras in-built functions\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "pad_sequences(np.array([vector]), maxlen=20, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super short recap on embedding layers\n",
    "\n",
    "Most often these sequences of numbers are used to load embeddings, which are used as the input of the next stage of the network. An embedding layer could be imagined as a dictionary; in go indexes, out comes representative embeddings.\n",
    "\n",
    "What happens inside an embedding layer can be demonstrated with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: [6, 7, 3, 4, 2]\n",
      "into:\n",
      "[[0.85038981 0.40489633 0.12422465 0.92529823]\n",
      " [0.21168522 0.15332721 0.75282947 0.5283064 ]\n",
      " [0.75143017 0.27964974 0.9766692  0.4946047 ]\n",
      " [0.89808113 0.43437026 0.10011865 0.18511744]\n",
      " [0.62647722 0.12335613 0.45529044 0.30627977]]\n",
      "and the matrix has a shape: (5, 4)\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 4\n",
    "#Let's just make a random matrix to act as our demonstration embedding matrix\n",
    "embedding_matrix = np.random.rand(len(token_ids),embedding_size)\n",
    "\n",
    "print ('from:', vector)\n",
    "print ('into:')\n",
    "print (embedding_matrix[vector])\n",
    "print ('and the matrix has a shape:', embedding_matrix[vector].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution recap\n",
    "\n",
    "### What's good about convolution?\n",
    "\n",
    "Convolutional networks are fast. They are fast because they are easily parallelizable. That is because no part of the convolution output depends on another one of the outputs. If that was the case, as it is in recurrent neural networks we couldn't make predictions before other parts of the layer have finished calculating.\n",
    "\n",
    "### 1-dimensional convolution\n",
    "\n",
    "During he earlier lecture and also to a large degree on the internet, image classification is used to demonstrate conv-nets. That is very understandable, for that is the area in which these networks traditionally shine and also because the convolution operation is a traditional tool for image processing.\n",
    "\n",
    "An image is a two dimensional input. It has height and it has width. A sequence of words has only one dimension. That is, you can traverse it forwards and backwards, but there is no traversing it \"up\" or \"down\" nor is there \"toward\" and \"from\" etc. It just is a one-dimensional sequence of tokens.\n",
    "\n",
    "Because of this, the convolution operation applied to text is usally one dimensional, instead of two dimensional convolution often applied to images.\n",
    "\n",
    "The relevant variables in the operation are:\n",
    "\n",
    "1. Kernel size, in effect how many conv operations are applied to the input\n",
    "2. Filter size, in effect how wide the applied conv operations are\n",
    "3. Stride length\n",
    "4. Padding\n",
    "\n",
    "#### Input / Output\n",
    "\n",
    "This operation takes as its input 2-d matrices (when we discount the batch_size). The matrix is in the shape of (steps, input_dim)\n",
    "\n",
    "The operation returns similarly a 2-d matrix. The size of the output matrix is (new_steps, filters). In this shape filters corresponds to the amount of filters. The new step count is a little bit trickier. It depends on stride length, kernel size and used padding style. \n",
    "https://keras.io/layers/convolutional/#conv1d\n",
    "\n",
    "\n",
    "### Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Untitled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-dimensional convolution and text\n",
    "\n",
    "Since the convolutional network bases its activations on a window of certain length, it is reacting to snippets of text the size of the convolution window. For example a conv-net with a window size of three, is capable of reacting to trigrams etc. In this sense convolutional text classifiers remind us of bag-of-ngram classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "The pooling operation is used for dimensionality reduction of the input. The type of pooling discussed in this lecture is max-pooling, but other variants exist. In the context of this lecture we will be using global pooling, similarly variants exist.\n",
    "\n",
    "Global, 1 dimensional, max pooling operation takes as its input a 2-d matrix and returns a vector. \n",
    "\n",
    "This operation is best explained with an illustration.\n",
    "\n",
    "### Illustration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we start with:\n",
      "[[0.85038981 0.40489633 0.12422465 0.92529823]\n",
      " [0.21168522 0.15332721 0.75282947 0.5283064 ]\n",
      " [0.75143017 0.27964974 0.9766692  0.4946047 ]\n",
      " [0.89808113 0.43437026 0.10011865 0.18511744]\n",
      " [0.62647722 0.12335613 0.45529044 0.30627977]]\n",
      "\n",
      "after max pooling:\n",
      "[0.89808113 0.43437026 0.9766692  0.92529823]\n"
     ]
    }
   ],
   "source": [
    "#We can illustrate global max pooling with numpy\n",
    "embeddings = embedding_matrix[vector]\n",
    "print ('we start with:')\n",
    "print (embeddings)\n",
    "print ()\n",
    "print ('after max pooling:')\n",
    "print (np.max(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "Rectified linear unit. An activation used often with conv-nets. As an activation it doesn't care about input size. Input and output are always of similar shape. Simply returns the value if it is >0 and 0 otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simple model\n",
    "\n",
    "To start of this lecture, let us look into an example convnet-text classifier included in keras examples: https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py\n",
    "\n",
    "It uses the IMDB-dataset already we are already familiar with and contains a single convolution layer. It's performance is said to be 0.89, which is somewhat disappointingly less than the bag-of-words example earlier.\n",
    "\n",
    "In addition to demonstrating a convolutional text classifier, the example nicely illuminates what typical deep-learning code looks like.\n",
    "\n",
    "For purposes of simplification I've yet very slightly simplified the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 83s 3ms/step - loss: 0.4066 - acc: 0.8100 - val_loss: 0.2944 - val_acc: 0.8770\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 83s 3ms/step - loss: 0.2289 - acc: 0.9094 - val_loss: 0.2716 - val_acc: 0.8874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1b8695e80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This example demonstrates the use of Convolution1D for text classification.\n",
    "Gets to 0.89 test accuracy after 2 epochs.\n",
    "90s/epoch on Intel i5 2.4Ghz CPU.\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 310.00 387.00\" width=\"310pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 306,-383 306,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140676957967752 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140676957967752</title>\n",
       "<polygon fill=\"none\" points=\"43.5,-332.5 43.5,-378.5 258.5,-378.5 258.5,-332.5 43.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-351.8\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"120.5,-332.5 120.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"120.5,-355.5 175.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"175.5,-332.5 175.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"217\" y=\"-363.3\">(None, 400)</text>\n",
       "<polyline fill=\"none\" points=\"175.5,-355.5 258.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"217\" y=\"-340.3\">(None, 400)</text>\n",
       "</g>\n",
       "<!-- 140676301067544 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140676301067544</title>\n",
       "<polygon fill=\"none\" points=\"31.5,-249.5 31.5,-295.5 270.5,-295.5 270.5,-249.5 31.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"71.5\" y=\"-268.8\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"111.5,-249.5 111.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"139\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"111.5,-272.5 166.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"139\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"166.5,-249.5 166.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218.5\" y=\"-280.3\">(None, 400)</text>\n",
       "<polyline fill=\"none\" points=\"166.5,-272.5 270.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218.5\" y=\"-257.3\">(None, 400, 50)</text>\n",
       "</g>\n",
       "<!-- 140676957967752&#45;&gt;140676301067544 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140676957967752-&gt;140676301067544</title>\n",
       "<path d=\"M151,-332.366C151,-324.152 151,-314.658 151,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-305.607 151,-295.607 147.5,-305.607 154.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140677428177888 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140677428177888</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-166.5 37.5,-212.5 264.5,-212.5 264.5,-166.5 37.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68.5\" y=\"-185.8\">Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"99.5,-166.5 99.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"99.5,-189.5 154.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"154.5,-166.5 154.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"209.5\" y=\"-197.3\">(None, 400, 50)</text>\n",
       "<polyline fill=\"none\" points=\"154.5,-189.5 264.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"209.5\" y=\"-174.3\">(None, 398, 250)</text>\n",
       "</g>\n",
       "<!-- 140676301067544&#45;&gt;140677428177888 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140676301067544-&gt;140677428177888</title>\n",
       "<path d=\"M151,-249.366C151,-241.152 151,-231.658 151,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-222.607 151,-212.607 147.5,-222.607 154.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140677428178000 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140677428178000</title>\n",
       "<polygon fill=\"none\" points=\"0,-83.5 0,-129.5 302,-129.5 302,-83.5 0,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68.5\" y=\"-102.8\">GlobalMaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"137,-83.5 137,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"137,-106.5 192,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"192,-83.5 192,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-114.3\">(None, 398, 250)</text>\n",
       "<polyline fill=\"none\" points=\"192,-106.5 302,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-91.3\">(None, 250)</text>\n",
       "</g>\n",
       "<!-- 140677428177888&#45;&gt;140677428178000 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140677428177888-&gt;140677428178000</title>\n",
       "<path d=\"M151,-166.366C151,-158.152 151,-148.658 151,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-139.607 151,-129.607 147.5,-139.607 154.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140676300799280 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140676300799280</title>\n",
       "<polygon fill=\"none\" points=\"57,-0.5 57,-46.5 245,-46.5 245,-0.5 57,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"107,-0.5 107,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"107,-23.5 162,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"162,-0.5 162,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.5\" y=\"-31.3\">(None, 250)</text>\n",
       "<polyline fill=\"none\" points=\"162,-23.5 245,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.5\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 140677428178000&#45;&gt;140676300799280 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140677428178000-&gt;140676300799280</title>\n",
       "<path d=\"M151,-83.3664C151,-75.1516 151,-65.6579 151,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-56.6068 151,-46.6068 147.5,-56.6069 154.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model,show_shapes=True,show_layer_names=False).create(prog='dot', format='svg'))\n",
    "#model_to_dot(model,show_shapes=True,show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we were to load pre-trained embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sigfried': 54043,\n",
       " 'hinge': 72849,\n",
       " 'baddiel': 44068,\n",
       " 'zemen': 83986,\n",
       " 'joycelyn': 26813,\n",
       " \"evening's\": 20485,\n",
       " 'transistions': 60115,\n",
       " 'emitting': 35264,\n",
       " 'sheritt': 78155,\n",
       " 'pitchforks': 50581,\n",
       " 'hayseeds': 68154,\n",
       " 'dains': 84537,\n",
       " 'veneration': 67401,\n",
       " 'train': 1371,\n",
       " \"'andres\": 59091,\n",
       " \"call'\": 87980,\n",
       " 'mins': 7397,\n",
       " 'displease': 68626,\n",
       " 'snuck': 16289,\n",
       " 'emphasize': 8551,\n",
       " 'speach': 77750,\n",
       " 'indiscernible': 31036,\n",
       " 'predicted': 8614,\n",
       " 'lowlights': 51982,\n",
       " 'roget': 63571,\n",
       " 'reloading': 30084,\n",
       " 'maximals': 85192,\n",
       " 'orin': 83069,\n",
       " 'nardini': 80432,\n",
       " 'snatchers': 16814,\n",
       " 'excluding': 16172,\n",
       " 'widening': 30443,\n",
       " 'definately': 11675,\n",
       " \"'edison'\": 48514,\n",
       " 'beckoned': 57148,\n",
       " 'duce': 38480,\n",
       " 'campmates': 75850,\n",
       " 'medioacre': 70573,\n",
       " 'shya': 75867,\n",
       " 'f430': 61443,\n",
       " 'portable': 21028,\n",
       " 'milwaukee': 24583,\n",
       " 'poll': 31523,\n",
       " 'thembrians': 44575,\n",
       " 'leste': 82651,\n",
       " 'replacement': 7650,\n",
       " 'oblige': 46688,\n",
       " 'theresa': 12453,\n",
       " 'carousing': 45884,\n",
       " 'vandermey': 68049,\n",
       " 'cellist': 62180,\n",
       " 'colm': 18667,\n",
       " 'up': 53,\n",
       " \"gallo's\": 50075,\n",
       " 'snippit': 77351,\n",
       " 'lucina': 60936,\n",
       " 'piece': 415,\n",
       " 'preggo': 53074,\n",
       " 'bratt': 53569,\n",
       " \"silverman's\": 87693,\n",
       " 'alternations': 69448,\n",
       " 'carts': 25397,\n",
       " 'keren': 69036,\n",
       " 'premedical': 60340,\n",
       " 'advises': 16529,\n",
       " 'locales': 9110,\n",
       " 'calvin': 11559,\n",
       " 'kohara': 61490,\n",
       " 'sympathise': 13502,\n",
       " 'rebukes': 79150,\n",
       " \"ferdie's\": 51429,\n",
       " 'accords': 50967,\n",
       " \"mayer's\": 39742,\n",
       " 'pseudonyms': 66114,\n",
       " 'outsmarted': 27206,\n",
       " 'gabriella': 19638,\n",
       " 'infirmed': 53697,\n",
       " \"downey's\": 40086,\n",
       " \"spieberg's\": 54432,\n",
       " 'croons': 25739,\n",
       " 'robe': 13194,\n",
       " 'regulations': 21656,\n",
       " 'grievances': 29450,\n",
       " 'farnel': 59664,\n",
       " 'bartlett': 39981,\n",
       " 'revealing': 3653,\n",
       " 'lazer': 44179,\n",
       " 'stinson': 77984,\n",
       " 'kiyoshi': 37213,\n",
       " 'squeal': 20611,\n",
       " 'freeeeee': 81516,\n",
       " 'losing': 2325,\n",
       " 'overexplanation': 59819,\n",
       " 'bulwark': 51619,\n",
       " 'use': 358,\n",
       " 'dispised': 56911,\n",
       " 'conehead': 82760,\n",
       " 'lacy': 37675,\n",
       " 'cbn': 45321,\n",
       " 'wrung': 42507,\n",
       " \"travesty's\": 54760,\n",
       " 'kathmandu': 23698,\n",
       " 'heidi': 20038,\n",
       " 'vacuity': 29515,\n",
       " 'wouk': 47430,\n",
       " \"'peurile'\": 67359,\n",
       " 'pastures': 22018,\n",
       " 'unsatisfactorily': 73995,\n",
       " 'quibble': 13001,\n",
       " 'furnishing': 32764,\n",
       " \"'furniture'\": 77120,\n",
       " \"'dumb\": 42839,\n",
       " 'occidental': 31131,\n",
       " 'individualist': 36576,\n",
       " 'hammond': 16130,\n",
       " 'chio': 53433,\n",
       " 'diminishing': 18512,\n",
       " 'como': 27576,\n",
       " 'garris': 24785,\n",
       " \"idea'\": 56559,\n",
       " 'cassandras': 74305,\n",
       " 'reaaaal': 78105,\n",
       " 'saiba': 60550,\n",
       " \"youth's\": 41858,\n",
       " 'delon': 12305,\n",
       " 'daena': 42179,\n",
       " 'bombarded': 23266,\n",
       " 'trotti': 31514,\n",
       " 'nakedly': 79418,\n",
       " 'keenly': 20610,\n",
       " 'reshoskys': 63394,\n",
       " 'donated': 18600,\n",
       " 'ebbs': 41823,\n",
       " 'asbury': 64812,\n",
       " 'sweating': 15850,\n",
       " 'tacones': 38510,\n",
       " 'amidst': 6926,\n",
       " 'obese': 14361,\n",
       " 'mumford': 51263,\n",
       " 'deirdre': 47547,\n",
       " 'lurch': 20655,\n",
       " 'bred': 16127,\n",
       " 'kristevian': 54574,\n",
       " 'hangdog': 45162,\n",
       " 'broken': 1909,\n",
       " 'arousal': 46470,\n",
       " 'forgotten': 1548,\n",
       " 'typing\\x85': 63903,\n",
       " \"characters'\": 4392,\n",
       " 'humility': 14275,\n",
       " 'worshipping': 25877,\n",
       " 'bellevue': 60200,\n",
       " 'valenteen': 53659,\n",
       " \"bettie's\": 13031,\n",
       " 'chacotero': 80857,\n",
       " \"roy'\": 81462,\n",
       " 'clueless': 5750,\n",
       " 'nabbing': 68984,\n",
       " \"konvitz'\": 45179,\n",
       " 'chandeliers': 82709,\n",
       " 'presences\\x85': 63642,\n",
       " 'altamont': 32875,\n",
       " \"bobb'e\": 45285,\n",
       " 'ritchie': 6956,\n",
       " 'pojar': 44667,\n",
       " 'montreal': 23256,\n",
       " 'corben': 55520,\n",
       " 'fanboy': 21022,\n",
       " 'townfolks': 74692,\n",
       " \"dead's\": 25443,\n",
       " 'bright': 1924,\n",
       " 'yamashiro': 60522,\n",
       " 'labirinto': 77672,\n",
       " 'bogeyman': 21799,\n",
       " 'unpretentious': 11374,\n",
       " 'rickie': 43378,\n",
       " 'victoriously': 49631,\n",
       " 'livingstone': 26176,\n",
       " 'whitewash': 24453,\n",
       " 'balzac': 38771,\n",
       " 'plebeianism': 54628,\n",
       " 'jog': 22099,\n",
       " 'carhart': 41293,\n",
       " 'kar': 17487,\n",
       " 'rabbi': 26904,\n",
       " 'mcmahon': 12274,\n",
       " 'bredon': 65724,\n",
       " \"saleem's\": 79470,\n",
       " 'heth': 78964,\n",
       " 'parasite': 16419,\n",
       " 'algerians': 61311,\n",
       " \"macbeth's\": 47416,\n",
       " \"writers'\": 21091,\n",
       " 'pony': 11644,\n",
       " 'annihilator': 68587,\n",
       " 'weighing': 28358,\n",
       " 'abdul': 37090,\n",
       " 'thatz': 57020,\n",
       " 'abner': 53732,\n",
       " \"'gangster\": 77051,\n",
       " 'hooted': 56985,\n",
       " 'infamously': 30476,\n",
       " 'wui': 72578,\n",
       " 'mountie': 25324,\n",
       " 'bihar': 84450,\n",
       " \"'perfect'\": 85520,\n",
       " 'wickes': 29269,\n",
       " 'tuberculosis': 22949,\n",
       " '‘lifer’': 75725,\n",
       " 'nausica': 76886,\n",
       " 'captian': 53613,\n",
       " 'silliness': 5293,\n",
       " 'castigated': 88205,\n",
       " 'moonraker': 36418,\n",
       " 'borga': 32550,\n",
       " 'wrecked': 16158,\n",
       " 'racist': 2755,\n",
       " \"attanborough's\": 53735,\n",
       " 'rending': 23668,\n",
       " 'conniving': 8803,\n",
       " 'pronouncements': 46821,\n",
       " 'femininity': 17718,\n",
       " \"chopra's\": 39924,\n",
       " '90': 1549,\n",
       " 'asmat': 67551,\n",
       " 'virgil': 23137,\n",
       " 'analysts': 69182,\n",
       " 'palusky': 87649,\n",
       " 'lessons': 3077,\n",
       " 'nouvelle': 19872,\n",
       " 'email': 9490,\n",
       " 'minorities': 16862,\n",
       " 'rosetti': 49559,\n",
       " \"croc's\": 85942,\n",
       " 'kridge': 43748,\n",
       " 'dopiest': 61201,\n",
       " \"america'\": 53771,\n",
       " 'townsfolk': 11521,\n",
       " 'comdey': 41681,\n",
       " 'moshimo': 62198,\n",
       " 'manitoba': 28907,\n",
       " 'ones\\x85': 72233,\n",
       " 'smut': 17392,\n",
       " 'leeched': 84101,\n",
       " 'ehh': 35369,\n",
       " \"'neighbours'\": 45086,\n",
       " 'integrated': 9811,\n",
       " \"lovers'\": 26235,\n",
       " 'bightman': 77552,\n",
       " \"'shock'\": 28867,\n",
       " 'scryeeee': 51946,\n",
       " 'demoralize': 85886,\n",
       " 'blushy': 45925,\n",
       " 'primatologists': 73148,\n",
       " 'allance': 87983,\n",
       " 'marching': 11785,\n",
       " 'strings': 5846,\n",
       " \"santoshi's\": 77758,\n",
       " 'ilsa': 27372,\n",
       " 'genuingly': 67373,\n",
       " 'possessing': 13714,\n",
       " 'covetous': 80984,\n",
       " 'kitano': 37615,\n",
       " 'catacombs': 38187,\n",
       " \"1981's\": 87522,\n",
       " \"bianlian's\": 86916,\n",
       " 'pepa': 72972,\n",
       " 'gerde': 88282,\n",
       " 'sauraus': 46489,\n",
       " 'vlkava': 73591,\n",
       " 'smoothness': 39199,\n",
       " 'violated': 13506,\n",
       " 'activating': 40858,\n",
       " 'saito': 35717,\n",
       " \"bentley's\": 67552,\n",
       " 'quiet': 1855,\n",
       " \"orca's\": 71702,\n",
       " 'harrow': 43735,\n",
       " 'mug': 14169,\n",
       " \"lefler's\": 69076,\n",
       " 'blandly': 18213,\n",
       " 'reposition': 60035,\n",
       " 'lowpoints': 59872,\n",
       " 'pazu': 11554,\n",
       " 'helped\\x85': 57333,\n",
       " \"spillane's\": 37893,\n",
       " 'perceive': 9449,\n",
       " 'clings': 40321,\n",
       " 'teacher': 1747,\n",
       " 'thither': 57935,\n",
       " 'sockets': 23118,\n",
       " 'dunstan': 85003,\n",
       " 'getgo': 78882,\n",
       " 'squadrons': 85513,\n",
       " 'bainter': 46584,\n",
       " 'pandas': 85797,\n",
       " 'harshly': 16920,\n",
       " \"'ice\": 34418,\n",
       " 'michelangelo': 24872,\n",
       " 'unerring': 34782,\n",
       " 'yamaha': 53297,\n",
       " \"'jedna\": 63588,\n",
       " 'intuitively': 27295,\n",
       " 'denotes': 29161,\n",
       " \"umpire's\": 70216,\n",
       " 'disband': 66502,\n",
       " 'indecently': 48214,\n",
       " 'porkys': 46836,\n",
       " 'thesiger': 20188,\n",
       " 'scuffles': 78620,\n",
       " 'yearnings': 34984,\n",
       " 'bloodthirst': 65470,\n",
       " 'merciless': 12629,\n",
       " 'demolish': 36646,\n",
       " 'rantzen': 35618,\n",
       " \"beretta's\": 58570,\n",
       " 'whooped': 83542,\n",
       " 'stales': 56040,\n",
       " 'krecmer': 86966,\n",
       " 'dematerializing': 73405,\n",
       " 'perjury': 88364,\n",
       " 'kammerud': 54890,\n",
       " 'cessation': 46632,\n",
       " 'smartassy': 65617,\n",
       " 'cohl': 68137,\n",
       " 'incoming': 23897,\n",
       " 'displacement': 39368,\n",
       " 'chiara': 40866,\n",
       " 'boschi': 42315,\n",
       " 'khamini': 57814,\n",
       " \"giancarlo's\": 64610,\n",
       " \"tierneys'\": 64013,\n",
       " \"columbu's\": 64023,\n",
       " 'blind': 2031,\n",
       " 'blurr': 74325,\n",
       " 'everingham': 64761,\n",
       " 'ermey': 49423,\n",
       " 'biopics': 16854,\n",
       " 'monumentally': 22736,\n",
       " 'pretension': 11829,\n",
       " 'conscription': 71058,\n",
       " 'yeeshhhhhhhhhhhhhhhhh': 83396,\n",
       " 'envelops': 30297,\n",
       " 'sandefur': 59263,\n",
       " 'unswerving': 42463,\n",
       " 'souffle': 61612,\n",
       " 'yards': 13538,\n",
       " 'feedbacks': 36805,\n",
       " \"steckler's\": 74988,\n",
       " \"mazovia's\": 61823,\n",
       " 'slammer': 28641,\n",
       " 'choking': 18165,\n",
       " 'leit': 52558,\n",
       " 'aïssa': 41703,\n",
       " 'zack': 13085,\n",
       " 'performance\\x85even': 72431,\n",
       " 'drea': 42824,\n",
       " 'trends': 15928,\n",
       " \"'memorial\": 60895,\n",
       " 'starlets': 19683,\n",
       " 'duties': 9616,\n",
       " 'mudd': 72489,\n",
       " 'circumcision': 44486,\n",
       " 'cashback': 41401,\n",
       " 'dancey': 63941,\n",
       " \"leader's\": 52161,\n",
       " 'tolerant': 13248,\n",
       " 'retrouvé': 76564,\n",
       " 'mysterious\\x85': 53526,\n",
       " \"rossi's\": 43231,\n",
       " 'frack': 74599,\n",
       " '\\x96organized': 67576,\n",
       " 'projects': 3448,\n",
       " 'wade': 16609,\n",
       " 'décor': 37732,\n",
       " \"harry'\": 26857,\n",
       " 'shoenumber': 79245,\n",
       " 'balsmeyer': 80963,\n",
       " 'idk': 34383,\n",
       " 'guiding': 12407,\n",
       " 'demerit': 45236,\n",
       " 'lacking': 1889,\n",
       " \"chekhov's\": 58698,\n",
       " 'gazillion': 82839,\n",
       " \"kagan's\": 44419,\n",
       " 'dread': 6246,\n",
       " 'zeman': 50881,\n",
       " 'toker': 43022,\n",
       " 'atenborough': 36870,\n",
       " 'bricklayers': 63743,\n",
       " 'camára': 84482,\n",
       " 'nra': 27700,\n",
       " 'explodes': 6886,\n",
       " \"voyage'\": 71514,\n",
       " 'sonorous': 36173,\n",
       " 'spontaneously': 14393,\n",
       " 'quartermain': 35292,\n",
       " 'fanclub': 55215,\n",
       " 'ironists': 53556,\n",
       " 'stumble': 7737,\n",
       " 'badmen': 80332,\n",
       " 'blai': 50670,\n",
       " 'shaloub': 32266,\n",
       " 'waisting': 53976,\n",
       " 'sprawl': 28090,\n",
       " 'mortgages': 37769,\n",
       " 'geoprge': 54625,\n",
       " \"curate's\": 68233,\n",
       " 'trenholm': 18674,\n",
       " 'clamoring': 34890,\n",
       " 'vulneable': 80117,\n",
       " 'worthy': 1514,\n",
       " 'rogell': 44603,\n",
       " 'discography': 67966,\n",
       " 'mobilized': 58825,\n",
       " 'deceive': 17899,\n",
       " 'els': 50251,\n",
       " 'spoke': 4594,\n",
       " 'bronx': 11833,\n",
       " 'pervasively': 53646,\n",
       " 'dedication': 8771,\n",
       " 'iraqi': 17157,\n",
       " 'profligate': 44034,\n",
       " 'compton': 29308,\n",
       " 'regarded': 5413,\n",
       " 'vishal': 26236,\n",
       " 'seized': 20457,\n",
       " 'palo': 79865,\n",
       " 'harling': 51445,\n",
       " 'receptacle': 33428,\n",
       " 'razing': 70939,\n",
       " 'noonann': 55256,\n",
       " 'shoehorned': 32968,\n",
       " 'merchandising': 21533,\n",
       " '6wks': 61512,\n",
       " 'tribulations': 10872,\n",
       " 'pacey': 39183,\n",
       " 'sunbeams': 77493,\n",
       " 'mauritania': 53944,\n",
       " 'desecration': 29014,\n",
       " 'eagerness': 30180,\n",
       " \"tsing's\": 84474,\n",
       " 'bodybag': 44285,\n",
       " \"ron's\": 30784,\n",
       " 'warden': 7410,\n",
       " 'garrulous': 77453,\n",
       " 'dividend': 79804,\n",
       " 'guitarists': 48246,\n",
       " 'craves': 19862,\n",
       " 'slums': 15976,\n",
       " 'dine': 23154,\n",
       " 'megazones': 64454,\n",
       " 'núñez': 80274,\n",
       " 'commemorated': 38647,\n",
       " 'mattie': 45838,\n",
       " 'promoted': 6522,\n",
       " 'disowns': 85269,\n",
       " 'developing': 4237,\n",
       " 'interfaith': 52812,\n",
       " 'clyton': 63128,\n",
       " \"buy's\": 59142,\n",
       " 'turturro': 10892,\n",
       " 'unmet': 80368,\n",
       " 'despict': 73241,\n",
       " 'sprezzatura': 80546,\n",
       " 'rodolphe': 44025,\n",
       " 'rousselot': 72903,\n",
       " 'japanes': 55147,\n",
       " \"rivera's\": 73138,\n",
       " 'circumstances': 2328,\n",
       " \"eli's\": 37456,\n",
       " 'whisper': 14170,\n",
       " \"'white\": 30160,\n",
       " \"mcgoldrick's\": 52781,\n",
       " 'questioned': 11324,\n",
       " 'mizz': 41808,\n",
       " 'xperiment': 61322,\n",
       " 'pshycological': 67043,\n",
       " 'ondu': 63113,\n",
       " 'haber': 79181,\n",
       " 'newswoman': 75497,\n",
       " 'marketable': 24364,\n",
       " 'goggins': 52708,\n",
       " 'parlor': 16383,\n",
       " 'twomarlowe': 76738,\n",
       " 'personation': 59310,\n",
       " 'zx81': 86305,\n",
       " \"kieslowski's\": 55618,\n",
       " '\\xa0i': 65641,\n",
       " 'prostrate': 52193,\n",
       " 'griggs': 41884,\n",
       " 'oyster': 84502,\n",
       " 'thuy': 29480,\n",
       " 'kenitalia': 84068,\n",
       " 'oren': 67024,\n",
       " 'wedlock': 26553,\n",
       " 'circe': 60080,\n",
       " 'bradycardia': 63843,\n",
       " 'schulmädchen': 86441,\n",
       " 'semprinni': 64806,\n",
       " 'disgracing': 87713,\n",
       " 'palette': 9608,\n",
       " \"d'alice\": 78074,\n",
       " \"kaye's\": 39941,\n",
       " '1910': 38062,\n",
       " 'adaptions': 36509,\n",
       " 'barnabus': 82640,\n",
       " 'underated': 39513,\n",
       " 'pilmark´s': 59908,\n",
       " \"scale'\": 70137,\n",
       " 'faw': 84994,\n",
       " \"hunt's\": 30978,\n",
       " 'remarked': 15557,\n",
       " \"mcanally's\": 43437,\n",
       " 'retentively': 87320,\n",
       " 'stinks': 4382,\n",
       " 'infects': 26987,\n",
       " 'horridly': 31644,\n",
       " 'stood': 3402,\n",
       " 'furballs': 49486,\n",
       " 'merr': 84897,\n",
       " \"norton's\": 24918,\n",
       " 'kerry': 10258,\n",
       " 'turnoff': 66416,\n",
       " 'wreckage': 19607,\n",
       " 'portent': 44211,\n",
       " 'dorothy': 3334,\n",
       " '7': 690,\n",
       " 'expansion': 17782,\n",
       " 'breads': 58022,\n",
       " \"wanda's\": 52131,\n",
       " 'pancreatic': 46097,\n",
       " 'synchronised': 35750,\n",
       " 'suds': 33321,\n",
       " 'witnesses': 4681,\n",
       " 'occassional': 65629,\n",
       " 'branka': 69652,\n",
       " 'sickest': 19947,\n",
       " 'serves': 2462,\n",
       " 'doofuses': 83802,\n",
       " 'restructuring': 69471,\n",
       " 'rareness': 57436,\n",
       " 'dupuis': 70513,\n",
       " 'cloudscape': 80694,\n",
       " 'perception': 6248,\n",
       " 'kinsey': 42688,\n",
       " 'afleck': 42750,\n",
       " \"hetfield's\": 52531,\n",
       " 'bestowed': 21789,\n",
       " 'tually': 83458,\n",
       " 'albacore': 72541,\n",
       " 'sliminess': 42512,\n",
       " 'brokedown': 15358,\n",
       " 'unshaved': 72419,\n",
       " 'clenteen': 39517,\n",
       " 'haddad': 31471,\n",
       " \"sharon's\": 87514,\n",
       " 'fleshes': 26040,\n",
       " 'headstart': 82448,\n",
       " 'repulsiveness': 40268,\n",
       " 'crooning': 38752,\n",
       " 'gamely': 24625,\n",
       " 'jacobite': 73262,\n",
       " 'lasars': 69982,\n",
       " 'contemporaneous': 32691,\n",
       " 'deus': 11684,\n",
       " 'liar': 9501,\n",
       " 'lindo': 31720,\n",
       " 'über': 32894,\n",
       " 'jannsen': 43959,\n",
       " 'autobiograhical': 68457,\n",
       " 'adoringly': 33942,\n",
       " 'surname': 23384,\n",
       " 'h2': 76855,\n",
       " \"celebritie's\": 75486,\n",
       " 'suicidal': 12575,\n",
       " 'precision': 10818,\n",
       " 'bleek': 29706,\n",
       " 'caretakers': 20571,\n",
       " \"'whycome'\": 67261,\n",
       " 'witchboard': 86347,\n",
       " 'reconsider': 17424,\n",
       " \"'spring\": 70174,\n",
       " 'charitable': 13512,\n",
       " 'kasugi': 81888,\n",
       " 'parroting': 88330,\n",
       " 'great': 84,\n",
       " \"lou's\": 25851,\n",
       " \"mill's\": 75134,\n",
       " 'whih': 65587,\n",
       " \"malaysian's\": 85904,\n",
       " 'bashings': 43523,\n",
       " 'engagements': 86887,\n",
       " 'conversions': 32607,\n",
       " '23': 6476,\n",
       " 'whatever\\x85': 65229,\n",
       " \"'loner\": 81083,\n",
       " 'bribing': 35234,\n",
       " 'adversity': 11394,\n",
       " 'emporer': 35352,\n",
       " 'josephine': 21331,\n",
       " 'bicycling': 60236,\n",
       " 'seiing': 60617,\n",
       " 'rayguns': 56272,\n",
       " 'cheesier': 25156,\n",
       " 'everybody': 1459,\n",
       " 'unmotivated': 13591,\n",
       " \"macarhur's\": 77454,\n",
       " 'unaccustomed': 42992,\n",
       " 'trumph': 61905,\n",
       " 'tuckwiller': 75714,\n",
       " 'ragging': 85254,\n",
       " 'commenters': 19077,\n",
       " 'uptightness': 44212,\n",
       " 'helmit': 52292,\n",
       " 'tazmainian': 64306,\n",
       " 'newmail': 68105,\n",
       " \"lordi's\": 38301,\n",
       " 'clausen': 16879,\n",
       " 'presumbably': 62139,\n",
       " 'coherence': 10793,\n",
       " 'liebmann': 40368,\n",
       " 'adi': 82480,\n",
       " 'husk': 36575,\n",
       " \"patton'\": 48343,\n",
       " 'subpoena': 56484,\n",
       " 'replacements': 36791,\n",
       " 'fairplay': 65970,\n",
       " 'esamples': 82720,\n",
       " 'bjore': 66485,\n",
       " 'samandar': 47013,\n",
       " '2furious': 52455,\n",
       " 'chematodes': 44101,\n",
       " 'weinzweig': 68439,\n",
       " 'allocated': 31218,\n",
       " 'rko': 9163,\n",
       " \"european'\": 42596,\n",
       " 'rubbiush': 64056,\n",
       " 'gifting': 66729,\n",
       " 'ww3': 42897,\n",
       " 'devo': 32533,\n",
       " \"'tess'\": 88035,\n",
       " 'clearlly': 56958,\n",
       " 'fx': 3706,\n",
       " 'serenading': 61342,\n",
       " 'staring': 4485,\n",
       " 'saxophones': 41270,\n",
       " 'wetting': 42099,\n",
       " 'roughneck': 40372,\n",
       " 'deathmatch': 51431,\n",
       " 'carpathian': 28230,\n",
       " 'bellucci': 37739,\n",
       " 'torchon': 46702,\n",
       " \"double's\": 59464,\n",
       " 'boppers': 45234,\n",
       " 'funjatta': 48290,\n",
       " 'delirium': 22545,\n",
       " 'duquenne': 23632,\n",
       " 'kubrick': 5008,\n",
       " 'findus': 42405,\n",
       " 'toughs': 32044,\n",
       " 'damp': 27146,\n",
       " 'bensonhurst': 47342,\n",
       " \"reactor's\": 87217,\n",
       " 'reduced': 3744,\n",
       " 'stationmaster': 64980,\n",
       " 'unwary': 60920,\n",
       " 'akim': 63233,\n",
       " 'pantomime': 20216,\n",
       " \"ippoliti's\": 71373,\n",
       " 'nicki': 39263,\n",
       " 'soliloquy': 17883,\n",
       " \"buster's\": 30126,\n",
       " 'disturbances': 69416,\n",
       " 'instills': 30394,\n",
       " 'tetsurô': 21069,\n",
       " 'tcheky': 47999,\n",
       " 'gilbert': 7083,\n",
       " 'celia': 16480,\n",
       " 'protesters': 21901,\n",
       " 'windbreaker': 59247,\n",
       " 'junket': 37626,\n",
       " \"'cowboy'\": 47400,\n",
       " \"cruella's\": 24198,\n",
       " 'vehement': 46078,\n",
       " '“mr': 76802,\n",
       " 'sünden': 75640,\n",
       " \"'bagman'\": 83498,\n",
       " \"body'\": 38139,\n",
       " 'nanavati': 25172,\n",
       " 'cheered': 12297,\n",
       " 'topmost': 56979,\n",
       " 'electrician': 20785,\n",
       " 'covertly': 42239,\n",
       " 'jesters': 82435,\n",
       " 'trattoria': 64921,\n",
       " 'sanborn': 73603,\n",
       " 'oyl': 28696,\n",
       " 'unintelligent': 15112,\n",
       " 'phantasms': 41586,\n",
       " 'til': 12941,\n",
       " 'unapologetically': 23903,\n",
       " 'reignites': 86636,\n",
       " 'ingenue': 20882,\n",
       " 'dalamatians': 54994,\n",
       " 'butkus': 47578,\n",
       " 'seeks': 4843,\n",
       " 'attributed': 9805,\n",
       " 'pitiable': 15775,\n",
       " 'gov': 43971,\n",
       " 'casablanka': 58572,\n",
       " 'reade': 44117,\n",
       " 'binkie': 54724,\n",
       " 'criticised': 15895,\n",
       " 'inexcusable': 14342,\n",
       " 'bhat': 65042,\n",
       " 'verbalize': 43004,\n",
       " \"stalker'\": 45688,\n",
       " 'reenactments': 43475,\n",
       " 'isms': 31784,\n",
       " 'rasputin': 33117,\n",
       " '80': 3039,\n",
       " 'incision': 77574,\n",
       " 'sympathizers': 24745,\n",
       " 'emotionally': 2145,\n",
       " 'columbus': 21142,\n",
       " 'inflicts': 25941,\n",
       " 'bamboo': 15090,\n",
       " 'fiancè': 76842,\n",
       " 'vignette': 15673,\n",
       " 'embodied': 15637,\n",
       " 'donen': 26946,\n",
       " 'revels': 19806,\n",
       " '\\x91fear': 73490,\n",
       " 'sortee': 54096,\n",
       " 'r2d2': 32248,\n",
       " 'khakee': 31574,\n",
       " 'christmases': 29758,\n",
       " 'haphazard': 11486,\n",
       " 'lose': 1582,\n",
       " 'principal': 4218,\n",
       " 'caduta': 55654,\n",
       " \"cassel's\": 46140,\n",
       " 'pavlovian': 57485,\n",
       " 'psychologizing': 63716,\n",
       " 'fmvs': 52082,\n",
       " \"pax's\": 83879,\n",
       " 'snoopers': 36525,\n",
       " 'bowers': 40515,\n",
       " 'comprehensibility': 70226,\n",
       " 'donnovan': 59675,\n",
       " 'pennington': 49056,\n",
       " 'baloo': 14101,\n",
       " 'manchild': 54680,\n",
       " 'peyton': 27499,\n",
       " 'cheerful': 7768,\n",
       " 'treads': 19632,\n",
       " 'probarly': 74032,\n",
       " 'matches': 4256,\n",
       " 'retrieval': 35329,\n",
       " 'alligators': 24783,\n",
       " 'garret': 38035,\n",
       " 'minmay': 85588,\n",
       " 'understating': 61648,\n",
       " 'normalizing': 81808,\n",
       " 'mcleod': 23615,\n",
       " 'nineveh': 57549,\n",
       " 'five': 674,\n",
       " 'priding': 70472,\n",
       " 'haliday': 30281,\n",
       " 'lying': 3170,\n",
       " 'lustre': 37030,\n",
       " 'deana': 34117,\n",
       " 'ganzel': 30691,\n",
       " 'debated': 23805,\n",
       " 'bin\\x97so': 58288,\n",
       " 'kostelanitz': 59959,\n",
       " 'wtse': 81277,\n",
       " 'poured': 13580,\n",
       " \"bombin'\": 62095,\n",
       " '57': 18597,\n",
       " 'lipton': 57215,\n",
       " 'deteriorating': 17217,\n",
       " 'expurgated': 40440,\n",
       " 'arms': 2797,\n",
       " 'cybertron': 57039,\n",
       " \"'afternoon\": 54085,\n",
       " 'spielbergian': 45664,\n",
       " 'challenge': 2929,\n",
       " 'gripped': 15474,\n",
       " 'stalkings': 45893,\n",
       " 'shrugs': 26018,\n",
       " 'morgan\\x97but': 70866,\n",
       " 'elsewhere': 3297,\n",
       " 'chubby': 11207,\n",
       " 'stunners': 49297,\n",
       " 'happier': 9010,\n",
       " 'lampidorra': 45239,\n",
       " 'polite': 9793,\n",
       " \"recollection's\": 52122,\n",
       " 'burrier': 71053,\n",
       " 'bart': 8853,\n",
       " 'ashwar': 30826,\n",
       " 'sprites': 68173,\n",
       " 'highbrow': 26017,\n",
       " 'benfica': 47006,\n",
       " 'lowlevel': 67058,\n",
       " 'enigmatically': 39602,\n",
       " \"instrumental's\": 68992,\n",
       " 'ortiz': 39058,\n",
       " '1mln': 84120,\n",
       " 'overpowering': 16120,\n",
       " 'holocost': 57218,\n",
       " 'pierpont': 33984,\n",
       " 'samotá': 79980,\n",
       " \"'equiptment'\": 54578,\n",
       " 'cinequest': 86384,\n",
       " 'kum': 35287,\n",
       " 'troublingly': 75364,\n",
       " 'thre': 48559,\n",
       " 'bowel': 22513,\n",
       " 'verged': 37793,\n",
       " 'coppy': 63330,\n",
       " 'spinozean': 87552,\n",
       " 'globally': 37898,\n",
       " 'harel': 28525,\n",
       " 'alternately': 9641,\n",
       " 'oberon': 13381,\n",
       " 'tennesse': 43917,\n",
       " 'sifting': 49706,\n",
       " 'bad': 75,\n",
       " 'sedatives': 62229,\n",
       " 'sassoon': 50095,\n",
       " 'leanne': 78610,\n",
       " \"dentist's\": 24093,\n",
       " \"ming's\": 64585,\n",
       " 'dabbing': 33297,\n",
       " 'browses': 80636,\n",
       " 'wheelsy': 49779,\n",
       " 'blasé': 44296,\n",
       " 'consolidate': 73667,\n",
       " 'yum': 16557,\n",
       " 'unsubstantial': 41077,\n",
       " 'fotr': 57137,\n",
       " 'subverts': 36821,\n",
       " 'milk': 5167,\n",
       " 'vindictiveness': 38825,\n",
       " 'vindictively': 37437,\n",
       " 'filmscore': 63214,\n",
       " \"reda's\": 37518,\n",
       " 'mollified': 70177,\n",
       " 'brags': 38588,\n",
       " 'immense': 6631,\n",
       " 'taekwon': 60347,\n",
       " \"branaugh's\": 58420,\n",
       " 'stilwell': 60441,\n",
       " 'castelo': 76108,\n",
       " 'wast': 38555,\n",
       " 'aditiya': 62511,\n",
       " \"'paris\": 22290,\n",
       " 'interject': 30103,\n",
       " \"ok'\": 86818,\n",
       " 'mayedas': 76261,\n",
       " 'cramp': 23686,\n",
       " \"knotts'\": 51260,\n",
       " 'micki': 80277,\n",
       " 'wanky': 67969,\n",
       " 'rajendra': 55680,\n",
       " 'amrish': 22632,\n",
       " 'animal': 1623,\n",
       " 'cryo': 62275,\n",
       " 'mateo': 45041,\n",
       " 'season3': 71129,\n",
       " \"o'gill\": 60047,\n",
       " 'anita': 7783,\n",
       " 'hormones': 19016,\n",
       " 'organic': 11065,\n",
       " 'huber': 86374,\n",
       " 'romancing': 22245,\n",
       " \"other's\": 5307,\n",
       " 'precipice': 55089,\n",
       " 'nanak': 71117,\n",
       " 'killearn': 33849,\n",
       " \"'descent'\": 77424,\n",
       " 'palma': 5558,\n",
       " 'impossible': 1164,\n",
       " 'saizescus': 58403,\n",
       " 'infinnerty': 84055,\n",
       " 'laid': 2981,\n",
       " 'prepoire': 85140,\n",
       " 'mistry': 71220,\n",
       " 'glommed': 64895,\n",
       " 'venerate': 72142,\n",
       " 'vegetation': 28353,\n",
       " \"cyborg's\": 62858,\n",
       " 'oru': 86137,\n",
       " 'compiled': 18876,\n",
       " 'tomboy': 15108,\n",
       " 'diry': 76437,\n",
       " 'prototypes': 52635,\n",
       " '3th': 86706,\n",
       " \"francesca's\": 69203,\n",
       " 'topnotch': 30205,\n",
       " \"''talent\": 84926,\n",
       " \"freddie's\": 45370,\n",
       " \"kitt's\": 48241,\n",
       " 'cooperated': 67574,\n",
       " 'airfield': 75388,\n",
       " 'tentatively': 61503,\n",
       " 'wrightly': 73525,\n",
       " 'dsm': 29490,\n",
       " 'outerspace': 22038,\n",
       " 'microscopically': 43813,\n",
       " 'mutia': 67274,\n",
       " 'vermin': 16552,\n",
       " 'bijou': 55071,\n",
       " 'psyched': 36760,\n",
       " 'bana': 24815,\n",
       " 'ripping': 5990,\n",
       " '188': 82047,\n",
       " 'page2': 88321,\n",
       " '1040a': 82639,\n",
       " \"lucas'\": 14584,\n",
       " \"thelis's\": 77409,\n",
       " 'dump': 6537,\n",
       " 'baddeley': 49081,\n",
       " 'armistice': 58426,\n",
       " \"din't\": 76435,\n",
       " 'shouldn´t': 63688,\n",
       " 'deliverer': 62833,\n",
       " 'fascinatingly': 25429,\n",
       " 'christensen': 9082,\n",
       " 'everrrryone': 57797,\n",
       " 'ragno': 59373,\n",
       " 'uchida': 44042,\n",
       " \"'e'\": 76125,\n",
       " 'salvageable': 54193,\n",
       " \"blaster's\": 64016,\n",
       " \"gough's\": 60388,\n",
       " 'apologia': 74388,\n",
       " 'consequence': 7216,\n",
       " 'spradlin': 48414,\n",
       " \"'then\": 59024,\n",
       " 'ppl': 18599,\n",
       " 'friedkin': 25520,\n",
       " 'anextremely': 46768,\n",
       " 'smoochy': 37946,\n",
       " 'operating': 8881,\n",
       " 'firelight': 34942,\n",
       " 'eschews': 22266,\n",
       " 'dimes': 77770,\n",
       " 'incredulous': 20538,\n",
       " 'nabooboo': 87132,\n",
       " 'nastassia': 49006,\n",
       " 'thighs': 20275,\n",
       " \"'ankhein'\": 69073,\n",
       " '\\x85hmmmm': 75795,\n",
       " 'cuarn': 85593,\n",
       " 'convalesces': 41932,\n",
       " 'aughties': 59798,\n",
       " 'flatly': 23260,\n",
       " 'disrespected': 30234,\n",
       " '06th': 65863,\n",
       " 'rocll': 55363,\n",
       " 'uden': 70894,\n",
       " 'banaras': 74835,\n",
       " 'covered': 2377,\n",
       " 'exams': 21374,\n",
       " 'resident': 4924,\n",
       " 'miscalculations': 85288,\n",
       " 'bhoomika': 30620,\n",
       " 'plied': 63253,\n",
       " 'nonactor': 62091,\n",
       " 'superimpose': 40650,\n",
       " 'teasing': 14751,\n",
       " 'messier': 52407,\n",
       " 'firework': 73741,\n",
       " 'topeka': 73013,\n",
       " \"scuddamore's\": 84225,\n",
       " \"seinfeld's\": 41091,\n",
       " 'documentarian': 22653,\n",
       " 'amplify': 29867,\n",
       " 'lautrec': 35905,\n",
       " \"'who's\": 29858,\n",
       " \"'dancer'\": 62416,\n",
       " 'tape': 2211,\n",
       " 'minglun': 80037,\n",
       " 'warfel': 74467,\n",
       " 'overact': 11446,\n",
       " 'unit': 4811,\n",
       " 'suevia': 80186,\n",
       " 'restroom': 16533,\n",
       " 'inspires': 10098,\n",
       " 'flaw': 3276,\n",
       " 'armies': 13016,\n",
       " 'bedouin': 32771,\n",
       " 'perished': 21926,\n",
       " 'birthdays': 37748,\n",
       " 'mechanised': 62423,\n",
       " 'suit': 1732,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's load our embeddings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "vector_model=KeyedVectors.load_word2vec_format(\"./wiki-news-300d-1M.vec\", binary=False, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...and let's normalize\n",
    "vector_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4894\n"
     ]
    }
   ],
   "source": [
    "#let's create an embedding matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "embedding_matrix = np.zeros((max_features, 300))\n",
    "countr = 0\n",
    "for token, idx in vocab.items():\n",
    "    \n",
    "    if idx < max_features:\n",
    "        if token in vector_model:\n",
    "            embedding_matrix[idx] = vector_model[token]\n",
    "            countr += 1\n",
    "print (countr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 308s 12ms/step - loss: 0.3604 - acc: 0.8390 - val_loss: 0.3265 - val_acc: 0.8615\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 306s 12ms/step - loss: 0.1783 - acc: 0.9331 - val_loss: 0.2582 - val_acc: 0.8946\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 307s 12ms/step - loss: 0.0828 - acc: 0.9742 - val_loss: 0.2786 - val_acc: 0.8970\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 307s 12ms/step - loss: 0.0250 - acc: 0.9962 - val_loss: 0.3202 - val_acc: 0.8948\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 311s 12ms/step - loss: 0.0066 - acc: 0.9998 - val_loss: 0.3499 - val_acc: 0.8988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1bf3d8cf8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "\n",
    "embedding_layer = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights = [embedding_matrix],\n",
    "                    input_length=maxlen)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yay! It got a little better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classifier with multiple convolutional layers\n",
    "\n",
    "Inspired by an illustration in the previous lecture, only a little simplified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 830s 33ms/step - loss: 0.3459 - acc: 0.8417 - val_loss: 0.2850 - val_acc: 0.8804\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 831s 33ms/step - loss: 0.1726 - acc: 0.9343 - val_loss: 0.2789 - val_acc: 0.8873\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 838s 34ms/step - loss: 0.0692 - acc: 0.9797 - val_loss: 0.2779 - val_acc: 0.8982\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 834s 33ms/step - loss: 0.0186 - acc: 0.9970 - val_loss: 0.3226 - val_acc: 0.9002\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 844s 34ms/step - loss: 0.0050 - acc: 0.9997 - val_loss: 0.3693 - val_acc: 0.8991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0dd4d81d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "\n",
    "#Let's define the inputs\n",
    "x = Input(shape=(maxlen,))\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "\n",
    "embedding_layer = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights = [embedding_matrix],\n",
    "                    input_length=maxlen)\n",
    "\n",
    "embeddings = embedding_layer(x)\n",
    "\n",
    "conv_res = []\n",
    "for width in range(2,5):\n",
    "\n",
    "    conv_result = Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(embeddings)\n",
    "    pooled = (GlobalMaxPooling1D())(conv_result) \n",
    "    conv_res.append(pooled)\n",
    "    \n",
    "\n",
    "concatenated = (Concatenate())(conv_res)\n",
    "    \n",
    "# We add a vanilla hidden layer:\n",
    "out = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "model = Model(x, out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yay! Just a little better again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"470pt\" viewBox=\"0.00 0.00 950.00 470.00\" width=\"950pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 466)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-466 946,-466 946,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140672535367632 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140672535367632</title>\n",
       "<polygon fill=\"none\" points=\"363.5,-415.5 363.5,-461.5 578.5,-461.5 578.5,-415.5 363.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"402\" y=\"-434.8\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"440.5,-415.5 440.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"468\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"440.5,-438.5 495.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"468\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"495.5,-415.5 495.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"537\" y=\"-446.3\">(None, 400)</text>\n",
       "<polyline fill=\"none\" points=\"495.5,-438.5 578.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"537\" y=\"-423.3\">(None, 400)</text>\n",
       "</g>\n",
       "<!-- 140672388019368 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140672388019368</title>\n",
       "<polygon fill=\"none\" points=\"348.5,-332.5 348.5,-378.5 593.5,-378.5 593.5,-332.5 348.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388.5\" y=\"-351.8\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"428.5,-332.5 428.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"456\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"428.5,-355.5 483.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"456\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"483.5,-332.5 483.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"538.5\" y=\"-363.3\">(None, 400)</text>\n",
       "<polyline fill=\"none\" points=\"483.5,-355.5 593.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"538.5\" y=\"-340.3\">(None, 400, 300)</text>\n",
       "</g>\n",
       "<!-- 140672535367632&#45;&gt;140672388019368 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140672535367632-&gt;140672388019368</title>\n",
       "<path d=\"M471,-415.366C471,-407.152 471,-397.658 471,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"474.5,-388.607 471,-378.607 467.5,-388.607 474.5,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140672535546736 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140672535546736</title>\n",
       "<polygon fill=\"none\" points=\"74.5,-249.5 74.5,-295.5 301.5,-295.5 301.5,-249.5 74.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"105.5\" y=\"-268.8\">Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"136.5,-249.5 136.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"136.5,-272.5 191.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"191.5,-249.5 191.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.5\" y=\"-280.3\">(None, 400, 300)</text>\n",
       "<polyline fill=\"none\" points=\"191.5,-272.5 301.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.5\" y=\"-257.3\">(None, 398, 250)</text>\n",
       "</g>\n",
       "<!-- 140672388019368&#45;&gt;140672535546736 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140672388019368-&gt;140672535546736</title>\n",
       "<path d=\"M394.043,-332.473C357.232,-321.937 313.004,-309.279 274.978,-298.395\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"275.519,-294.909 264.942,-295.522 273.593,-301.639 275.519,-294.909\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140676272302232 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140676272302232</title>\n",
       "<polygon fill=\"none\" points=\"357.5,-249.5 357.5,-295.5 584.5,-295.5 584.5,-249.5 357.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388.5\" y=\"-268.8\">Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"419.5,-249.5 419.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"447\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"419.5,-272.5 474.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"447\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"474.5,-249.5 474.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"529.5\" y=\"-280.3\">(None, 400, 300)</text>\n",
       "<polyline fill=\"none\" points=\"474.5,-272.5 584.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"529.5\" y=\"-257.3\">(None, 398, 250)</text>\n",
       "</g>\n",
       "<!-- 140672388019368&#45;&gt;140676272302232 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140672388019368-&gt;140676272302232</title>\n",
       "<path d=\"M471,-332.366C471,-324.152 471,-314.658 471,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"474.5,-305.607 471,-295.607 467.5,-305.607 474.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140676242176824 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140676242176824</title>\n",
       "<polygon fill=\"none\" points=\"639.5,-249.5 639.5,-295.5 866.5,-295.5 866.5,-249.5 639.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"670.5\" y=\"-268.8\">Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"701.5,-249.5 701.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"729\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"701.5,-272.5 756.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"729\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"756.5,-249.5 756.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811.5\" y=\"-280.3\">(None, 400, 300)</text>\n",
       "<polyline fill=\"none\" points=\"756.5,-272.5 866.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811.5\" y=\"-257.3\">(None, 398, 250)</text>\n",
       "</g>\n",
       "<!-- 140672388019368&#45;&gt;140676242176824 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140672388019368-&gt;140676242176824</title>\n",
       "<path d=\"M547.685,-332.473C584.366,-321.937 628.438,-309.279 666.33,-298.395\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"667.685,-301.647 676.33,-295.522 665.752,-294.919 667.685,-301.647\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140676272920224 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140676272920224</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 302,-212.5 302,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68.5\" y=\"-185.8\">GlobalMaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"137,-166.5 137,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"137,-189.5 192,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"192,-166.5 192,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-197.3\">(None, 398, 250)</text>\n",
       "<polyline fill=\"none\" points=\"192,-189.5 302,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-174.3\">(None, 250)</text>\n",
       "</g>\n",
       "<!-- 140672535546736&#45;&gt;140676272920224 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140672535546736-&gt;140676272920224</title>\n",
       "<path d=\"M177.89,-249.366C174.015,-240.884 169.517,-231.037 165.322,-221.853\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.437,-220.249 161.098,-212.607 162.07,-223.157 168.437,-220.249\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140676272041320 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140676272041320</title>\n",
       "<polygon fill=\"none\" points=\"320,-166.5 320,-212.5 622,-212.5 622,-166.5 320,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388.5\" y=\"-185.8\">GlobalMaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"457,-166.5 457,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"484.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"457,-189.5 512,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"484.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"512,-166.5 512,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"567\" y=\"-197.3\">(None, 398, 250)</text>\n",
       "<polyline fill=\"none\" points=\"512,-189.5 622,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"567\" y=\"-174.3\">(None, 250)</text>\n",
       "</g>\n",
       "<!-- 140676272302232&#45;&gt;140676272041320 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140676272302232-&gt;140676272041320</title>\n",
       "<path d=\"M471,-249.366C471,-241.152 471,-231.658 471,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"474.5,-222.607 471,-212.607 467.5,-222.607 474.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140676242296560 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140676242296560</title>\n",
       "<polygon fill=\"none\" points=\"640,-166.5 640,-212.5 942,-212.5 942,-166.5 640,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"708.5\" y=\"-185.8\">GlobalMaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"777,-166.5 777,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"777,-189.5 832,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"832,-166.5 832,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"887\" y=\"-197.3\">(None, 398, 250)</text>\n",
       "<polyline fill=\"none\" points=\"832,-189.5 942,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"887\" y=\"-174.3\">(None, 250)</text>\n",
       "</g>\n",
       "<!-- 140676242176824&#45;&gt;140676242296560 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140676242176824-&gt;140676242296560</title>\n",
       "<path d=\"M763.384,-249.366C767.363,-240.884 771.982,-231.037 776.291,-221.853\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"779.55,-223.147 780.629,-212.607 773.213,-220.174 779.55,-223.147\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140672388013304 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140672388013304</title>\n",
       "<polygon fill=\"none\" points=\"282.5,-83.5 282.5,-129.5 659.5,-129.5 659.5,-83.5 282.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324\" y=\"-102.8\">Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"365.5,-83.5 365.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"393\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"365.5,-106.5 420.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"393\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"420.5,-83.5 420.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"540\" y=\"-114.3\">[(None, 250), (None, 250), (None, 250)]</text>\n",
       "<polyline fill=\"none\" points=\"420.5,-106.5 659.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"540\" y=\"-91.3\">(None, 750)</text>\n",
       "</g>\n",
       "<!-- 140676272920224&#45;&gt;140672388013304 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140676272920224-&gt;140672388013304</title>\n",
       "<path d=\"M238.019,-166.473C280.174,-155.803 330.931,-142.955 374.294,-131.979\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"375.163,-135.369 383.999,-129.522 373.445,-128.583 375.163,-135.369\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140676272041320&#45;&gt;140672388013304 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140676272041320-&gt;140672388013304</title>\n",
       "<path d=\"M471,-166.366C471,-158.152 471,-148.658 471,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"474.5,-139.607 471,-129.607 467.5,-139.607 474.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140676242296560&#45;&gt;140672388013304 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140676242296560-&gt;140672388013304</title>\n",
       "<path d=\"M703.981,-166.473C661.826,-155.803 611.069,-142.955 567.706,-131.979\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"568.555,-128.583 558.001,-129.522 566.837,-135.369 568.555,-128.583\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140676242276760 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140676242276760</title>\n",
       "<polygon fill=\"none\" points=\"377,-0.5 377,-46.5 565,-46.5 565,-0.5 377,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"402\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"427,-0.5 427,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"454.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"427,-23.5 482,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"454.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"482,-0.5 482,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"523.5\" y=\"-31.3\">(None, 750)</text>\n",
       "<polyline fill=\"none\" points=\"482,-23.5 565,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"523.5\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 140672388013304&#45;&gt;140676242276760 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140672388013304-&gt;140676242276760</title>\n",
       "<path d=\"M471,-83.3664C471,-75.1516 471,-65.6579 471,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"474.5,-56.6068 471,-46.6068 467.5,-56.6069 474.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model,show_shapes=True,show_layer_names=False).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good reading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
