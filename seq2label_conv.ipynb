{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Label with conv-nets\n",
    "\n",
    "## Goals of the lecture\n",
    "\n",
    "1. Understand seq2label problem setting\n",
    "2. Understanding how 1d-convolution and its practicalities work\n",
    "3. Acquiring capability to implement seq2label models with conv-nets on keras\n",
    "\n",
    "## Outline\n",
    "\n",
    "0. Intro\n",
    "1. Few words about seq2label, data and its form\n",
    "2. Detailed look into a simple example model\n",
    "3. Look into a more complicated model\n",
    "4. Look into what might be happening here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2label\n",
    "\n",
    "\n",
    "### Recap on Bag-Of-Words\n",
    "\n",
    "\n",
    "#### Input\n",
    "\n",
    "As was previously discussed, when we are bulding a bag of words - classifier, our input is a set of unordered words and the output is a single label. \n",
    "\n",
    "The list of words is given to the model as a feature vector, a single numerical vector. So, to repeat its input is a single vector.\n",
    "\n",
    "To do this transformation, all tokens need to have a numerical identification. That is, each word is associated with a specific number. In practice this is retrieved from a vocabulary as was shown before.\n",
    "\n",
    "### Output\n",
    "\n",
    "Its output is a label. As such it is a function from a vector into a label.\n",
    "\n",
    "Labels are typically represented as a vector, which has a value for all possible labels, or a single value if the task is binary.\n",
    "\n",
    "### How Seq2Label differs from this?\n",
    "\n",
    "The only difference from the bow is the form of the input.\n",
    "\n",
    "Like in the bow example, the input is a single vector, but instead of representing a set, it represents a sequence. This is achieved simply by adding the token identifiers after each other.\n",
    "\n",
    "The output is identical.\n",
    "\n",
    "### A Simple Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : is\n",
      "1 : essential\n",
      "2 : on\n",
      "3 : sat\n",
      "4 : it\n",
      "5 : the\n",
      "6 : mat\n",
      "7 : cat\n"
     ]
    }
   ],
   "source": [
    "#Let's define a simple and short vocabulary\n",
    "tokens = list(set('it is essential the cat sat on the mat'.split()))\n",
    "token_ids = {}\n",
    "for i,t in enumerate(tokens):\n",
    "    print (i,':',t)\n",
    "    token_ids[t] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'sat', 'on', 'the', 'mat']\n"
     ]
    }
   ],
   "source": [
    "#Let's define a sentence we want to turn into a vector form\n",
    "\n",
    "sentence = 'cat sat on the mat'.split()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Bag of words vector\n",
    "import numpy as np\n",
    "\n",
    "#Vector is as long as the vocabulary, initial values are zeros\n",
    "vector = np.zeros((len(token_ids),))\n",
    "for token in sentence:\n",
    "    vector[token_ids[token]] = 1.0\n",
    "print (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 3, 2, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "#Sequential input vector\n",
    "\n",
    "vector = [token_ids[t] for t in sentence]\n",
    "print (vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding of the data\n",
    "\n",
    "Some deep-learning frameworks, such as keras, require the input to have a preset shape which is the same for all examples. This is not the case with all frameworks, but its good to cover. If for nothing else, for the sake of this tutorial.\n",
    "\n",
    "Since these input sequences are not the same length, we need to make them so. This is accomplished by padding, which in practice means adding zeros to the input vectors to make them uniform in size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7, 3, 2, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Padding can easily be achieved with keras in-built functions\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "pad_sequences(np.array([vector]), maxlen=20, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super short recap on embedding layers\n",
    "\n",
    "Most often these sequences of numbers are used to load embeddings, which are used as the input of the next stage of the network. An embedding layer could be imagined as a dictionary; in go indexes, out comes representative embeddings.\n",
    "\n",
    "What happens inside an embedding layer can be demonstrated with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_size = 4\n",
    "#Let's just make a random matrix to act as our demonstration embedding matrix\n",
    "embedding_matrix = np.random.rand(len(token_ids),embedding_size)\n",
    "\n",
    "print ('from:', vector)\n",
    "print ('into:')\n",
    "print (embedding_matrix[vector])\n",
    "print ('and the matrix has a shape:', embedding_matrix[vector].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution recap\n",
    "\n",
    "### What's good about convolution?\n",
    "\n",
    "Convolutional networks are fast. They are fast because they are easily parallelizable. That is because no part of the convolution output depends on another one of the outputs. If that was the case, as it is in recurrent neural networks we couldn't make predictions before other parts of the layer have finished calculating.\n",
    "\n",
    "### 1-dimensional convolution\n",
    "\n",
    "During he earlier lecture and also to a large degree on the internet, image classification is used to demonstrate conv-nets. That is very understandable, for that is the area in which these networks traditionally shine and also because the convolution operation is a traditional tool for image processing.\n",
    "\n",
    "An image is a two dimensional input. It has height and it has width. A sequence of words has only one dimension. That is, you can traverse it forwards and backwards, but there is no traversing it \"up\" or \"down\" nor is there \"toward\" and \"from\" etc. It just is a one-dimensional sequence of tokens.\n",
    "\n",
    "Because of this, the convolution operation applied to text is usally one dimensional, instead of two dimensional convolution often applied to images.\n",
    "\n",
    "The relevant variables in the operation are:\n",
    "\n",
    "1. Kernel size, in effect how many conv operations are applied to the input\n",
    "2. Filter size, in effect how wide the applied conv operations are\n",
    "3. Stride length\n",
    "4. Padding\n",
    "\n",
    "#### Input / Output\n",
    "\n",
    "This operation takes as its input 2-d matrices (when we discount the batch_size). The matrix is in the shape of (steps, input_dim)\n",
    "\n",
    "The operation returns similarly a 2-d matrix. The size of the output matrix is (new_steps, filters). In this shape filters correspond to the amount of filters used. The new step count is a little bit trickier. It depends on stride length, kernel size and used padding style. \n",
    "https://keras.io/layers/convolutional/#conv1d\n",
    "\n",
    "\n",
    "### Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Untitled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-dimensional convolution and text\n",
    "\n",
    "Since the convolutional network bases its activations on a window of certain length, it is reacting to snippets of text the size of the convolution window. For example a conv-net with a window size of three, is capable of reacting to trigrams etc. In this sense convolutional text classifiers remind us of bag-of-ngram classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "The pooling operation is used for dimensionality reduction of the input. The type of pooling discussed in this lecture is max-pooling, but other variants exist. In the context of this lecture we will be using global pooling, similarly variants exist.\n",
    "\n",
    "Global, 1 dimensional, max pooling operation takes as its input a 2-d matrix and returns a vector. \n",
    "\n",
    "This operation is best explained with an illustration.\n",
    "\n",
    "### Illustration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we start with:\n",
      "[[0.42331819 0.05998545 0.82866219 0.69192177]\n",
      " [0.97540224 0.96001808 0.70915789 0.49302126]\n",
      " [0.78890864 0.96926192 0.09650553 0.06303868]\n",
      " [0.90819731 0.46243111 0.2940533  0.60752277]\n",
      " [0.60756078 0.12002846 0.14382413 0.42879468]]\n",
      "\n",
      "after max pooling:\n",
      "[0.97540224 0.96926192 0.82866219 0.69192177]\n"
     ]
    }
   ],
   "source": [
    "#We can illustrate global max pooling with numpy\n",
    "embeddings = embedding_matrix[vector]\n",
    "print ('we start with:')\n",
    "print (embeddings)\n",
    "print ()\n",
    "print ('after max pooling:')\n",
    "print (np.max(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "Rectified linear unit. An activation used often with conv-nets. As an activation it doesn't care about input size. Input and output are always of similar shape. Simply returns the value if it is >0 and 0 otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simple model\n",
    "\n",
    "To start of this lecture, let us look into an example convnet-text classifier included in keras examples: https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py\n",
    "\n",
    "It uses the IMDB-dataset already we are already familiar with and contains a single convolution layer. It's performance is said to be 0.89, which is somewhat disappointingly less than the bag-of-words example earlier.\n",
    "\n",
    "In addition to demonstrating a convolutional text classifier, the example nicely illuminates what typical deep-learning code looks like.\n",
    "\n",
    "For purposes of simplification I've yet very slightly simplified the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 315s 13ms/step - loss: 0.3557 - acc: 0.8416 - val_loss: 0.3372 - val_acc: 0.8556\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 313s 13ms/step - loss: 0.1804 - acc: 0.9304 - val_loss: 0.2550 - val_acc: 0.8948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f517b216f28>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This example demonstrates the use of Convolution1D for text classification.\n",
    "Gets to 0.89 test accuracy after 2 epochs.\n",
    "90s/epoch on Intel i5 2.4Ghz CPU.\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The simple model, written in functional API and without binary crossentropy\n",
    "\n",
    "Use this to play with the different parameters. How much do they affect the end result? How is this model different from the one above it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "(25000,)\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 305s 12ms/step - loss: 0.3485 - acc: 0.8456 - val_loss: 0.2998 - val_acc: 0.8732\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 304s 12ms/step - loss: 0.1665 - acc: 0.9389 - val_loss: 0.2492 - val_acc: 0.8989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84648cdda0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print (x_train.shape)\n",
    "\n",
    "#Since we are using output the size of 2, we will have to do one-hot encoding\n",
    "#x_test = to_categorical(x_test)\n",
    "#y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_test = onehot_encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "y_train = onehot_encoder.transform(y_train.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "#Let's define the inputs\n",
    "x = Input(shape=(maxlen,))\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "\n",
    "embedding_layer = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen)\n",
    "\n",
    "embeddings = embedding_layer(x)\n",
    "\n",
    "conv_layer = Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)\n",
    "conv_result = conv_layer(embeddings)\n",
    "pooled = (GlobalMaxPooling1D())(conv_result) \n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "out = Dense(2, activation='softmax')(pooled)\n",
    "\n",
    "model = Model(x, out)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 310.00 387.00\" width=\"310pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 306,-383 306,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140206696956088 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140206696956088</title>\n",
       "<polygon fill=\"none\" points=\"43.5,-332.5 43.5,-378.5 258.5,-378.5 258.5,-332.5 43.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-351.8\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"120.5,-332.5 120.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"120.5,-355.5 175.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"175.5,-332.5 175.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"217\" y=\"-363.3\">(None, 400)</text>\n",
       "<polyline fill=\"none\" points=\"175.5,-355.5 258.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"217\" y=\"-340.3\">(None, 400)</text>\n",
       "</g>\n",
       "<!-- 140206692128976 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140206692128976</title>\n",
       "<polygon fill=\"none\" points=\"28.5,-249.5 28.5,-295.5 273.5,-295.5 273.5,-249.5 28.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68.5\" y=\"-268.8\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"108.5,-249.5 108.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"136\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"108.5,-272.5 163.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"136\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"163.5,-249.5 163.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218.5\" y=\"-280.3\">(None, 400)</text>\n",
       "<polyline fill=\"none\" points=\"163.5,-272.5 273.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218.5\" y=\"-257.3\">(None, 400, 300)</text>\n",
       "</g>\n",
       "<!-- 140206696956088&#45;&gt;140206692128976 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140206696956088-&gt;140206692128976</title>\n",
       "<path d=\"M151,-332.366C151,-324.152 151,-314.658 151,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-305.607 151,-295.607 147.5,-305.607 154.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140206696848352 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140206696848352</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-166.5 37.5,-212.5 264.5,-212.5 264.5,-166.5 37.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68.5\" y=\"-185.8\">Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"99.5,-166.5 99.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"99.5,-189.5 154.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"154.5,-166.5 154.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"209.5\" y=\"-197.3\">(None, 400, 300)</text>\n",
       "<polyline fill=\"none\" points=\"154.5,-189.5 264.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"209.5\" y=\"-174.3\">(None, 398, 250)</text>\n",
       "</g>\n",
       "<!-- 140206692128976&#45;&gt;140206696848352 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140206692128976-&gt;140206696848352</title>\n",
       "<path d=\"M151,-249.366C151,-241.152 151,-231.658 151,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-222.607 151,-212.607 147.5,-222.607 154.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140206698096344 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140206698096344</title>\n",
       "<polygon fill=\"none\" points=\"0,-83.5 0,-129.5 302,-129.5 302,-83.5 0,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68.5\" y=\"-102.8\">GlobalMaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"137,-83.5 137,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"137,-106.5 192,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"192,-83.5 192,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-114.3\">(None, 398, 250)</text>\n",
       "<polyline fill=\"none\" points=\"192,-106.5 302,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-91.3\">(None, 250)</text>\n",
       "</g>\n",
       "<!-- 140206696848352&#45;&gt;140206698096344 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140206696848352-&gt;140206698096344</title>\n",
       "<path d=\"M151,-166.366C151,-158.152 151,-148.658 151,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-139.607 151,-129.607 147.5,-139.607 154.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140206696847288 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140206696847288</title>\n",
       "<polygon fill=\"none\" points=\"57,-0.5 57,-46.5 245,-46.5 245,-0.5 57,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"107,-0.5 107,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"107,-23.5 162,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"162,-0.5 162,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.5\" y=\"-31.3\">(None, 250)</text>\n",
       "<polyline fill=\"none\" points=\"162,-23.5 245,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.5\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 140206698096344&#45;&gt;140206696847288 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140206698096344-&gt;140206696847288</title>\n",
       "<path d=\"M151,-83.3664C151,-75.1516 151,-65.6579 151,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-56.6068 151,-46.6068 147.5,-56.6069 154.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model,show_shapes=True,show_layer_names=False).create(prog='dot', format='svg'))\n",
    "#model_to_dot(model,show_shapes=True,show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we were to load pre-trained embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'shower'\": 66431,\n",
       " 'caricaturish': 53075,\n",
       " 'chitre': 70850,\n",
       " 'looonnnggg': 80846,\n",
       " 'moviewise': 58717,\n",
       " 'salomé': 80275}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab\n",
    "\n",
    "{'chitre': 70850,\n",
    " 'moviewise': 58717,\n",
    " 'looonnnggg': 80846,\n",
    " 'salomé': 80275,\n",
    " 'caricaturish': 53075,\n",
    " \"'shower'\": 66431,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's load our embeddings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "vector_model=KeyedVectors.load_word2vec_format(\"./wiki-news-300d-1M.vec\", binary=False, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...and let's normalize\n",
    "vector_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4894\n"
     ]
    }
   ],
   "source": [
    "#let's create an embedding matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "embedding_matrix = np.zeros((max_features, 300))\n",
    "countr = 0\n",
    "for token, idx in vocab.items():\n",
    "    \n",
    "    if idx < max_features:\n",
    "        if token in vector_model:\n",
    "            embedding_matrix[idx] = vector_model[token]\n",
    "            countr += 1\n",
    "print (countr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 326s 13ms/step - loss: 0.3611 - acc: 0.8380 - val_loss: 0.3247 - val_acc: 0.8604\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 320s 13ms/step - loss: 0.1771 - acc: 0.9342 - val_loss: 0.2487 - val_acc: 0.8982\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 313s 13ms/step - loss: 0.0834 - acc: 0.9745 - val_loss: 0.2647 - val_acc: 0.8994\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 311s 12ms/step - loss: 0.0251 - acc: 0.9957 - val_loss: 0.3129 - val_acc: 0.8978\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 313s 13ms/step - loss: 0.0064 - acc: 0.9998 - val_loss: 0.3420 - val_acc: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f517b4e82e8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "\n",
    "embedding_layer = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights = [embedding_matrix],\n",
    "                    input_length=maxlen)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yay! It got a little better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classifier with multiple convolutional layers\n",
    "\n",
    "Inspired by an illustration in the previous lecture, only a little simplified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 826s 33ms/step - loss: 0.3419 - acc: 0.8434 - val_loss: 0.2694 - val_acc: 0.8880\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 825s 33ms/step - loss: 0.1693 - acc: 0.9370 - val_loss: 0.2630 - val_acc: 0.8920\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 821s 33ms/step - loss: 0.0685 - acc: 0.9799 - val_loss: 0.2750 - val_acc: 0.8982\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 841s 34ms/step - loss: 0.0184 - acc: 0.9972 - val_loss: 0.3191 - val_acc: 0.9020\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 832s 33ms/step - loss: 0.0047 - acc: 0.9998 - val_loss: 0.3611 - val_acc: 0.8990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f517e58a208>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "\n",
    "#Let's define the inputs\n",
    "x = Input(shape=(maxlen,))\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "\n",
    "embedding_layer = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights = [embedding_matrix],\n",
    "                    input_length=maxlen)\n",
    "\n",
    "embeddings = embedding_layer(x)\n",
    "\n",
    "conv_res = []\n",
    "for width in range(2,5):\n",
    "\n",
    "    conv_result = Conv1D(filters, width, padding='valid', activation='relu', strides=1)(embeddings)\n",
    "    pooled = (GlobalMaxPooling1D())(conv_result) \n",
    "    conv_res.append(pooled)\n",
    "    \n",
    "\n",
    "concatenated = (Concatenate())(conv_res)\n",
    "    \n",
    "# We add a vanilla hidden layer:\n",
    "out = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "model = Model(x, out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yay! Just a little better again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 310.00 387.00\" width=\"310pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 306,-383 306,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140206696956088 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140206696956088</title>\n",
       "<polygon fill=\"none\" points=\"43.5,-332.5 43.5,-378.5 258.5,-378.5 258.5,-332.5 43.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-351.8\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"120.5,-332.5 120.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"120.5,-355.5 175.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"175.5,-332.5 175.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"217\" y=\"-363.3\">(None, 400)</text>\n",
       "<polyline fill=\"none\" points=\"175.5,-355.5 258.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"217\" y=\"-340.3\">(None, 400)</text>\n",
       "</g>\n",
       "<!-- 140206692128976 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140206692128976</title>\n",
       "<polygon fill=\"none\" points=\"28.5,-249.5 28.5,-295.5 273.5,-295.5 273.5,-249.5 28.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68.5\" y=\"-268.8\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"108.5,-249.5 108.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"136\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"108.5,-272.5 163.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"136\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"163.5,-249.5 163.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218.5\" y=\"-280.3\">(None, 400)</text>\n",
       "<polyline fill=\"none\" points=\"163.5,-272.5 273.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218.5\" y=\"-257.3\">(None, 400, 300)</text>\n",
       "</g>\n",
       "<!-- 140206696956088&#45;&gt;140206692128976 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140206696956088-&gt;140206692128976</title>\n",
       "<path d=\"M151,-332.366C151,-324.152 151,-314.658 151,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-305.607 151,-295.607 147.5,-305.607 154.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140206696848352 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140206696848352</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-166.5 37.5,-212.5 264.5,-212.5 264.5,-166.5 37.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68.5\" y=\"-185.8\">Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"99.5,-166.5 99.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"99.5,-189.5 154.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"154.5,-166.5 154.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"209.5\" y=\"-197.3\">(None, 400, 300)</text>\n",
       "<polyline fill=\"none\" points=\"154.5,-189.5 264.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"209.5\" y=\"-174.3\">(None, 398, 250)</text>\n",
       "</g>\n",
       "<!-- 140206692128976&#45;&gt;140206696848352 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140206692128976-&gt;140206696848352</title>\n",
       "<path d=\"M151,-249.366C151,-241.152 151,-231.658 151,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-222.607 151,-212.607 147.5,-222.607 154.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140206698096344 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140206698096344</title>\n",
       "<polygon fill=\"none\" points=\"0,-83.5 0,-129.5 302,-129.5 302,-83.5 0,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68.5\" y=\"-102.8\">GlobalMaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"137,-83.5 137,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"137,-106.5 192,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"192,-83.5 192,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-114.3\">(None, 398, 250)</text>\n",
       "<polyline fill=\"none\" points=\"192,-106.5 302,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-91.3\">(None, 250)</text>\n",
       "</g>\n",
       "<!-- 140206696848352&#45;&gt;140206698096344 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140206696848352-&gt;140206698096344</title>\n",
       "<path d=\"M151,-166.366C151,-158.152 151,-148.658 151,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-139.607 151,-129.607 147.5,-139.607 154.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140206696847288 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140206696847288</title>\n",
       "<polygon fill=\"none\" points=\"57,-0.5 57,-46.5 245,-46.5 245,-0.5 57,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"107,-0.5 107,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"107,-23.5 162,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"162,-0.5 162,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.5\" y=\"-31.3\">(None, 250)</text>\n",
       "<polyline fill=\"none\" points=\"162,-23.5 245,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.5\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 140206698096344&#45;&gt;140206696847288 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140206698096344-&gt;140206696847288</title>\n",
       "<path d=\"M151,-83.3664C151,-75.1516 151,-65.6579 151,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"154.5,-56.6068 151,-46.6068 147.5,-56.6069 154.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model,show_shapes=True,show_layer_names=False).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good reading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8\n",
    "http://setosa.io/ev/image-kernels/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
