{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "bow_classifier_with_embeddings_simpler.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/Deep_Learning_in_LangTech_course/blob/master/bow_classifier_with_embeddings_simpler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27W85LMaYeQR",
        "colab_type": "text"
      },
      "source": [
        "# Bag-of-words classifier with pretrained word embeddings\n",
        "\n",
        "- During the lecture we will cover the concept of embeddings and the simple word2vec method\n",
        "- If we have a trained word embeddings model, we can transfer that knowledge into a new task and model. This is **transfer learning**\n",
        "- What we achieve here: Initialize the weights in the classifier with pretrained word embeddings\n",
        "- Word embeddings downloaded at: https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "\n",
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85jjQtPxYeQW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "b650158c-7174-48ad-dc7a-6e2031c3c2ba"
      },
      "source": [
        "%%script bash\n",
        "\n",
        "mkdir -p data # luodaan kasio \n",
        "cd data\n",
        "wget --quiet https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip # haetaan sinne pakattu malli\n",
        "unzip wiki-news-300d-1M.vec.zip # puretaan malli\n",
        "wget https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/raw/master/data/imdb_train.json # haetaan myös imdb-data\n",
        "cd .."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wiki-news-300d-1M.vec.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [wget http]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [s://githu]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [b.com/Tur]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [kuNLP/Dee]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [p_Learnin]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [g_in_Lang]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [Tech_cour]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [se/raw/ma]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [ster/data]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [/imdb_tra]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [in.json #]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [ haetaan ]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [myös imd]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [b-data]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [cd ..]\n",
            "replace wiki-news-300d-1M.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
            "(EOF or read error, treating as \"[N]one\" ...)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs-PTeXnYeQo",
        "colab_type": "code",
        "outputId": "112f6af3-2701-40b5-d878-286fb363a109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import json\n",
        "import random\n",
        "with open(\"data/imdb_train.json\") as f:\n",
        "    data=json.load(f)\n",
        "random.shuffle(data) \n",
        "print(data[0])\n",
        "\n",
        "# We need to gather the texts, into a list\n",
        "texts=[one_example[\"text\"] for one_example in data]\n",
        "labels=[one_example[\"class\"] for one_example in data]\n",
        "print(texts[:2])\n",
        "print(labels[:2])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'class': 'neg', 'text': 'Ripping this movie apart is like shooting fish in a barrel. It\\'s too easy. So I\\'m going to challenge myself to acknowledge the positive aspects of Little Man. First, I\\'m impressed with the special effects. It really did look like Marlon Wayans\\' head was attached to the body of a little person. I never doubted it for a minute.  Secondly, I loved some of the unexpected cameos. David Alan Grier played an annoying restaurant singer, and his renditions of \\\\Havin\\' My Baby\\\\\" and \\\\\"Movin\\' On Up\\\\\" were priceless. John Witherspoon, who, coincidentally, played Grier\\'s father in 1992\\'s Boomerang (if you remember, he \\\\\"coordinated\\\\\" the mushroom belt with the mushroom jacket) now plays Vanessa\\'s father in Little Man. So that was fun.  Beyond that, this movie is about as believable as White Chicks. How dumb is it when even the doctor can\\'t tell that it\\'s a 40-year-old man and not a baby? He\\'s got a full set of teeth!!! How is it possible that no one seems to notice that it\\'s not a baby? Little Man is so bad that there\\'s a Rob Schneider cameo. And please, if you\\'re stupid enough to waste $8 on this movie, at least do me a favor and DO NOT bring your children. This movie is way too sexual for small children (lots of jokes and innuendo about sex, going down, eating out, etc.), and I felt embarrassed for the parents who brought their kids to the screening I was forced to endure. If you insist on seeing an idiotic film, as least spare your children the pain and suffering.\"'}\n",
            "['Ripping this movie apart is like shooting fish in a barrel. It\\'s too easy. So I\\'m going to challenge myself to acknowledge the positive aspects of Little Man. First, I\\'m impressed with the special effects. It really did look like Marlon Wayans\\' head was attached to the body of a little person. I never doubted it for a minute.  Secondly, I loved some of the unexpected cameos. David Alan Grier played an annoying restaurant singer, and his renditions of \\\\Havin\\' My Baby\\\\\" and \\\\\"Movin\\' On Up\\\\\" were priceless. John Witherspoon, who, coincidentally, played Grier\\'s father in 1992\\'s Boomerang (if you remember, he \\\\\"coordinated\\\\\" the mushroom belt with the mushroom jacket) now plays Vanessa\\'s father in Little Man. So that was fun.  Beyond that, this movie is about as believable as White Chicks. How dumb is it when even the doctor can\\'t tell that it\\'s a 40-year-old man and not a baby? He\\'s got a full set of teeth!!! How is it possible that no one seems to notice that it\\'s not a baby? Little Man is so bad that there\\'s a Rob Schneider cameo. And please, if you\\'re stupid enough to waste $8 on this movie, at least do me a favor and DO NOT bring your children. This movie is way too sexual for small children (lots of jokes and innuendo about sex, going down, eating out, etc.), and I felt embarrassed for the parents who brought their kids to the screening I was forced to endure. If you insist on seeing an idiotic film, as least spare your children the pain and suffering.\"', 'This motion picture comes straight out of the dark dungeon of Full Moon Entertainment. This production company gained fame and fortune during the first half of the 90\\'s by producing terribly bad and cheesy horror movies. The most famous disasters in their ouvre are \\\\Subspecies\\\\\", \\\\\"Seedpeople\\\\\" and \\\\\"Trancers\\\\\". None of these are recommended and neither is Doctor Mordrid, actually. Hyperactive director Charles Band did come to the right company for his film. Doctor Mordrid is amazingly dumb and cheesy and almost completely humourless. I only saw it because it stars Jeffrey Combs. I learned that it can have several disadvantages if you\\'re a fan of him. For every good movie, it seems like he has made 5 inferior ones. Anyways, the story is about the battle between 2 ancient sorcerers. One good one who\\'s here since 150 years to protect the humans ( Jeffrey as Dr.Mordrid ) and one wicked one called Kabal. He wants to destroy every form of human life for some reason I already forgot. Combs gets his instructions from mentor. That \\\\\"guy\\\\\" only exists of a pair of eyes in space. Very very cheesy, that is ! Every once and a while a blinding lightflash is shown on the screen but that\\'s about the only form of Special effects this movie has got. The whole thing is just a piece of whining and nagging and when the two wizards finally face each other, it\\'s over before you know it...I would have expected for the wicked wizard to at least fight back a little, but nooooooo.... In some scenes, you really can detect some originality and creativity ( like for example Jeffrey\\'s lecture about the influence of the moon on criminals ) and if you really pay attention, you might even find some very small but nice aspects ( like the raven which is called Edgar Allen) but overall, it\\'s a terrible waste of time and energy. I\\'m a big fan of Jeffrey and maybe he is a superhero in my eyes...but he sure doesn\\'t have to put on a stupid maillot for that.\"']\n",
            "['neg', 'neg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6rjCM_3YeQy",
        "colab_type": "text"
      },
      "source": [
        "### Use gensim to read the embedding model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWzeNxAbYeQ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "05939a29-e2c2-4b6b-99a7-e83c6fa66e9c"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#Only grab the 100K most common entries\n",
        "vector_model = KeyedVectors.load_word2vec_format(\"data/wiki-news-300d-1M.vec\", binary=False, limit=100000)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0D-0R2fPeBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3236d6bc-83ba-4454-bec0-5121f232e359"
      },
      "source": [
        "vector_model"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f7eb585bcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YT0yQ_uYeRE",
        "colab_type": "text"
      },
      "source": [
        "## Working with the embeddings\n",
        "\n",
        "* `vector_model.vocab`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwQPAWfuYeRH",
        "colab_type": "code",
        "outputId": "345643e7-c1f8-42a9-fa39-37fa51786e8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# sort based on the index to make sure they are in the correct order\n",
        "words = [k for k, v in sorted(vector_model.vocab.items(), key = lambda x:x[1].index)]\n",
        "print(\"Words from embedding model:\", len(words))\n",
        "print(\"First 50 words:\", words[:50])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words from embedding model: 100000\n",
            "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlWvul4PYeRS",
        "colab_type": "text"
      },
      "source": [
        "### Normalize the vectors\n",
        "\n",
        "- Easier to learn on top of these vectors when the magnitude does not vary much"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxEeuxNfYeRX",
        "colab_type": "code",
        "outputId": "e7d7258d-6141-460c-9254-a2988475883e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "print(\"Before normalization:\", vector_model.get_vector(\"and\")[:10])\n",
        "\n",
        "vector_model.init_sims(replace = True)\n",
        "\n",
        "print(\"After normalization:\", vector_model.get_vector(\"and\")[:10])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before normalization: [-0.0314  0.0149 -0.0205  0.0557  0.0205 -0.0405  0.0044 -0.0118 -0.0424\n",
            " -0.049 ]\n",
            "After normalization: [-0.02372573  0.01125839 -0.01548973  0.04208673  0.01548973 -0.03060166\n",
            "  0.00332463 -0.00891604 -0.0320373  -0.03702423]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPrdychYeRh",
        "colab_type": "text"
      },
      "source": [
        "### Text analyzer and vectorizer\n",
        "\n",
        "- When we use an embedding layer (keras.layers.Embedding) the input data must be a sequence, not a bag-of-words vector\n",
        "- This prepares us for working with sequences, but we must give up on our trusty `CountVectorizer`\n",
        "- You can use CountVectorizer only as an analyzer without building the feature matrix\n",
        "- We will have to build the vectorizer part later ourselves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7e3CTSAYeRk",
        "colab_type": "code",
        "outputId": "6293ac42-68b7-4771-ccb0-4b8898b07fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer=\"word\", lowercase=False)\n",
        "analyzer = vectorizer.build_analyzer()\n",
        "analyzer(\"I, have a dog\") # CountVectorizerin oletusparametrina stop-list joka pudottaa yhden kirjaimen mittaiset sanat pois"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['have', 'dog']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R19qwKq9XFUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ffa8ea78-cffd-48c5-c225-07168266a16e"
      },
      "source": [
        "vector_model.vocab[\"and\"]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.keyedvectors.Vocab at 0x7f7eb3c19ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKTp9EH7YeRv",
        "colab_type": "text"
      },
      "source": [
        "# Vectorizing as a sequence\n",
        "\n",
        "* Each document is a row\n",
        "* Words are turned into indices, their order is preserved\n",
        "* We will have to introduce padding, since documents are of different lengths, but we will need to have a array\n",
        "* Padding: fill shorter documents with zeros at end until the length of the longest document is reached"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcG1u-x9YeRy",
        "colab_type": "code",
        "outputId": "76d494dd-fb2f-41ee-db3d-1cfc8e7b07db",
        "colab": {}
      },
      "source": [
        "def vectorize_into_sequences(texts, analyzer, vector_model):\n",
        "    result=[] # all docs, list of lists\n",
        "    for document in texts:\n",
        "        doc=[] # one doc\n",
        "        for w in analyzer(document): # tokenize with CountVectorizer: extract words\n",
        "            if w in vector_model.vocab: # is the word in the model vocab?\n",
        "                doc.append(vector_model.vocab[w].index+1) # if so, append corresponding value to result list AND +1 to make space for padding\n",
        "        result.append(doc)\n",
        "    return result\n",
        "\n",
        "seq = vectorize_into_sequential(texts, analyzer, vector_model)\n",
        "\n",
        "# Try it out!\n",
        "print(vectorize_into_sequential([\"I have a dog!\", \"The dog is used to produce a long sentence.\", \"Not so my cat.\"], analyzer, vector_model))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[37, 2370], [21, 2370, 14, 154, 6, 1153, 388, 939], [915, 58, 94, 3512]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxcAxZ4UYeSA",
        "colab_type": "text"
      },
      "source": [
        "* above is the vectorized data before padding\n",
        "* padding is quite easy, in the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8hsxeTNYeSC",
        "colab_type": "code",
        "outputId": "a7ef32df-295b-46c9-e882-3bfa32967272",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vectorized_data_padded = pad_sequences(seq, padding='post')\n",
        "print(\"Shape:\", vectorized_data_padded.shape) # dokumenttien määrä, pisimmän dokumentin pituus\n",
        "print(\"First example:\", vectorized_data_padded[0])\n",
        "\n",
        "# Pisin dokumentti määrää datan muodon. Muihin dokumentteihin lisätään\n",
        "# Kerasin avulla nollia, kunnes saadaan sama koko"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (25000, 2273)\n",
            "First example: [ 132 1115   23 ...    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjfz5omQYeSL",
        "colab_type": "text"
      },
      "source": [
        "...and that is our data, nicely padded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcQVdnNtYeSO",
        "colab_type": "text"
      },
      "source": [
        "### Labels into numerical vectors\n",
        "\n",
        "- Same as in the original BOW classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51TjM4LmYeSR",
        "colab_type": "code",
        "outputId": "8c59ad04-1cbf-4d4e-b05a-7e7b6fd2b215",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder=LabelEncoder() #Turns class labels into integers\n",
        "class_numbers=label_encoder.fit_transform(labels)\n",
        "print(\"class_numbers shape=\",class_numbers.shape)\n",
        "print(\"class_numbers\",class_numbers)\n",
        "print(\"class labels\",label_encoder.classes_)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class_numbers shape= (25000,)\n",
            "class_numbers [1 0 1 ... 0 0 0]\n",
            "class labels ['neg' 'pos']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMm5Te8MYeSZ",
        "colab_type": "text"
      },
      "source": [
        "## Network\n",
        "\n",
        "* The embedding matrix can be obtained straight from the vector_model\n",
        "* We have a little problem, though because we added a padding symbol at index 0\n",
        "* So now we need to add a row of zeros for it, or else our embedding lookup will be off by one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoTS1oGaYeSd",
        "colab_type": "code",
        "outputId": "fea0d5e5-f9e6-4443-a810-f9b19302f548",
        "colab": {}
      },
      "source": [
        "# This is where the embedding matrix is\n",
        "orig_embedding_matrix=vector_model.vectors\n",
        "print(\"Orig shape:\",orig_embedding_matrix.shape, orig_embedding_matrix.dtype)\n",
        "zero_line=numpy.zeros((1,orig_embedding_matrix.shape[1]),dtype=orig_embedding_matrix.dtype)\n",
        "#Stack the zeros on top of the embedding matrix\n",
        "embedding_matrix=numpy.vstack((zero_line,orig_embedding_matrix))\n",
        "print(\"New  shape:\",embedding_matrix.shape)\n",
        "print(\"First two rows:\", embedding_matrix[:2,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Orig shape: (100000, 300) float32\n",
            "New  shape: (100001, 300)\n",
            "First two rows: [[ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00]\n",
            " [ 1.0730e-01  8.9000e-03  6.0000e-04  5.5000e-03 -6.4600e-02 -6.0000e-02\n",
            "   4.5000e-02 -1.3300e-02 -3.5700e-02  4.3000e-02 -3.5600e-02 -3.2000e-03\n",
            "   7.3000e-03 -1.0000e-04  2.5800e-02 -1.6600e-02  7.5000e-03  6.8600e-02\n",
            "   3.9200e-02  7.5300e-02  1.1500e-02 -8.7000e-03  4.2100e-02  2.6500e-02\n",
            "  -6.0100e-02  2.4200e-01  1.9900e-02 -7.3900e-02 -3.1000e-03 -2.6300e-02\n",
            "  -6.2000e-03  1.6800e-02 -3.5700e-02 -2.4900e-02  1.9000e-02 -1.8400e-02\n",
            "  -5.3700e-02  1.4200e-01  6.0000e-02  2.2600e-02 -3.8000e-03 -6.7500e-02\n",
            "  -3.6000e-03 -8.0000e-03  5.7000e-02  2.0800e-02  2.2300e-02 -2.5600e-02\n",
            "  -1.5300e-02  2.2000e-03 -4.8200e-02  1.3100e-02 -6.0160e-01 -8.8000e-03\n",
            "   1.0600e-02  2.2900e-02  3.3600e-02  7.1000e-03  8.8700e-02  2.3700e-02\n",
            "  -2.9000e-02 -4.0500e-02 -1.2500e-02  1.4700e-02  4.7500e-02  6.4700e-02\n",
            "   4.7400e-02  1.9900e-02  4.0800e-02  3.2200e-02  3.6000e-03  3.5000e-02\n",
            "  -7.2300e-02 -3.0500e-02  1.8400e-02 -2.6000e-03  2.4000e-02 -1.6000e-02\n",
            "  -3.0800e-02  4.3400e-02  1.4700e-02 -4.5700e-02 -2.6700e-02 -1.7030e-01\n",
            "  -9.9000e-03  4.1700e-02  2.3500e-02 -2.6000e-02 -1.5190e-01 -1.1600e-02\n",
            "  -3.0600e-02 -4.1300e-02  3.3000e-02  7.2300e-02  3.6500e-02 -1.0000e-04\n",
            "   4.2000e-03  3.4600e-02  2.7700e-02 -3.0500e-02  7.8400e-02 -4.0400e-02\n",
            "   1.8700e-02 -2.2500e-02 -2.0600e-02 -1.7900e-02 -2.4280e-01  6.6900e-02\n",
            "   5.2300e-02  5.2700e-02  1.4900e-02 -7.0800e-02 -9.8700e-02  2.6300e-02\n",
            "  -6.1100e-02  3.0200e-02  2.1600e-02  3.1300e-02 -1.4000e-02 -2.4950e-01\n",
            "  -3.4600e-02 -4.8000e-02  2.5000e-02  2.1300e-01 -3.3000e-02 -1.5530e-01\n",
            "  -2.9200e-02 -3.4600e-02  1.0740e-01  1.0000e-03 -1.1700e-02 -5.7000e-03\n",
            "  -1.2800e-01 -3.8000e-03  1.3000e-02 -1.1570e-01 -1.0800e-02  2.7500e-02\n",
            "   1.5800e-02 -1.6900e-02  7.0000e-03  2.4700e-02  5.1000e-02  1.0292e+00\n",
            "  -2.8300e-02 -3.1000e-02 -2.6000e-03 -3.4300e-02  5.7800e-02  4.4400e-02\n",
            "   8.1200e-02 -2.1100e-02 -8.7200e-02  1.6900e-02  4.9900e-02  4.8500e-02\n",
            "   2.2700e-02 -3.2300e-02 -3.5000e-03  4.3500e-02 -2.7500e-02  1.5400e-02\n",
            "   1.3500e-02 -4.8400e-02 -6.9900e-02 -5.0200e-02  2.7450e-01 -3.0000e-04\n",
            "  -3.7100e-02  5.1700e-02 -9.0800e-02  1.3000e-03  3.6000e-02  2.8000e-02\n",
            "   8.3900e-02  9.8000e-02 -4.9000e-02 -2.4230e-01 -1.4200e-02  2.4000e-03\n",
            "  -2.0700e-02  1.2000e-03  8.8000e-03 -1.4300e-02 -1.9700e-02  5.1500e-02\n",
            "  -8.5000e-03  2.5700e-02  2.1540e-01  3.0100e-02  2.1100e-02  5.3000e-02\n",
            "  -5.0000e-04  1.7700e-02  1.6000e-03 -5.3000e-03 -1.6200e-02 -2.2300e-02\n",
            "  -1.8620e-01  3.9800e-02  6.5800e-02 -9.6200e-02 -7.6000e-03 -7.5000e-03\n",
            "  -3.4200e-02 -2.6500e-02  4.2000e-02  5.2200e-02 -2.6600e-02  2.0100e-02\n",
            "  -1.3310e-01 -3.6700e-02  3.5100e-02  5.1800e-02 -8.7000e-03  5.9900e-02\n",
            "  -1.0860e-01 -1.8800e-02  4.8100e-02  1.0500e-02 -6.0000e-03  1.5100e-02\n",
            "  -3.1000e-03  7.7000e-03 -2.7600e-02 -3.7300e-02 -2.0300e-02  4.7200e-02\n",
            "   2.4600e-02  1.4400e-01  5.4200e-02 -2.2500e-02  2.4950e-01  1.6170e-01\n",
            "   3.8000e-03  1.1190e-01 -2.3000e-02 -7.8500e-02  2.5000e-02 -6.1600e-02\n",
            "  -4.8500e-02  2.2500e-02  2.8100e-02  4.1000e-03  1.1200e-02  1.7200e-02\n",
            "   2.9100e-02 -2.8200e-02  2.6000e-03  4.0550e-01  3.9200e-02  8.8000e-03\n",
            "   2.2800e-02  2.9900e-02  1.1950e-01  5.4500e-02 -2.0000e-03  2.0000e-03\n",
            "   4.9000e-02  1.4500e-02 -8.6000e-03  9.8000e-03 -2.3600e-02  1.7100e-02\n",
            "  -7.6500e-02 -4.0000e-02  1.2800e-02  1.1000e-03  4.2000e-03  2.4400e-02\n",
            "   7.5000e-03  2.0000e-02  2.0100e-02  1.9600e-02 -3.7700e-02 -4.3200e-02\n",
            "  -7.3000e-03 -2.1000e-03  1.8300e-02  7.6000e-03  1.8050e-01 -5.5100e-02\n",
            "   7.5000e-03 -5.1600e-02  4.2000e-02 -6.8000e-03 -7.1100e-02 -1.4080e-01\n",
            "   5.0400e-02  2.7600e-02  4.7000e-02  3.2300e-02 -2.1900e-02  1.0000e-03\n",
            "   8.9000e-03  2.7600e-02  1.8600e-02  5.0000e-03  1.1730e-01 -4.0000e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "eWM4nQNFYeSl",
        "colab_type": "text"
      },
      "source": [
        "### Sequential input\n",
        "\n",
        "- Remember how the shape of the input data matrix had undefined number of columns\n",
        "- Now we must make it into fixed size (same for each example)\n",
        "- Padding: include zeros until you reach the correct size\n",
        "- You will hear more about this next week!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tV743F9YeSn",
        "colab_type": "text"
      },
      "source": [
        "### Our network structure:\n",
        "\n",
        "- Input layer, Embedding layer with pretrained weights, Average of embeddings, Non-linear activation, Classification layer\n",
        "- The key point here is the embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKyM8tPCYeSq",
        "colab_type": "code",
        "outputId": "c6290bba-994a-4eab-c88b-b66702d3ef24",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, GlobalAveragePooling1D\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "example_count,sequence_len=vectorized_data_padded.shape\n",
        "class_count=len(label_encoder.classes_)\n",
        "vector_size=embedding_matrix.shape[1] # embedding dim (\"hidden layer\") must be the same as in the pretrained model\n",
        "vocab_size=embedding_matrix.shape[0]\n",
        "\n",
        "inp=Input(shape=(sequence_len,))\n",
        "embeddings=Embedding(vocab_size, vector_size, mask_zero=True, weights=[embedding_matrix])(inp)\n",
        "# Suoriutumiseen vaikuttaa se, otetaanko inputkerroksen ja hiddenlayerin väliin painot valmiina \n",
        "# (transferlearning) vai treenataanko ne from scratch\n",
        "average_embeddings=GlobalAveragePooling1D()(embeddings) # is masking-aware\n",
        "hidden=Dense(50,activation=\"tanh\")(average_embeddings)\n",
        "outp=Dense(class_count, activation=\"softmax\")(hidden)\n",
        "model=Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "optimizer=Adam(lr=0.001) # define the learning rate\n",
        "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# train\n",
        "stop_cb=EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "hist=model.fit(vectorized_data_padded,class_numbers,batch_size=100,verbose=1,epochs=50,validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 2273)              0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 2273, 300)         30000300  \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50)                15050     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 102       \n",
            "=================================================================\n",
            "Total params: 30,015,452\n",
            "Trainable params: 30,015,452\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 22500 samples, validate on 2500 samples\n",
            "Epoch 1/50\n",
            "22500/22500 [==============================] - 221s 10ms/step - loss: 0.4113 - accuracy: 0.8184 - val_loss: 0.2762 - val_accuracy: 0.8816\n",
            "Epoch 2/50\n",
            "14100/22500 [=================>............] - ETA: 1:18 - loss: 0.1802 - accuracy: 0.9348"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-26f0311716d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_data_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_numbers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/venv-jlab/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m~/venv-jlab/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/venv-jlab/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m~/venv-jlab/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0sFFkDDYeS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"History:\",hist.history[\"val_acc\"])\n",
        "print(\"Max accuracy:\",numpy.max(hist.history[\"val_acc\"]))\n",
        "plt.ylim(0.85,1.0)\n",
        "plt.plot(hist.history[\"val_acc\"],label=\"Validation set accuracy\")\n",
        "plt.plot(hist.history[\"acc\"],label=\"Training set accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}