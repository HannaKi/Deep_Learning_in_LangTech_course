{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting properties of embeddings\n",
    "\n",
    "* We will look at some of the properties w2v embeddings have\n",
    "* Test them ourselves in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case you forgot, this is how one can load the embeddings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load embedding model from a file\n",
    "# binary: True if saved as binary file (.bin), False if saved as text file (.vectors or .txt for example)\n",
    "# limit: How many words to read from the model\n",
    "model_english=KeyedVectors.load_word2vec_format(\"data/gigaword-and-wikipedia.bin\", binary=True, limit=100000)\n",
    "model_finnish=KeyedVectors.load_word2vec_format(\"data/pb34_wf_200_v2_skgram.bin\", binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words for 'locomotive':\n",
      "[('locomotives', 0.9040292501449585), ('railcar', 0.8419736623764038), ('railcars', 0.806695282459259), ('locos', 0.7832419872283936), ('steam', 0.7655093669891357), ('bogie', 0.764204740524292), ('shunting', 0.7400067448616028), ('steam-powered', 0.7359079122543335), ('carriages', 0.7352656126022339), ('diesel-electric', 0.7294440269470215)]\n",
      "\n",
      "Most similar words for 'veturi':\n",
      "[('vaunu', 0.7221510410308838), ('veturin', 0.6715301275253296), ('raide', 0.6576318144798279), ('juna', 0.6463427543640137), ('kuorma-auto', 0.6372989416122437), ('perävaunu', 0.6346830725669861), ('veturina', 0.623757004737854), ('traktori', 0.6218146085739136), ('linja-auto', 0.6097179651260376), ('kiskoilla', 0.6076352596282959)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Most similar words for 'locomotive':\")\n",
    "print(model_english.most_similar(\"locomotive\",topn=10))\n",
    "print()\n",
    "print(\"Most similar words for 'veturi':\")\n",
    "print(model_finnish.most_similar(\"veturi\",topn=10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping spaces\n",
    "\n",
    "* One of the more famous properties of the embeddings\n",
    "* Learn a **linear** mapping from one language to another\n",
    "* Can we replicate this?\n",
    "* Learn a network with a single dense output layer\n",
    "* English vector in -- Finnish vector out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "* Need English-Finnish pairs of words to train on\n",
    "* ...google translate, maybe?\n",
    "* Googling around finds this https://github.com/ssut/py-googletrans\n",
    "* ...unofficial API, will get your IP banned if overused, so let's be careful!\n",
    "* official API needs registration, etc.\n",
    "\n",
    "`pip3 install --user googletrans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin= locomotive text= veturi\n",
      "origin= milk text= maito\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator=Translator()\n",
    "translations=translator.translate([\"locomotive\",\"milk\"],src=\"en\",dest=\"fi\")\n",
    "for t in translations:\n",
    "    print(\"origin=\",t.origin,\"text=\",t.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seems to work fine!\n",
    "* Let's grab some translations\n",
    "* The docs say \"max 16000 characters per request\"\n",
    "* We need to translate some hundreds of words at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab 100000\n",
      "<class 'dict'>\n",
      "Vocab(count:99365, index:635)\n",
      "[('</s>', <gensim.models.keyedvectors.Vocab object at 0x7f0cb02be860>), (',', <gensim.models.keyedvectors.Vocab object at 0x7f0cb02bef98>), ('the', <gensim.models.keyedvectors.Vocab object at 0x7f0cb02be320>), ('.', <gensim.models.keyedvectors.Vocab object at 0x7f0cb02be780>), ('of', <gensim.models.keyedvectors.Vocab object at 0x7f0cb02beef0>)]\n",
      "Freq sorted ['</s>', ',', 'the', '.', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(\"English vocab\",len(model_english.vocab))\n",
    "print(model_english.vocab.__class__)\n",
    "print(model_english.vocab[\"car\"])\n",
    "#We need a list, in order of frequency\n",
    "words=sorted(model_english.vocab.items(),key=lambda word_dim:word_dim[1].count,reverse=True)\n",
    "print(words[:5])\n",
    "words_freq_sorted=[w for w,_ in words]\n",
    "print(\"Freq sorted\",words_freq_sorted[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have things like `</s>` and `.` in the vocabulary, those we don't want to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'to', 'and', 'in', 'a', 'for', 'The', 'is', 'that', 'was', 'on', 'with', 'said', 'as', 'by', 'at', 'from', 'he', 'his']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "english_word_re=re.compile(\"^[a-zA-Z]+$\") #about as stupid simplification as you can get!\n",
    "final_word_list=[]\n",
    "for w in words_freq_sorted:\n",
    "    if english_word_re.match(w):\n",
    "        final_word_list.append(w)\n",
    "print(final_word_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Finnish ['ja', 'on', 'ei', 'että', 'se', 'oli', 'mutta', 'tai', 'kun', 'myös', 'ovat', 'ole', 'niin', 'jos', 'kuin'] ... ['seurakunnan', 'selvä', 'tulleet', 'seuraavaan', 'sijasta', 'kuollut', 'I', 'asioihin', 'loput', 'luona', 'talven', 'per', 'ihanan', 'palvelua', 'tietokoneen']\n",
      "Final English ['the', 'of', 'to', 'and', 'in', 'a', 'for', 'The', 'is', 'that', 'was', 'on', 'with', 'said', 'as'] ... ['Greece', 'rally', 'democracy', 'revenue', 'add', 'criticism', 'offices', 'Hussein', 'kids', 'relief', 'promised', 'advance', 'talking', 'boost', 'dispute']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#same thing as above, nicely packed into a function\n",
    "def clean_vocab(gensim_model,regexp):\n",
    "    words=sorted(gensim_model.vocab.items(),key=lambda word_dim:word_dim[1].count,reverse=True)\n",
    "    words_freq_sorted=[w for w,_ in words]\n",
    "    word_re=re.compile(regexp)\n",
    "    final_word_list=[]\n",
    "    for w in words_freq_sorted:\n",
    "        if word_re.match(w):\n",
    "            final_word_list.append(w)\n",
    "    return final_word_list\n",
    "\n",
    "finnish_vocab=clean_vocab(model_finnish,'^[a-zA-ZäöåÖÄÅ]+$')\n",
    "english_vocab=clean_vocab(model_english,'^[a-zA-Z]+$')\n",
    "print(\"Final Finnish\",finnish_vocab[:15],\"...\",finnish_vocab[2000:2015])\n",
    "print(\"Final English\",english_vocab[:15],\"...\",english_vocab[2000:2015])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en -> fi batch at 0 ....OK\n",
      "en -> fi batch at 20 ....OK\n",
      "en -> fi batch at 40 ....OK\n"
     ]
    }
   ],
   "source": [
    "#Little test\n",
    "import time\n",
    "def translate(words,src,dest,batch_size=1000):\n",
    "    result=[] #[(\"dog\",\"koira\"),....]\n",
    "    translator=Translator()\n",
    "    for idx in range(0,len(words),batch_size):\n",
    "        batch=words[idx:idx+batch_size]\n",
    "        try:\n",
    "            translations=translator.translate(batch,src=src,dest=dest)\n",
    "            for t in translations:\n",
    "                result.append((t.origin,t.text))\n",
    "            time.sleep(0.2) #sleep between batches\n",
    "            print(src,\"->\",dest,\"batch at\",idx,\"....OK\")\n",
    "        except: #we end here, if the lines between try ... except throw an error\n",
    "            print(src,\"->\",dest,\"batch at\",idx,\"....FAILED\")\n",
    "            time.sleep(61) #sleep a little longer so Google is not angry\n",
    "            print(src,\"->\",dest,\"...RESTARTING\")\n",
    "            \n",
    "    return result\n",
    "\n",
    "x=translate(english_vocab[:50],\"en\",\"fi\",20) # a small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', ''), ('of', 'of'), ('to', 'että'), ('and', 'ja'), ('in', 'sisään'), ('a', ''), ('for', 'varten'), ('The', ''), ('is', 'on'), ('that', 'että'), ('was', 'oli'), ('on', 'päällä'), ('with', 'kanssa'), ('said', 'sanoi'), ('as', 'kuten'), ('by', 'mennessä'), ('at', 'at'), ('from', 'alkaen'), ('he', 'hän'), ('his', 'hänen'), ('it', 'se'), ('be', 'olla'), ('are', 'olemme'), ('an', ''), ('has', 'on'), ('have', 'omistaa'), ('were', 'olivat'), ('not', 'ei'), ('who', 'Kuka'), ('had', 'oli'), ('which', 'joka'), ('will', 'tahtoa'), ('or', 'tai'), ('their', 'heidän'), ('but', 'mutta'), ('its', 'sen'), ('In', 'Sisään'), ('this', 'Tämä'), ('they', 'ne'), ('been', 'ollut'), ('I', 'minä'), ('also', 'myös'), ('would', 'olisi'), ('one', 'yksi'), ('He', 'Hän'), ('after', 'jälkeen'), ('more', 'lisää'), ('two', 'kaksi'), ('first', 'ensimmäinen'), ('about', 'noin')]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* looks okay\n",
    "* let's run this and save the result for later use, so we don't get banned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en -> fi batch at 0 ....OK\n",
      "en -> fi batch at 150 ....OK\n",
      "en -> fi batch at 300 ....OK\n",
      "en -> fi batch at 450 ....OK\n",
      "en -> fi batch at 600 ....OK\n",
      "en -> fi batch at 750 ....OK\n",
      "en -> fi batch at 900 ....OK\n",
      "en -> fi batch at 1050 ....FAILED\n",
      "en -> fi ...RESTARTING\n",
      "en -> fi batch at 1200 ....FAILED\n",
      "en -> fi ...RESTARTING\n",
      "en -> fi batch at 1350 ....FAILED\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-44b7f1a72387>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(words, src, dest, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mtranslations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    318\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6860c2a42a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0men_fi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_fi_transl.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_fi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfi_en\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinnish_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-44b7f1a72387>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(words, src, dest, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"batch at\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"....FAILED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m61\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sleep a little longer so Google is not angry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"...RESTARTING\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "en_fi=translate(english_vocab,\"en\",\"fi\",batch_size=150)\n",
    "with open(\"en_fi_transl.json\",\"wt\") as f:\n",
    "    json.dump(en_fi,f)\n",
    "fi_en=translate(finnish_vocab,\"fi\",\"en\",batch_size=150)\n",
    "with open(\"fi_en_transl.json\",\"wt\") as f:\n",
    "    json.dump(fi_en,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* well, we got banned :D\n",
    "* Let's just translate as text files, in the google translate interface\n",
    "* (quality time manually feeding these into Google translate --- I could have used the official API :)\n",
    "* ...but now it's done, so who cares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump 10K words at a time into a file, which can be fed to google translate\n",
    "def build_files(words,fname,batch_size):\n",
    "    for idx in range(0,len(words),batch_size):\n",
    "        batch=words[idx:idx+batch_size]\n",
    "        with open(\"trdata/{}_batch_{}.txt\".format(fname,idx),\"wt\") as f:\n",
    "            print(\"\\n\".join(batch),file=f)\n",
    "\n",
    "build_files(english_vocab,\"en-fi-source\",10000)\n",
    "build_files(finnish_vocab,\"fi-en-source\",10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I built manually four files like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trdata/enfi_source_all.txt\n",
      "trdata/enfi_target_all.txt\n",
      "trdata/fien_source_all.txt\n",
      "trdata/fien_target_all.txt\n",
      "  95275 trdata/fien_source_all.txt\n",
      "  95275 trdata/fien_target_all.txt\n",
      "  83618 trdata/enfi_source_all.txt\n",
      "  83618 trdata/enfi_target_all.txt\n",
      " 357786 total\n",
      "FI -> EN\n",
      "ja\tand\n",
      "on\tis\n",
      "ei\tNo\n",
      "että\tthat\n",
      "se\tit\n",
      "oli\twas\n",
      "mutta\tbut\n",
      "tai\tor\n",
      "kun\twhen\n",
      "myös\talso\n",
      "EN -> FI\n",
      "the\t\n",
      "of\tof\n",
      "to\tettä\n",
      "and\tja\n",
      "in\tsisään\n",
      "a\t\n",
      "for\tvarten\n",
      "The\t\n",
      "is\ton\n",
      "that\tettä\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls trdata/fien_* trdata/enfi_*\n",
    "wc -l trdata/fien_* trdata/enfi_*\n",
    "echo \"FI -> EN\"\n",
    "paste trdata/fien_source_all.txt trdata/fien_target_all.txt  | head -n 10\n",
    "echo \"EN -> FI\"\n",
    "paste trdata/enfi_source_all.txt trdata/enfi_target_all.txt  | head -n 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read in and filter\n",
    "* To make sure we get high-quality stuff, we will look for same pairs in fin-eng and eng-fin direction\n",
    "* That way we will also make sure our translations are among the top 100K words in both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len fien 95275\n",
      "Len enfi 83610\n",
      "Len common 7100\n",
      "[('vuokrata', 'rent'), ('Jenkins', 'Jenkins'), ('Roses', 'Roses'), ('Beth', 'Beth'), ('TR', 'TR'), ('taantuma', 'downturn'), ('maskotti', 'mascot'), ('Antonin', 'Antonin'), ('kroonisesti', 'chronically'), ('kompromisseja', 'compromises'), ('tunnollisesti', 'scrupulously'), ('tuntea', 'feel'), ('Linus', 'Linus'), ('innoissaan', 'excited'), ('Napoli', 'Naples'), ('haitata', 'hinder'), ('koskematon', 'pristine'), ('Fiona', 'Fiona'), ('muistio', 'memorandum'), ('muslimit', 'Muslims'), ('safari', 'safari'), ('kiima', 'rut'), ('lopulta', 'eventually'), ('nämä', 'these'), ('mekanismit', 'mechanisms'), ('matkamuistoja', 'souvenirs'), ('Nigerian', 'Nigerian'), ('sari', 'sari'), ('petojen', 'beasts'), ('kutistuu', 'shrinking'), ('Camilla', 'Camilla'), ('Lajunen', 'Lajunen'), ('Joe', 'Joe'), ('tuote', 'product'), ('juhlapäivät', 'holidays'), ('XP', 'XP'), ('hyppii', 'hopping'), ('opaskirja', 'guidebook'), ('Dalai', 'Dalai'), ('Melissa', 'Melissa'), ('keskittyminen', 'concentration'), ('ystävät', 'friends'), ('runoilijat', 'poets'), ('vaatimaton', 'modest'), ('kasinot', 'casinos'), ('syy', 'reason'), ('tottuneesti', 'expertly'), ('Jaffa', 'Jaffa'), ('enimmäkseen', 'mostly'), ('Suzuki', 'Suzuki'), ('maailmanloppu', 'apocalypse'), ('Bon', 'Bon'), ('mennyt', 'gone'), ('tykkää', 'likes'), ('vapaaehtoisesti', 'voluntarily'), ('Thatcher', 'Thatcher'), ('voittaja', 'winner'), ('herkkyys', 'sensitivity'), ('jättäen', 'leaving'), ('järjestetty', 'organized'), ('syvä', 'deep'), ('Merkel', 'Merkel'), ('Einar', 'Einar'), ('mac', 'mac'), ('saaret', 'islands'), ('terrori', 'terror'), ('neuvotella', 'negotiate'), ('kierrosta', 'rounds'), ('hengitys', 'respiratory'), ('oivallus', 'realization'), ('siisti', 'neat'), ('analysoidaan', 'analyzed'), ('kohdanneet', 'encountered'), ('sushi', 'sushi'), ('Toshiba', 'Toshiba'), ('duo', 'duo'), ('Ronin', 'Ronin'), ('Frans', 'Frans'), ('bonus', 'bonus'), ('Feng', 'Feng'), ('Bach', 'Bach'), ('tuloaan', 'joining'), ('jo', 'already'), ('Ignatius', 'Ignatius'), ('erikoistuneet', 'specialize'), ('AEG', 'AEG'), ('että', 'that'), ('sivistynyt', 'civilized'), ('ku', 'ku'), ('WD', 'WD'), ('myös', 'also'), ('luterilaisuus', 'Lutheranism'), ('juoma', 'beverage'), ('kirjallinen', 'written'), ('antelias', 'generous'), ('rotta', 'rat'), ('Massachusetts', 'Massachusetts'), ('kaappaa', 'captures'), ('postikulut', 'postage'), ('kalmari', 'squid'), ('tutkia', 'explore'), ('intohimoisesti', 'passionately'), ('Luca', 'Luca'), ('Marek', 'Marek'), ('perinteinen', 'traditional'), ('lukija', 'reader'), ('myrsky', 'storm'), ('raivo', 'rage'), ('epäonnistuu', 'fails'), ('kurdien', 'Kurdish'), ('kitistä', 'whine'), ('interaktiivinen', 'interactive'), ('erottaa', 'distinguish'), ('ilmoitti', 'announced'), ('vallitseva', 'prevalent'), ('Emmy', 'Emmy'), ('läpikuultava', 'translucent'), ('merkittävästi', 'significantly'), ('käytettävyys', 'usability'), ('muotoinen', 'shaped'), ('malaria', 'malaria'), ('sairaalat', 'hospitals'), ('osasto', 'department'), ('olennaiseen', 'essentials'), ('Forza', 'Forza'), ('pyöräily', 'cycling'), ('toimitukset', 'deliveries'), ('murre', 'dialect'), ('Armin', 'Armin'), ('Kia', 'Kia'), ('kiiltävä', 'shiny'), ('teki', 'did'), ('juoksu', 'running'), ('hylätä', 'reject'), ('maksettava', 'due'), ('Liz', 'Liz'), ('pyyhe', 'towel'), ('verkko', 'network'), ('keskikenttäpelaaja', 'midfielder'), ('pehmeästi', 'softly'), ('kuninkaallinen', 'royal'), ('ja', 'and'), ('rapeita', 'crunchy'), ('ikuisuus', 'eternity'), ('Thorp', 'Thorp'), ('kehyksiä', 'frames'), ('työnantaja', 'employer'), ('kuisti', 'porch'), ('Palms', 'Palms'), ('Netflix', 'Netflix'), ('NBC', 'NBC'), ('marsalkka', 'marshal'), ('kohdun', 'uterine'), ('itsenäisesti', 'independently'), ('mehut', 'juices'), ('erehtyä', 'err'), ('traagisesti', 'tragically'), ('monni', 'catfish'), ('par', 'par'), ('TNS', 'TNS'), ('julkkikset', 'celebrities'), ('tekijät', 'factors'), ('juoksi', 'ran'), ('Steve', 'Steve'), ('lisääntyy', 'increases'), ('IBM', 'IBM'), ('Rami', 'Rami'), ('RK', 'RK'), ('Viktoria', 'Viktoria'), ('Sherman', 'Sherman'), ('puutarhuri', 'gardener'), ('legendaarinen', 'legendary'), ('katkeruus', 'bitterness'), ('pelata', 'play'), ('oravia', 'squirrels'), ('tuhonnut', 'destroyed'), ('Atria', 'Atria'), ('päättelee', 'concludes'), ('riittävyys', 'adequacy'), ('harjoitellaan', 'practicing'), ('DE', 'DE'), ('nostalginen', 'nostalgic'), ('Sisko', 'Sister'), ('lämpö', 'heat'), ('valaiseva', 'illustrative'), ('kylpytakki', 'bathrobe'), ('monimuotoisuus', 'diversity'), ('paitsi', 'except'), ('remmi', 'thong'), ('monikko', 'plural'), ('nuo', 'those'), ('Spielberg', 'Spielberg'), ('pyöristää', 'round'), ('des', 'des'), ('Nino', 'Nino'), ('vaarallinen', 'dangerous'), ('välttämättömyys', 'necessity'), ('Hugo', 'Hugo'), ('hajoaminen', 'decomposition'), ('Patrik', 'Patrik'), ('postilaatikko', 'mailbox'), ('laukku', 'bag'), ('terveiset', 'regards'), ('häpeä', 'shame'), ('tuntematon', 'unknown'), ('olemassaolo', 'existence'), ('hankkeet', 'projects'), ('ahkera', 'diligent'), ('sovittaminen', 'adaptation'), ('pimeys', 'darkness'), ('alas', 'down'), ('ammunta', 'shooting'), ('opastus', 'guidance'), ('PT', 'PT'), ('ti', 'ti'), ('Pavel', 'Pavel'), ('Monroe', 'Monroe'), ('jatkuva', 'continuous'), ('Kira', 'Kira'), ('kHz', 'kHz'), ('Jokinen', 'Jokinen'), ('college', 'college'), ('Lehtinen', 'Lehtinen'), ('rakenne', 'structure'), ('suoristaa', 'straighten'), ('puuha', 'chore'), ('kanssa', 'with'), ('liikunta', 'exercise'), ('jyrkkä', 'steep'), ('toteutukset', 'implementations'), ('kiusata', 'bully'), ('tuottajia', 'producers'), ('luokiteltu', 'classified'), ('mainonta', 'advertising'), ('kytkökset', 'affiliations'), ('tönäisi', 'poking'), ('Benin', 'Benin'), ('pelastaja', 'savior'), ('toistaa', 'repeat'), ('apteekki', 'pharmacy'), ('opetukset', 'teachings'), ('komposti', 'compost'), ('lajikkeet', 'varieties'), ('Jeremy', 'Jeremy'), ('Christina', 'Christina'), ('kuoleva', 'moribund'), ('koulunkäynti', 'schooling'), ('adrenaliini', 'adrenaline'), ('kestävä', 'resistant'), ('kissa', 'cat'), ('kohota', 'rise'), ('ässä', 'ace'), ('Frida', 'Frida'), ('kiitollisuus', 'gratitude'), ('Milla', 'Milla'), ('Jennifer', 'Jennifer'), ('valvoi', 'oversaw'), ('PDF', 'PDF'), ('liittimet', 'connectors'), ('välillä', 'between'), ('terva', 'tar'), ('töykeä', 'rude'), ('Vince', 'Vince'), ('Wesley', 'Wesley'), ('tarkkuus', 'accuracy'), ('valokuvaajat', 'photographers'), ('nimellisesti', 'nominally'), ('verkkotunnuksen', 'domain'), ('käänteinen', 'reverse'), ('lounaat', 'lunches'), ('Anaheim', 'Anaheim'), ('Ivan', 'Ivan'), ('Gavin', 'Gavin'), ('WSOP', 'WSOP'), ('Eddie', 'Eddie'), ('vaaleanpunainen', 'pink'), ('kaksinkertaistunut', 'doubled'), ('ajatus', 'thought'), ('äidin', 'maternal'), ('karma', 'karma'), ('vankila', 'prison'), ('uhmaa', 'defying'), ('kuuluva', 'audible'), ('Vincent', 'Vincent'), ('kuninkaat', 'kings'), ('vastauksia', 'answers'), ('lievä', 'mild'), ('useat', 'several'), ('pirullinen', 'devilish'), ('Drew', 'Drew'), ('KTM', 'KTM'), ('määritellä', 'define'), ('TK', 'TK'), ('erityisesti', 'specially'), ('Palma', 'Palma'), ('Bieber', 'Bieber'), ('HDMI', 'HDMI'), ('Parker', 'Parker'), ('Acer', 'Acer'), ('Sierra', 'Sierra')]\n"
     ]
    }
   ],
   "source": [
    "fien=[] #list of (fin,eng) pairs obtained from the fin -> eng direction\n",
    "enfi=[] #list of (fin,eng) pairs, this time obtained from  the eng->fin direction\n",
    "with open(\"trdata/fien_source_all.txt\") as fi_file, open(\"trdata/fien_target_all.txt\") as en_file:\n",
    "    for fi,en in zip(fi_file,en_file):\n",
    "        fi=fi.strip()\n",
    "        en=en.strip()\n",
    "        if fi and en:\n",
    "            fien.append((fi,en))\n",
    "\n",
    "with open(\"trdata/enfi_target_all.txt\") as fi_file, open(\"trdata/enfi_source_all.txt\") as en_file:\n",
    "    for fi,en in zip(fi_file,en_file):\n",
    "        fi=fi.strip()\n",
    "        en=en.strip()\n",
    "        if fi and en:\n",
    "            enfi.append((fi,en))\n",
    "\n",
    "fien_set=set(fien)\n",
    "enfi_set=set(enfi)\n",
    "common=fien_set&enfi_set #keep only pairs which are shared\n",
    "print(\"Len fien\",len(fien_set))\n",
    "print(\"Len enfi\",len(enfi_set))\n",
    "print(\"Len common\",len(common))\n",
    "print(list(common)[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ouch - we lost most of the stuff, but such is life\n",
    "* what we got looks good, though :)\n",
    "* Let us yet filter away pairs like Ivan - Ivan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7100\n",
      "7100\n",
      "7100\n",
      "7100\n",
      "...all these four numbers should be the same\n"
     ]
    }
   ],
   "source": [
    "#Making sure all we found is in the top 100K - just crosschecking really\n",
    "print(len(set(finnish_vocab)&set(fi for fi,en in common)))\n",
    "print(len(set(english_vocab)&set(en for fi,en in common)))\n",
    "\n",
    "#Making sure all words are there exactly once - no risk of mixing train and validation\n",
    "print(len(set(fi for fi,en in common)))\n",
    "print(len(set(en for fi,en in common)))\n",
    "print(\"...all these four numbers should be the same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left with 4624 after removing identical pairs\n",
      "Shuffled pairs [('tutkijat', 'scientists'), ('vahtikoira', 'watchdog'), ('ärsyttää', 'annoy'), ('taksit', 'taxis'), ('vampyyrit', 'vampires'), ('kuningatar', 'queen'), ('suoritettavan', 'executable'), ('tuoksu', 'scent'), ('kiekko', 'puck'), ('taajuus', 'frequency'), ('kulttuurisesti', 'culturally'), ('akku', 'battery'), ('kitaristi', 'guitarist'), ('jäsenyys', 'membership'), ('kuva', 'picture'), ('perinteisesti', 'traditionally'), ('opastus', 'guidance'), ('että', 'that'), ('hamstrata', 'hoard'), ('jäljittää', 'trace')]\n",
      "Indices: [2552, 7487, 56671, 20734, 30688, 6869, 64895, 23148, 12150, 7147] [3011, 97132, 4169, 44914, 43348, 11172, 53182, 4434, 11044, 31671]\n",
      "English model.vectors shape: (100000, 200)\n",
      "Finnish model.vectors shape: (100000, 200)\n",
      "English selected vectors shape: (4624, 200)\n",
      "Finnish selected vectors shape: (4624, 200)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "pairs=[(fi,en) for fi,en in common if fi!=en] #Only keep pairs where source does not equal target\n",
    "print(\"Left with\",len(pairs),\"after removing identical pairs\")\n",
    "random.shuffle(pairs) #always, always make sure to shuffle!\n",
    "\n",
    "print(\"Shuffled pairs\",pairs[:20])\n",
    "\n",
    "#Now we need to grab the vectors for the words in question\n",
    "en_indices=[model_english.vocab[en].index for fi,en in pairs] #English\n",
    "fi_indices=[model_finnish.vocab[fi].index for fi,en in pairs] #Finnish\n",
    "print(\"Indices:\",en_indices[:10],fi_indices[:10])\n",
    "#...and the vectors are hidden in the models\n",
    "print(\"English model.vectors shape:\",model_english.vectors.shape)\n",
    "print(\"Finnish model.vectors shape:\",model_finnish.vectors.shape)\n",
    "en_vectors=model_english.vectors[en_indices] #Selects the rows in just the correct order\n",
    "fi_vectors=model_finnish.vectors[fi_indices] #Selects the rows in just the correct order\n",
    "print(\"English selected vectors shape:\",en_vectors.shape)\n",
    "print(\"Finnish selected vectors shape:\",fi_vectors.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now `en_vectors` is vectors for the 4624 English words in our translation pairs\n",
    "* `fi_vectors` is same for Finnish\n",
    "* ...our training data is done - we have the pairs of input--desired output\n",
    "\n",
    "## Learning transformation from English to Finnish\n",
    "\n",
    "* 200-dim vector in, 200-dim vector out\n",
    "* Loss needs to be different, this is not classification!\n",
    "* `mse` stands for mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_47 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 200)               40200     \n",
      "=================================================================\n",
      "Total params: 40,200\n",
      "Trainable params: 40,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4161 samples, validate on 463 samples\n",
      "Epoch 1/30\n",
      "4161/4161 [==============================] - 3s 718us/step - loss: 0.0820 - val_loss: 0.0639\n",
      "Epoch 2/30\n",
      "4161/4161 [==============================] - 1s 189us/step - loss: 0.0602 - val_loss: 0.0529\n",
      "Epoch 3/30\n",
      "4161/4161 [==============================] - 1s 124us/step - loss: 0.0515 - val_loss: 0.0473\n",
      "Epoch 4/30\n",
      "4161/4161 [==============================] - 1s 148us/step - loss: 0.0467 - val_loss: 0.0441\n",
      "Epoch 5/30\n",
      "4161/4161 [==============================] - 1s 222us/step - loss: 0.0438 - val_loss: 0.0421\n",
      "Epoch 6/30\n",
      "4161/4161 [==============================] - 1s 134us/step - loss: 0.0419 - val_loss: 0.0407\n",
      "Epoch 7/30\n",
      "4161/4161 [==============================] - 1s 121us/step - loss: 0.0406 - val_loss: 0.0399\n",
      "Epoch 8/30\n",
      "4161/4161 [==============================] - 1s 125us/step - loss: 0.0397 - val_loss: 0.0392\n",
      "Epoch 9/30\n",
      "4161/4161 [==============================] - 1s 122us/step - loss: 0.0390 - val_loss: 0.0388\n",
      "Epoch 10/30\n",
      "4161/4161 [==============================] - 1s 128us/step - loss: 0.0385 - val_loss: 0.0385\n",
      "Epoch 11/30\n",
      "4161/4161 [==============================] - 1s 159us/step - loss: 0.0381 - val_loss: 0.0383\n",
      "Epoch 12/30\n",
      "4161/4161 [==============================] - 0s 98us/step - loss: 0.0378 - val_loss: 0.0381\n",
      "Epoch 13/30\n",
      "4161/4161 [==============================] - 0s 77us/step - loss: 0.0376 - val_loss: 0.0380\n",
      "Epoch 14/30\n",
      "4161/4161 [==============================] - 0s 44us/step - loss: 0.0374 - val_loss: 0.0379\n",
      "Epoch 15/30\n",
      "4161/4161 [==============================] - 0s 54us/step - loss: 0.0373 - val_loss: 0.0379\n",
      "Epoch 16/30\n",
      "4161/4161 [==============================] - 0s 52us/step - loss: 0.0372 - val_loss: 0.0379\n",
      "Epoch 17/30\n",
      "4161/4161 [==============================] - 0s 43us/step - loss: 0.0371 - val_loss: 0.0378\n",
      "Epoch 18/30\n",
      "4161/4161 [==============================] - 0s 53us/step - loss: 0.0370 - val_loss: 0.0378\n",
      "Epoch 19/30\n",
      "4161/4161 [==============================] - 0s 56us/step - loss: 0.0370 - val_loss: 0.0378\n",
      "Epoch 20/30\n",
      "4161/4161 [==============================] - 0s 55us/step - loss: 0.0369 - val_loss: 0.0378\n",
      "Epoch 21/30\n",
      "4161/4161 [==============================] - 0s 86us/step - loss: 0.0369 - val_loss: 0.0378\n",
      "Epoch 22/30\n",
      "4161/4161 [==============================] - 0s 69us/step - loss: 0.0369 - val_loss: 0.0378\n",
      "Epoch 23/30\n",
      "4161/4161 [==============================] - 0s 63us/step - loss: 0.0368 - val_loss: 0.0378\n",
      "Epoch 24/30\n",
      "4161/4161 [==============================] - 0s 80us/step - loss: 0.0368 - val_loss: 0.0378\n",
      "Epoch 25/30\n",
      "4161/4161 [==============================] - 0s 66us/step - loss: 0.0368 - val_loss: 0.0378\n",
      "Epoch 26/30\n",
      "4161/4161 [==============================] - 0s 63us/step - loss: 0.0368 - val_loss: 0.0378\n",
      "Epoch 27/30\n",
      "4161/4161 [==============================] - 0s 80us/step - loss: 0.0368 - val_loss: 0.0378\n",
      "Epoch 28/30\n",
      "4161/4161 [==============================] - 0s 75us/step - loss: 0.0368 - val_loss: 0.0379\n",
      "Epoch 29/30\n",
      "4161/4161 [==============================] - 0s 83us/step - loss: 0.0368 - val_loss: 0.0378\n",
      "Epoch 30/30\n",
      "4161/4161 [==============================] - 0s 65us/step - loss: 0.0368 - val_loss: 0.0379\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "\n",
    "\n",
    "inp=Input(shape=(en_vectors.shape[1],)) #input is 200-dim\n",
    "outp=Dense(fi_vectors.shape[1])(inp) #Simple linear transformation of the input\n",
    "\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"mse\")\n",
    "hist=model.fit(en_vectors,fi_vectors,batch_size=100,verbose=1,epochs=30,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('small', 1.0)]\n",
      "[('pieni', 1.0)]\n",
      "[('pieni', 0.7597441673278809), ('suurehko', 0.745261549949646), ('pienehkö', 0.7438690662384033), ('pikkuruinen', 0.7236558198928833), ('isohko', 0.7211355566978455)]\n",
      "\n",
      "\n",
      "[('charmer', 1.0)]\n",
      "[('hurmuri', 1.0)]\n",
      "[('herttainen', 0.7314914464950562), ('hymyilevä', 0.7270362377166748), ('hyväsydäminen', 0.7208757400512695), ('hurmuri', 0.719153881072998), ('veijari', 0.7173742055892944)]\n",
      "\n",
      "\n",
      "[('bachelor', 1.0)]\n",
      "[('poikamies', 1.0)]\n",
      "[('akateeminen', 0.6675968170166016), ('psykologia', 0.6520403027534485), ('tutkinto', 0.6477102041244507), ('koulutusohjelma', 0.6299420595169067), ('ylioppilas', 0.6283469200134277)]\n",
      "\n",
      "\n",
      "[('assignment', 0.9999999403953552)]\n",
      "[('toimeksianto', 0.9999999403953552)]\n",
      "[('työtehtävä', 0.6690058708190918), ('toimeksianto', 0.6338319778442383), ('aloitus', 0.6132286787033081), ('työ', 0.60988450050354), ('haastattelu', 0.6040139198303223)]\n",
      "\n",
      "\n",
      "[('settle', 1.0)]\n",
      "[('asettua', 1.0)]\n",
      "[('sovitella', 0.6602680683135986), ('neuvotella', 0.6348471641540527), ('ratkaista', 0.630579948425293), ('oikaista', 0.611323356628418), ('purkaa', 0.6111172437667847)]\n",
      "\n",
      "\n",
      "[('socializing', 1.0)]\n",
      "[('seurusteluun', 1.0)]\n",
      "[('yhdessäolo', 0.6868301630020142), ('puuhastelu', 0.6547372937202454), ('jutustelu', 0.6454211473464966), ('hauskanpito', 0.644983172416687), ('kanssakäyminen', 0.6353316903114319)]\n",
      "\n",
      "\n",
      "[('far', 1.0)]\n",
      "[('pitkälle', 1.0)]\n",
      "[('vaikkakin', 0.5960040092468262), ('kuitenkin', 0.5922564268112183), ('silti', 0.5909441709518433), ('mutta', 0.5851216316223145), ('heikosti', 0.5690462589263916)]\n",
      "\n",
      "\n",
      "[('indivisible', 1.0)]\n",
      "[('jakamaton', 1.0)]\n",
      "[('aineellinen', 0.6903623342514038), ('moraalinen', 0.6889781951904297), ('jumalallinen', 0.6882413029670715), ('jakamaton', 0.6867413520812988), ('universaali', 0.6784636378288269)]\n",
      "\n",
      "\n",
      "[('curly', 1.0)]\n",
      "[('kihara', 0.9999999403953552)]\n",
      "[('vaalea', 0.7531816959381104), ('vaaleanruskea', 0.7491306662559509), ('otsatukka', 0.7464267015457153), ('ruskea', 0.7457467913627625), ('letti', 0.7393220067024231)]\n",
      "\n",
      "\n",
      "[('yarn', 1.0)]\n",
      "[('lanka', 0.9999999403953552)]\n",
      "[('nahka', 0.7131831645965576), ('peruukki', 0.6628353595733643), ('kangas', 0.6556538343429565), ('silkki', 0.6551018357276917), ('puuvilla', 0.6527767777442932)]\n",
      "\n",
      "\n",
      "[('precise', 0.9999998807907104)]\n",
      "[('täsmällinen', 1.0)]\n",
      "[('täsmällinen', 0.7956066131591797), ('tarkka', 0.7643608450889587), ('epätarkka', 0.7115785479545593), ('selkeä', 0.6951771974563599), ('looginen', 0.6933305263519287)]\n",
      "\n",
      "\n",
      "[('bartender', 1.0)]\n",
      "[('baarimikko', 1.0)]\n",
      "[('baarimikko', 0.7323603630065918), ('mies', 0.7269106507301331), ('leidi', 0.7083531618118286), ('täti', 0.7014537453651428), ('heppu', 0.7000875473022461)]\n",
      "\n",
      "\n",
      "[('optimize', 1.0)]\n",
      "[('optimoida', 1.0)]\n",
      "[('optimoida', 0.7676687836647034), ('tehostaa', 0.691417932510376), ('maksimoida', 0.6911002993583679), ('automatisoida', 0.6844189167022705), ('integroida', 0.6766018867492676)]\n",
      "\n",
      "\n",
      "[('weekly', 1.0)]\n",
      "[('viikoittain', 1.0)]\n",
      "[('e-kirja', 0.5864243507385254), ('verkkolehti', 0.583365797996521), ('haastattelu', 0.573586106300354), ('verkkosivusto', 0.5731956958770752), ('ilmestyvä', 0.5637764930725098)]\n",
      "\n",
      "\n",
      "[('albeit', 0.9999999403953552)]\n",
      "[('vaikkakin', 0.9999999403953552)]\n",
      "[('vaikkakin', 0.696643590927124), ('joskin', 0.6772763133049011), ('silti', 0.6222859025001526), ('näennäisesti', 0.6212145686149597), ('melko', 0.620396077632904)]\n",
      "\n",
      "\n",
      "[('listing', 1.0)]\n",
      "[('listaus', 1.0)]\n",
      "[('numerointi', 0.514110803604126), ('kirjaaminen', 0.5103274583816528), ('merkintä', 0.5051668882369995), ('kirjaus', 0.4970371723175049), ('merkitseminen', 0.49560296535491943)]\n",
      "\n",
      "\n",
      "[('perfect', 1.0)]\n",
      "[('täydellinen', 0.9999998807907104)]\n",
      "[('täydellinen', 0.8058170080184937), ('hieno', 0.7539225220680237), ('loistava', 0.7226714491844177), ('upea', 0.7049005031585693), ('kaunis', 0.7005001306533813)]\n",
      "\n",
      "\n",
      "[('demonstrations', 1.0)]\n",
      "[('mielenosoitukset', 1.0)]\n",
      "[('mielenosoitukset', 0.7791154384613037), ('mellakat', 0.7129089832305908), ('protestit', 0.7115620374679565), ('mielenosoituksia', 0.6842939853668213), ('mielenosoitus', 0.6801104545593262)]\n",
      "\n",
      "\n",
      "[('savings', 0.9999999403953552)]\n",
      "[('säästöt', 1.0)]\n",
      "[('velat', 0.7651330232620239), ('tulot', 0.7553130984306335), ('velka', 0.7497296333312988), ('tuloja', 0.7388670444488525), ('lainat', 0.7375109195709229)]\n",
      "\n",
      "\n",
      "[('stills', 1.0)]\n",
      "[('still', 1.0)]\n",
      "[('filmit', 0.6934239864349365), ('animaatiot', 0.6705766320228577), ('videot', 0.6600626707077026), ('kuvat', 0.64724200963974), ('tehosteet', 0.6388233304023743)]\n",
      "\n",
      "\n",
      "[('painted', 1.0)]\n",
      "[('maalannut', 1.0)]\n",
      "[('maalattu', 0.802920937538147), ('maalatut', 0.7264993786811829), ('koristeellinen', 0.7117598056793213), ('koristeltu', 0.6991913318634033), ('maalattuna', 0.6937856674194336)]\n",
      "\n",
      "\n",
      "[('respiratory', 1.0000001192092896)]\n",
      "[('hengitys', 0.9999999403953552)]\n",
      "[('infektiot', 0.8191278576850891), ('tulehdukset', 0.8144928216934204), ('oireita', 0.8042201995849609), ('astma', 0.8033654689788818), ('krooninen', 0.8009648323059082)]\n",
      "\n",
      "\n",
      "[('expertly', 1.0)]\n",
      "[('tottuneesti', 1.0)]\n",
      "[('taidokkaasti', 0.7568061351776123), ('taitavasti', 0.7013403177261353), ('taidokas', 0.6900995969772339), ('taiten', 0.6685333251953125), ('rosoinen', 0.6436691880226135)]\n",
      "\n",
      "\n",
      "[('eighteen', 0.9999999403953552)]\n",
      "[('kahdeksantoista', 0.9999999403953552)]\n",
      "[('kuusitoista', 0.8653184175491333), ('viisitoista', 0.8502535820007324), ('viisi', 0.8497807383537292), ('kuusi', 0.8420941829681396), ('neljätoista', 0.8212096095085144)]\n",
      "\n",
      "\n",
      "[('written', 1.0)]\n",
      "[('kirjallinen', 1.0)]\n",
      "[('kirjoitettu', 0.6864264607429504), ('suomennettu', 0.6756492257118225), ('kirjoittamansa', 0.6681782603263855), ('kirjoitettiin', 0.6577907800674438), ('siteerattu', 0.6565667390823364)]\n",
      "\n",
      "\n",
      "[('mild', 0.9999999403953552)]\n",
      "[('lievä', 1.0)]\n",
      "[('lievä', 0.7530404329299927), ('voimakas', 0.729441225528717), ('kuiva', 0.7273125648498535), ('ärhäkkä', 0.6781606674194336), ('kylmä', 0.6720000505447388)]\n",
      "\n",
      "\n",
      "[('expectations', 1.0)]\n",
      "[('odotukset', 1.0)]\n",
      "[('odotukset', 0.6715565919876099), ('marginaalit', 0.646376371383667), ('myyntiluvut', 0.6391302943229675), ('paineet', 0.6248764991760254), ('prosentit', 0.6133362054824829)]\n",
      "\n",
      "\n",
      "[('lens', 1.0)]\n",
      "[('linssi', 1.0)]\n",
      "[('linssi', 0.7611081004142761), ('anturi', 0.6947470903396606), ('kalvo', 0.6822843551635742), ('sensori', 0.6700199842453003), ('peili', 0.6688025593757629)]\n",
      "\n",
      "\n",
      "[('brass', 1.0)]\n",
      "[('messinki', 0.9999999403953552)]\n",
      "[('rumpu', 0.7060574889183044), ('metallinen', 0.6991983652114868), ('tangot', 0.6919355988502502), ('metalliset', 0.6881908178329468), ('messinki', 0.6774690747261047)]\n",
      "\n",
      "\n",
      "[('stitches', 0.9999999403953552)]\n",
      "[('silmukkaa', 0.9999999403953552)]\n",
      "[('ranteen', 0.7361558675765991), ('kaulan', 0.707119345664978), ('leuan', 0.6973674297332764), ('ranne', 0.6876875162124634), ('reiden', 0.6840435266494751)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_en,val_fi,_=hist.validation_data #This we saw before - the validation data\n",
    "predicted_fi=model.predict(val_en) #Transform the English vectors in the validation data\n",
    "for en,fi,pred_fi in list(zip(val_en,val_fi,predicted_fi))[:30]:\n",
    "    print(model_english.similar_by_vector(en,topn=1)) #This is the original English word\n",
    "    print(model_finnish.similar_by_vector(fi,topn=1)) #This is the target Finnish word\n",
    "    print(model_finnish.similar_by_vector(pred_fi,topn=5)) # Top five closest hits to the transformed vector\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating more formally\n",
    "\n",
    "* Eyeballing the data is a moving target\n",
    "* Ideally, we'd have a more solid metric\n",
    "* Let us try top-1, top-5, and top-10 for the proportion of words which got the correct translation among top-N candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 33.477321814254864 percent correct\n",
      "Top5 50.75593952483801 percent correct\n",
      "Top10 57.45140388768899 percent correct\n"
     ]
    }
   ],
   "source": [
    "def eval(src_model,tgt_model,src_vecs,tgt_vecs,predicted_vecs):\n",
    "    top1,top5,top10,total=0,0,0,0\n",
    "    for src_v,tgt_v,pred_v in zip(src_vecs,tgt_vecs,predicted_vecs):\n",
    "        src_word=src_model.similar_by_vector(src_v)[0][0]\n",
    "        tgt_word=tgt_model.similar_by_vector(tgt_v)[0][0]\n",
    "        hits=list(w for w,sim in tgt_model.similar_by_vector(pred_v,topn=10))\n",
    "        total+=1\n",
    "        if tgt_word==hits[0]:\n",
    "            top1+=1\n",
    "        if tgt_word in hits[:5]:\n",
    "            top5+=1\n",
    "        if tgt_word in hits[:10]:\n",
    "            top10+=1\n",
    "    print(\"Top1\",top1/total*100,\"percent correct\")\n",
    "    print(\"Top5\",top5/total*100,\"percent correct\")\n",
    "    print(\"Top10\",top10/total*100,\"percent correct\")\n",
    "eval(model_english,model_finnish,val_en,val_fi,predicted_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* We have seen the vectors have interesting properties\n",
    "* In particular, spaces can be mapped onto each other\n",
    "* We have seen how this can be achieved with a simple linear transformation\n",
    "* Optimal transformation has a closed-form solution, but we were lazy and trained it with Keras quite successfully\n",
    "* This demonstrates how Keras can be used also for more generic tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word> dog\n",
      "    WARNING: this word was seen during training\n",
      "   kissa    0.8225976824760437\n",
      "   kani    0.819080114364624\n",
      "   aasi    0.7754292488098145\n",
      "   kisu    0.7641241550445557\n",
      "   elukka    0.7640793323516846\n",
      "   koira    0.763005256652832\n",
      "   katti    0.7530404329299927\n",
      "   marsu    0.7496534585952759\n",
      "   apina    0.7409812808036804\n",
      "   hamsteri    0.7383363842964172\n",
      "\n",
      "word> end\n"
     ]
    }
   ],
   "source": [
    "# Extra stuff - a function to query the translations, so we can play around\n",
    "def top_n(word,source_model,target_model,transformation_model,topn=5):\n",
    "    try:\n",
    "        source_idx=source_model.vocab[word].index\n",
    "    except:\n",
    "        print(\"Cannot retrieve vector for\",word)\n",
    "        return None\n",
    "    mapped=transformation_model.predict(source_model.vectors[source_idx,:].reshape(1,-1))\n",
    "    return target_model.similar_by_vector(mapped[0])\n",
    "    \n",
    "seen_words=set(en for fi,en in common) #These words were seen during training or validation\n",
    "while True:\n",
    "    wrd=input(\"word> \")\n",
    "    if wrd==\"end\":\n",
    "        break\n",
    "    if wrd in seen_words:\n",
    "        print(\"    WARNING: this word was seen during training\")\n",
    "    hits=top_n(wrd,model_english,model_finnish,model)\n",
    "    for word,sim in hits:\n",
    "        print(\"  \",word,\"  \",sim)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "venv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
