{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting properties of embeddings\n",
    "\n",
    "* We will look at some of the properties w2v embeddings have\n",
    "* Test them ourselves in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case you forgot, this is how one can load the embeddings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load embedding model from a file\n",
    "# binary: True if saved as binary file (.bin), False if saved as text file (.vectors or .txt for example)\n",
    "# limit: How many words to read from the model\n",
    "model_english=KeyedVectors.load_word2vec_format(\"data/gigaword-and-wikipedia.bin\", binary=True, limit=100000)\n",
    "model_finnish=KeyedVectors.load_word2vec_format(\"data/pb34_wf_200_v2_skgram.bin\", binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words for 'locomotive':\n",
      "[('locomotives', 0.9040292501449585), ('railcar', 0.8419736623764038), ('railcars', 0.806695282459259), ('locos', 0.7832419872283936), ('steam', 0.7655093669891357), ('bogie', 0.764204740524292), ('shunting', 0.7400067448616028), ('steam-powered', 0.7359079122543335), ('carriages', 0.7352656126022339), ('diesel-electric', 0.7294440269470215)]\n",
      "\n",
      "Most similar words for 'veturi':\n",
      "[('vaunu', 0.7221510410308838), ('veturin', 0.6715301275253296), ('raide', 0.6576318144798279), ('juna', 0.6463427543640137), ('kuorma-auto', 0.6372989416122437), ('perävaunu', 0.6346830725669861), ('veturina', 0.623757004737854), ('traktori', 0.6218146085739136), ('linja-auto', 0.6097179651260376), ('kiskoilla', 0.6076352596282959)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Most similar words for 'locomotive':\")\n",
    "print(model_english.most_similar(\"locomotive\",topn=10))\n",
    "print()\n",
    "print(\"Most similar words for 'veturi':\")\n",
    "print(model_finnish.most_similar(\"veturi\",topn=10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping spaces\n",
    "\n",
    "* One of the more famous properties of the embeddings\n",
    "* Learn a **linear** mapping from one language to another\n",
    "* Can we replicate this?\n",
    "* Learn a network with a single dense output layer\n",
    "* English vector in -- Finnish vector out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "* Need English-Finnish pairs of words to train on\n",
    "* ...google translate, maybe?\n",
    "* Googling around finds this https://github.com/ssut/py-googletrans\n",
    "* ...unofficial API, will get your IP banned if overused, so let's be careful!\n",
    "* official API needs registration, etc.\n",
    "\n",
    "`pip3 install --user googletrans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin= locomotive text= veturi\n",
      "origin= milk text= maito\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator=Translator()\n",
    "translations=translator.translate([\"locomotive\",\"milk\"],src=\"en\",dest=\"fi\")\n",
    "for t in translations:\n",
    "    print(\"origin=\",t.origin,\"text=\",t.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seems to work fine!\n",
    "* Let's grab some translations\n",
    "* The docs say \"max 16000 characters per request\"\n",
    "* We need to translate some hundreds of words at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab 100000\n",
      "<class 'dict'>\n",
      "Vocab(count:99365, index:635)\n",
      "[('</s>', <gensim.models.keyedvectors.Vocab object at 0x7f0a85552cf8>), (',', <gensim.models.keyedvectors.Vocab object at 0x7f0a85552320>), ('the', <gensim.models.keyedvectors.Vocab object at 0x7f0a85552470>), ('.', <gensim.models.keyedvectors.Vocab object at 0x7f0a85552da0>), ('of', <gensim.models.keyedvectors.Vocab object at 0x7f0a855525c0>)]\n",
      "Freq sorted ['</s>', ',', 'the', '.', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(\"English vocab\",len(model_english.vocab))\n",
    "print(model_english.vocab.__class__)\n",
    "print(model_english.vocab[\"car\"])\n",
    "#We need a list, in order of frequency\n",
    "words=sorted(model_english.vocab.items(),key=lambda word_dim:word_dim[1].count,reverse=True)\n",
    "print(words[:5])\n",
    "words_freq_sorted=[w for w,_ in words]\n",
    "print(\"Freq sorted\",words_freq_sorted[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have things like `</s>` and `.` in the vocabulary, those we don't want to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'to', 'and', 'in', 'a', 'for', 'The', 'is', 'that', 'was', 'on', 'with', 'said', 'as', 'by', 'at', 'from', 'he', 'his']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "english_word_re=re.compile(\"^[a-zA-Z]+$\") #about as stupid simplification as you can get!\n",
    "final_word_list=[]\n",
    "for w in words_freq_sorted:\n",
    "    if english_word_re.match(w):\n",
    "        final_word_list.append(w)\n",
    "print(final_word_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Finnish ['ja', 'on', 'ei', 'että', 'se', 'oli', 'mutta', 'tai', 'kun', 'myös', 'ovat', 'ole', 'niin', 'jos', 'kuin'] ... ['seurakunnan', 'selvä', 'tulleet', 'seuraavaan', 'sijasta', 'kuollut', 'I', 'asioihin', 'loput', 'luona', 'talven', 'per', 'ihanan', 'palvelua', 'tietokoneen']\n",
      "Final English ['the', 'of', 'to', 'and', 'in', 'a', 'for', 'The', 'is', 'that', 'was', 'on', 'with', 'said', 'as'] ... ['Greece', 'rally', 'democracy', 'revenue', 'add', 'criticism', 'offices', 'Hussein', 'kids', 'relief', 'promised', 'advance', 'talking', 'boost', 'dispute']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#same thing as above, nicely packed into a function\n",
    "def clean_vocab(gensim_model,regexp):\n",
    "    words=sorted(gensim_model.vocab.items(),key=lambda word_dim:word_dim[1].count,reverse=True)\n",
    "    words_freq_sorted=[w for w,_ in words]\n",
    "    word_re=re.compile(regexp)\n",
    "    final_word_list=[]\n",
    "    for w in words_freq_sorted:\n",
    "        if word_re.match(w):\n",
    "            final_word_list.append(w)\n",
    "    return final_word_list\n",
    "\n",
    "finnish_vocab=clean_vocab(model_finnish,'^[a-zA-ZäöåÖÄÅ]+$')\n",
    "english_vocab=clean_vocab(model_english,'^[a-zA-Z]+$')\n",
    "print(\"Final Finnish\",finnish_vocab[:15],\"...\",finnish_vocab[2000:2015])\n",
    "print(\"Final English\",english_vocab[:15],\"...\",english_vocab[2000:2015])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en -> fi batch at 0 ....OK\n",
      "en -> fi batch at 20 ....OK\n",
      "en -> fi batch at 40 ....OK\n"
     ]
    }
   ],
   "source": [
    "#Little test\n",
    "import time\n",
    "def translate(words,src,dest,batch_size=1000):\n",
    "    result=[] #[(\"dog\",\"koira\"),....]\n",
    "    translator=Translator()\n",
    "    for idx in range(0,len(words),batch_size):\n",
    "        batch=words[idx:idx+batch_size]\n",
    "        try:\n",
    "            translations=translator.translate(batch,src=src,dest=dest)\n",
    "            for t in translations:\n",
    "                result.append((t.origin,t.text))\n",
    "            time.sleep(0.2) #sleep between batches\n",
    "            print(src,\"->\",dest,\"batch at\",idx,\"....OK\")\n",
    "        except: #we end here, if the lines between try ... except throw an error\n",
    "            print(src,\"->\",dest,\"batch at\",idx,\"....FAILED\")\n",
    "            time.sleep(61) #sleep a little longer so Google is not angry\n",
    "            print(src,\"->\",dest,\"...RESTARTING\")\n",
    "            \n",
    "    return result\n",
    "\n",
    "x=translate(english_vocab[:50],\"en\",\"fi\",20) # a small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', ''), ('of', 'of'), ('to', 'että'), ('and', 'ja'), ('in', 'sisään'), ('a', ''), ('for', 'varten'), ('The', ''), ('is', 'on'), ('that', 'että'), ('was', 'oli'), ('on', 'päällä'), ('with', 'kanssa'), ('said', 'sanoi'), ('as', 'kuten'), ('by', 'mennessä'), ('at', 'at'), ('from', 'alkaen'), ('he', 'hän'), ('his', 'hänen'), ('it', 'se'), ('be', 'olla'), ('are', 'olemme'), ('an', ''), ('has', 'on'), ('have', 'omistaa'), ('were', 'olivat'), ('not', 'ei'), ('who', 'Kuka'), ('had', 'oli'), ('which', 'joka'), ('will', 'tahtoa'), ('or', 'tai'), ('their', 'heidän'), ('but', 'mutta'), ('its', 'sen'), ('In', 'Sisään'), ('this', 'Tämä'), ('they', 'ne'), ('been', 'ollut'), ('I', 'minä'), ('also', 'myös'), ('would', 'olisi'), ('one', 'yksi'), ('He', 'Hän'), ('after', 'jälkeen'), ('more', 'lisää'), ('two', 'kaksi'), ('first', 'ensimmäinen'), ('about', 'noin')]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* looks okay\n",
    "* let's run this and save the result for later use, so we don't get banned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en -> fi batch at 0 ....OK\n",
      "en -> fi batch at 150 ....OK\n",
      "en -> fi batch at 300 ....OK\n",
      "en -> fi batch at 450 ....OK\n",
      "en -> fi batch at 600 ....OK\n",
      "en -> fi batch at 750 ....OK\n",
      "en -> fi batch at 900 ....OK\n",
      "en -> fi batch at 1050 ....FAILED\n",
      "en -> fi ...RESTARTING\n",
      "en -> fi batch at 1200 ....FAILED\n",
      "en -> fi ...RESTARTING\n",
      "en -> fi batch at 1350 ....FAILED\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-44b7f1a72387>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(words, src, dest, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mtranslations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    318\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6860c2a42a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0men_fi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_fi_transl.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_fi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfi_en\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinnish_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-44b7f1a72387>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(words, src, dest, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"batch at\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"....FAILED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m61\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sleep a little longer so Google is not angry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"...RESTARTING\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "en_fi=translate(english_vocab,\"en\",\"fi\",batch_size=150)\n",
    "with open(\"en_fi_transl.json\",\"wt\") as f:\n",
    "    json.dump(en_fi,f)\n",
    "fi_en=translate(finnish_vocab,\"fi\",\"en\",batch_size=150)\n",
    "with open(\"fi_en_transl.json\",\"wt\") as f:\n",
    "    json.dump(fi_en,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* well, we got banned :D\n",
    "* Let's just translate as text files, in the google translate interface\n",
    "* (quality time manually feeding these into Google translate --- I could have used the official API :)\n",
    "* ...but now it's done, so who cares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump 10K words at a time into a file, which can be fed to google translate\n",
    "def build_files(words,fname,batch_size):\n",
    "    for idx in range(0,len(words),batch_size):\n",
    "        batch=words[idx:idx+batch_size]\n",
    "        with open(\"trdata/{}_batch_{}.txt\".format(fname,idx),\"wt\") as f:\n",
    "            print(\"\\n\".join(batch),file=f)\n",
    "\n",
    "build_files(english_vocab,\"en-fi-source\",10000)\n",
    "build_files(finnish_vocab,\"fi-en-source\",10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I built manually four files like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trdata/enfi_source_all.txt\n",
      "trdata/enfi_target_all.txt\n",
      "trdata/fien_source_all.txt\n",
      "trdata/fien_target_all.txt\n",
      "  95275 trdata/fien_source_all.txt\n",
      "  95275 trdata/fien_target_all.txt\n",
      "  83618 trdata/enfi_source_all.txt\n",
      "  83618 trdata/enfi_target_all.txt\n",
      " 357786 total\n",
      "FI -> EN\n",
      "ja\tand\n",
      "on\tis\n",
      "ei\tNo\n",
      "että\tthat\n",
      "se\tit\n",
      "oli\twas\n",
      "mutta\tbut\n",
      "tai\tor\n",
      "kun\twhen\n",
      "myös\talso\n",
      "EN -> FI\n",
      "the\t\n",
      "of\tof\n",
      "to\tettä\n",
      "and\tja\n",
      "in\tsisään\n",
      "a\t\n",
      "for\tvarten\n",
      "The\t\n",
      "is\ton\n",
      "that\tettä\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls trdata/fien_* trdata/enfi_*\n",
    "wc -l trdata/fien_* trdata/enfi_*\n",
    "echo \"FI -> EN\"\n",
    "paste trdata/fien_source_all.txt trdata/fien_target_all.txt  | head -n 10\n",
    "echo \"EN -> FI\"\n",
    "paste trdata/enfi_source_all.txt trdata/enfi_target_all.txt  | head -n 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read in and filter\n",
    "* To make sure we get high-quality stuff, we will look for same pairs in fin-eng and eng-fin direction\n",
    "* That way we will also make sure our translations are among the top 100K words in both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len fien 95275\n",
      "Len enfi 83610\n",
      "Len common 7100\n",
      "[('vuokrata', 'rent'), ('Jenkins', 'Jenkins'), ('Roses', 'Roses'), ('Beth', 'Beth'), ('TR', 'TR'), ('taantuma', 'downturn'), ('maskotti', 'mascot'), ('Antonin', 'Antonin'), ('kroonisesti', 'chronically'), ('kompromisseja', 'compromises'), ('tunnollisesti', 'scrupulously'), ('tuntea', 'feel'), ('Linus', 'Linus'), ('innoissaan', 'excited'), ('Napoli', 'Naples'), ('haitata', 'hinder'), ('koskematon', 'pristine'), ('Fiona', 'Fiona'), ('muistio', 'memorandum'), ('muslimit', 'Muslims'), ('safari', 'safari'), ('kiima', 'rut'), ('lopulta', 'eventually'), ('nämä', 'these'), ('mekanismit', 'mechanisms'), ('matkamuistoja', 'souvenirs'), ('Nigerian', 'Nigerian'), ('sari', 'sari'), ('petojen', 'beasts'), ('kutistuu', 'shrinking'), ('Camilla', 'Camilla'), ('Lajunen', 'Lajunen'), ('Joe', 'Joe'), ('tuote', 'product'), ('juhlapäivät', 'holidays'), ('XP', 'XP'), ('hyppii', 'hopping'), ('opaskirja', 'guidebook'), ('Dalai', 'Dalai'), ('Melissa', 'Melissa'), ('keskittyminen', 'concentration'), ('ystävät', 'friends'), ('runoilijat', 'poets'), ('vaatimaton', 'modest'), ('kasinot', 'casinos'), ('syy', 'reason'), ('tottuneesti', 'expertly'), ('Jaffa', 'Jaffa'), ('enimmäkseen', 'mostly'), ('Suzuki', 'Suzuki'), ('maailmanloppu', 'apocalypse'), ('Bon', 'Bon'), ('mennyt', 'gone'), ('tykkää', 'likes'), ('vapaaehtoisesti', 'voluntarily'), ('Thatcher', 'Thatcher'), ('voittaja', 'winner'), ('herkkyys', 'sensitivity'), ('jättäen', 'leaving'), ('järjestetty', 'organized'), ('syvä', 'deep'), ('Merkel', 'Merkel'), ('Einar', 'Einar'), ('mac', 'mac'), ('saaret', 'islands'), ('terrori', 'terror'), ('neuvotella', 'negotiate'), ('kierrosta', 'rounds'), ('hengitys', 'respiratory'), ('oivallus', 'realization'), ('siisti', 'neat'), ('analysoidaan', 'analyzed'), ('kohdanneet', 'encountered'), ('sushi', 'sushi'), ('Toshiba', 'Toshiba'), ('duo', 'duo'), ('Ronin', 'Ronin'), ('Frans', 'Frans'), ('bonus', 'bonus'), ('Feng', 'Feng'), ('Bach', 'Bach'), ('tuloaan', 'joining'), ('jo', 'already'), ('Ignatius', 'Ignatius'), ('erikoistuneet', 'specialize'), ('AEG', 'AEG'), ('että', 'that'), ('sivistynyt', 'civilized'), ('ku', 'ku'), ('WD', 'WD'), ('myös', 'also'), ('luterilaisuus', 'Lutheranism'), ('juoma', 'beverage'), ('kirjallinen', 'written'), ('antelias', 'generous'), ('rotta', 'rat'), ('Massachusetts', 'Massachusetts'), ('kaappaa', 'captures'), ('postikulut', 'postage'), ('kalmari', 'squid'), ('tutkia', 'explore'), ('intohimoisesti', 'passionately'), ('Luca', 'Luca'), ('Marek', 'Marek'), ('perinteinen', 'traditional'), ('lukija', 'reader'), ('myrsky', 'storm'), ('raivo', 'rage'), ('epäonnistuu', 'fails'), ('kurdien', 'Kurdish'), ('kitistä', 'whine'), ('interaktiivinen', 'interactive'), ('erottaa', 'distinguish'), ('ilmoitti', 'announced'), ('vallitseva', 'prevalent'), ('Emmy', 'Emmy'), ('läpikuultava', 'translucent'), ('merkittävästi', 'significantly'), ('käytettävyys', 'usability'), ('muotoinen', 'shaped'), ('malaria', 'malaria'), ('sairaalat', 'hospitals'), ('osasto', 'department'), ('olennaiseen', 'essentials'), ('Forza', 'Forza'), ('pyöräily', 'cycling'), ('toimitukset', 'deliveries'), ('murre', 'dialect'), ('Armin', 'Armin'), ('Kia', 'Kia'), ('kiiltävä', 'shiny'), ('teki', 'did'), ('juoksu', 'running'), ('hylätä', 'reject'), ('maksettava', 'due'), ('Liz', 'Liz'), ('pyyhe', 'towel'), ('verkko', 'network'), ('keskikenttäpelaaja', 'midfielder'), ('pehmeästi', 'softly'), ('kuninkaallinen', 'royal'), ('ja', 'and'), ('rapeita', 'crunchy'), ('ikuisuus', 'eternity'), ('Thorp', 'Thorp'), ('kehyksiä', 'frames'), ('työnantaja', 'employer'), ('kuisti', 'porch'), ('Palms', 'Palms'), ('Netflix', 'Netflix'), ('NBC', 'NBC'), ('marsalkka', 'marshal'), ('kohdun', 'uterine'), ('itsenäisesti', 'independently'), ('mehut', 'juices'), ('erehtyä', 'err'), ('traagisesti', 'tragically'), ('monni', 'catfish'), ('par', 'par'), ('TNS', 'TNS'), ('julkkikset', 'celebrities'), ('tekijät', 'factors'), ('juoksi', 'ran'), ('Steve', 'Steve'), ('lisääntyy', 'increases'), ('IBM', 'IBM'), ('Rami', 'Rami'), ('RK', 'RK'), ('Viktoria', 'Viktoria'), ('Sherman', 'Sherman'), ('puutarhuri', 'gardener'), ('legendaarinen', 'legendary'), ('katkeruus', 'bitterness'), ('pelata', 'play'), ('oravia', 'squirrels'), ('tuhonnut', 'destroyed'), ('Atria', 'Atria'), ('päättelee', 'concludes'), ('riittävyys', 'adequacy'), ('harjoitellaan', 'practicing'), ('DE', 'DE'), ('nostalginen', 'nostalgic'), ('Sisko', 'Sister'), ('lämpö', 'heat'), ('valaiseva', 'illustrative'), ('kylpytakki', 'bathrobe'), ('monimuotoisuus', 'diversity'), ('paitsi', 'except'), ('remmi', 'thong'), ('monikko', 'plural'), ('nuo', 'those'), ('Spielberg', 'Spielberg'), ('pyöristää', 'round'), ('des', 'des'), ('Nino', 'Nino'), ('vaarallinen', 'dangerous'), ('välttämättömyys', 'necessity'), ('Hugo', 'Hugo'), ('hajoaminen', 'decomposition'), ('Patrik', 'Patrik'), ('postilaatikko', 'mailbox'), ('laukku', 'bag'), ('terveiset', 'regards'), ('häpeä', 'shame'), ('tuntematon', 'unknown'), ('olemassaolo', 'existence'), ('hankkeet', 'projects'), ('ahkera', 'diligent'), ('sovittaminen', 'adaptation'), ('pimeys', 'darkness'), ('alas', 'down'), ('ammunta', 'shooting'), ('opastus', 'guidance'), ('PT', 'PT'), ('ti', 'ti'), ('Pavel', 'Pavel'), ('Monroe', 'Monroe'), ('jatkuva', 'continuous'), ('Kira', 'Kira'), ('kHz', 'kHz'), ('Jokinen', 'Jokinen'), ('college', 'college'), ('Lehtinen', 'Lehtinen'), ('rakenne', 'structure'), ('suoristaa', 'straighten'), ('puuha', 'chore'), ('kanssa', 'with'), ('liikunta', 'exercise'), ('jyrkkä', 'steep'), ('toteutukset', 'implementations'), ('kiusata', 'bully'), ('tuottajia', 'producers'), ('luokiteltu', 'classified'), ('mainonta', 'advertising'), ('kytkökset', 'affiliations'), ('tönäisi', 'poking'), ('Benin', 'Benin'), ('pelastaja', 'savior'), ('toistaa', 'repeat'), ('apteekki', 'pharmacy'), ('opetukset', 'teachings'), ('komposti', 'compost'), ('lajikkeet', 'varieties'), ('Jeremy', 'Jeremy'), ('Christina', 'Christina'), ('kuoleva', 'moribund'), ('koulunkäynti', 'schooling'), ('adrenaliini', 'adrenaline'), ('kestävä', 'resistant'), ('kissa', 'cat'), ('kohota', 'rise'), ('ässä', 'ace'), ('Frida', 'Frida'), ('kiitollisuus', 'gratitude'), ('Milla', 'Milla'), ('Jennifer', 'Jennifer'), ('valvoi', 'oversaw'), ('PDF', 'PDF'), ('liittimet', 'connectors'), ('välillä', 'between'), ('terva', 'tar'), ('töykeä', 'rude'), ('Vince', 'Vince'), ('Wesley', 'Wesley'), ('tarkkuus', 'accuracy'), ('valokuvaajat', 'photographers'), ('nimellisesti', 'nominally'), ('verkkotunnuksen', 'domain'), ('käänteinen', 'reverse'), ('lounaat', 'lunches'), ('Anaheim', 'Anaheim'), ('Ivan', 'Ivan'), ('Gavin', 'Gavin'), ('WSOP', 'WSOP'), ('Eddie', 'Eddie'), ('vaaleanpunainen', 'pink'), ('kaksinkertaistunut', 'doubled'), ('ajatus', 'thought'), ('äidin', 'maternal'), ('karma', 'karma'), ('vankila', 'prison'), ('uhmaa', 'defying'), ('kuuluva', 'audible'), ('Vincent', 'Vincent'), ('kuninkaat', 'kings'), ('vastauksia', 'answers'), ('lievä', 'mild'), ('useat', 'several'), ('pirullinen', 'devilish'), ('Drew', 'Drew'), ('KTM', 'KTM'), ('määritellä', 'define'), ('TK', 'TK'), ('erityisesti', 'specially'), ('Palma', 'Palma'), ('Bieber', 'Bieber'), ('HDMI', 'HDMI'), ('Parker', 'Parker'), ('Acer', 'Acer'), ('Sierra', 'Sierra')]\n"
     ]
    }
   ],
   "source": [
    "fien=[] #list of (fin,eng) pairs obtained from the fin -> eng direction\n",
    "enfi=[] #list of (fin,eng) pairs, this time obtained from  the eng->fin direction\n",
    "with open(\"trdata/fien_source_all.txt\") as fi_file, open(\"trdata/fien_target_all.txt\") as en_file:\n",
    "    for fi,en in zip(fi_file,en_file):\n",
    "        fi=fi.strip()\n",
    "        en=en.strip()\n",
    "        if fi and en:\n",
    "            fien.append((fi,en))\n",
    "\n",
    "with open(\"trdata/enfi_target_all.txt\") as fi_file, open(\"trdata/enfi_source_all.txt\") as en_file:\n",
    "    for fi,en in zip(fi_file,en_file):\n",
    "        fi=fi.strip()\n",
    "        en=en.strip()\n",
    "        if fi and en:\n",
    "            enfi.append((fi,en))\n",
    "\n",
    "fien_set=set(fien)\n",
    "enfi_set=set(enfi)\n",
    "common=fien_set&enfi_set #keep only pairs which are shared\n",
    "print(\"Len fien\",len(fien_set))\n",
    "print(\"Len enfi\",len(enfi_set))\n",
    "print(\"Len common\",len(common))\n",
    "print(list(common)[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ouch - we lost most of the stuff, but such is life\n",
    "* what we got looks good, though :)\n",
    "* Let us yet filter away pairs like Ivan - Ivan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7100\n",
      "7100\n",
      "7100\n",
      "7100\n",
      "...all these four numbers should be the same\n"
     ]
    }
   ],
   "source": [
    "#Making sure all we found is in the top 100K - just crosschecking really\n",
    "print(len(set(finnish_vocab)&set(fi for fi,en in common)))\n",
    "print(len(set(english_vocab)&set(en for fi,en in common)))\n",
    "\n",
    "#Making sure all words are there exactly once - no risk of mixing train and validation\n",
    "print(len(set(fi for fi,en in common)))\n",
    "print(len(set(en for fi,en in common)))\n",
    "print(\"...all these four numbers should be the same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left with 4624 after removing identical pairs\n",
      "Shuffled pairs [('määrätty', 'prescribed'), ('nirso', 'picky'), ('saavuttaa', 'reach'), ('tyttö', 'girl'), ('poikki', 'across'), ('esitellä', 'introduce'), ('kuivausrumpu', 'dryer'), ('hellyttävä', 'disarming'), ('erakko', 'hermit'), ('vihreä', 'green'), ('kouluttaja', 'trainer'), ('liittimet', 'connectors'), ('arvioidessaan', 'assessing'), ('kiihtyvyys', 'acceleration'), ('häivyttää', 'fade'), ('säilyttäminen', 'conservation'), ('gootti', 'Goth'), ('päällystetty', 'coated'), ('Turkki', 'Turkey'), ('liittymistä', 'accession')]\n",
      "Indices: [13578, 61577, 1098, 2189, 545, 5729, 43845, 17675, 41973, 2124] [6844, 44487, 2343, 886, 3910, 4982, 88022, 90311, 53644, 2534]\n",
      "English model.vectors shape: (100000, 200)\n",
      "Finnish model.vectors shape: (100000, 200)\n",
      "English selected vectors shape: (4624, 200)\n",
      "Finnish selected vectors shape: (4624, 200)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "pairs=[(fi,en) for fi,en in common if fi!=en] #Only keep pairs where source does not equal target\n",
    "print(\"Left with\",len(pairs),\"after removing identical pairs\")\n",
    "random.shuffle(pairs) #always, always make sure to shuffle!\n",
    "\n",
    "print(\"Shuffled pairs\",pairs[:20])\n",
    "\n",
    "#Now we need to grab the vectors for the words in question\n",
    "en_indices=[model_english.vocab[en].index for fi,en in pairs] #English\n",
    "fi_indices=[model_finnish.vocab[fi].index for fi,en in pairs] #Finnish\n",
    "print(\"Indices:\",en_indices[:10],fi_indices[:10])\n",
    "#...and the vectors are hidden in the models\n",
    "print(\"English model.vectors shape:\",model_english.vectors.shape)\n",
    "print(\"Finnish model.vectors shape:\",model_finnish.vectors.shape)\n",
    "en_vectors=model_english.vectors[en_indices] #Selects the rows in just the correct order\n",
    "fi_vectors=model_finnish.vectors[fi_indices] #Selects the rows in just the correct order\n",
    "print(\"English selected vectors shape:\",en_vectors.shape)\n",
    "print(\"Finnish selected vectors shape:\",fi_vectors.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now `en_vectors` is vectors for the 4624 English words in our translation pairs\n",
    "* `fi_vectors` is same for Finnish\n",
    "* ...our training data is done - we have the pairs of input--desired output\n",
    "\n",
    "## Learning transformation from English to Finnish\n",
    "\n",
    "* 200-dim vector in, 200-dim vector out\n",
    "* Loss needs to be different, this is not classification!\n",
    "* `mse` stands for mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_46 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 200)               40200     \n",
      "=================================================================\n",
      "Total params: 40,200\n",
      "Trainable params: 40,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4161 samples, validate on 463 samples\n",
      "Epoch 1/30\n",
      "4161/4161 [==============================] - 2s 577us/step - loss: 0.0822 - val_loss: 0.0642\n",
      "Epoch 2/30\n",
      "4161/4161 [==============================] - 0s 80us/step - loss: 0.0599 - val_loss: 0.0529\n",
      "Epoch 3/30\n",
      "4161/4161 [==============================] - 0s 94us/step - loss: 0.0514 - val_loss: 0.0473\n",
      "Epoch 4/30\n",
      "4161/4161 [==============================] - 0s 103us/step - loss: 0.0467 - val_loss: 0.0441\n",
      "Epoch 5/30\n",
      "4161/4161 [==============================] - 0s 70us/step - loss: 0.0438 - val_loss: 0.0421\n",
      "Epoch 6/30\n",
      "4161/4161 [==============================] - 0s 69us/step - loss: 0.0419 - val_loss: 0.0407\n",
      "Epoch 7/30\n",
      "4161/4161 [==============================] - 0s 64us/step - loss: 0.0406 - val_loss: 0.0398\n",
      "Epoch 8/30\n",
      "4161/4161 [==============================] - 0s 47us/step - loss: 0.0396 - val_loss: 0.0392\n",
      "Epoch 9/30\n",
      "4161/4161 [==============================] - 0s 49us/step - loss: 0.0390 - val_loss: 0.0387\n",
      "Epoch 10/30\n",
      "4161/4161 [==============================] - 0s 42us/step - loss: 0.0385 - val_loss: 0.0384\n",
      "Epoch 11/30\n",
      "4161/4161 [==============================] - 0s 48us/step - loss: 0.0381 - val_loss: 0.0382\n",
      "Epoch 12/30\n",
      "4161/4161 [==============================] - 0s 45us/step - loss: 0.0378 - val_loss: 0.0380\n",
      "Epoch 13/30\n",
      "4161/4161 [==============================] - 0s 63us/step - loss: 0.0376 - val_loss: 0.0379\n",
      "Epoch 14/30\n",
      "4161/4161 [==============================] - 0s 88us/step - loss: 0.0374 - val_loss: 0.0379\n",
      "Epoch 15/30\n",
      "4161/4161 [==============================] - 1s 138us/step - loss: 0.0373 - val_loss: 0.0378\n",
      "Epoch 16/30\n",
      "4161/4161 [==============================] - 0s 93us/step - loss: 0.0372 - val_loss: 0.0377\n",
      "Epoch 17/30\n",
      "4161/4161 [==============================] - 0s 91us/step - loss: 0.0371 - val_loss: 0.0377\n",
      "Epoch 18/30\n",
      "4161/4161 [==============================] - 0s 92us/step - loss: 0.0370 - val_loss: 0.0377\n",
      "Epoch 19/30\n",
      "4161/4161 [==============================] - 0s 110us/step - loss: 0.0370 - val_loss: 0.0377\n",
      "Epoch 20/30\n",
      "4161/4161 [==============================] - 0s 95us/step - loss: 0.0369 - val_loss: 0.0377\n",
      "Epoch 21/30\n",
      "4161/4161 [==============================] - 0s 93us/step - loss: 0.0369 - val_loss: 0.0377\n",
      "Epoch 22/30\n",
      "4161/4161 [==============================] - 0s 99us/step - loss: 0.0369 - val_loss: 0.0377\n",
      "Epoch 23/30\n",
      "4161/4161 [==============================] - 0s 73us/step - loss: 0.0369 - val_loss: 0.0377\n",
      "Epoch 24/30\n",
      "4161/4161 [==============================] - 0s 54us/step - loss: 0.0368 - val_loss: 0.0378\n",
      "Epoch 25/30\n",
      "4161/4161 [==============================] - 0s 52us/step - loss: 0.0368 - val_loss: 0.0377\n",
      "Epoch 26/30\n",
      "4161/4161 [==============================] - 0s 55us/step - loss: 0.0368 - val_loss: 0.0377\n",
      "Epoch 27/30\n",
      "4161/4161 [==============================] - 0s 52us/step - loss: 0.0368 - val_loss: 0.0377\n",
      "Epoch 28/30\n",
      "4161/4161 [==============================] - 0s 68us/step - loss: 0.0368 - val_loss: 0.0377\n",
      "Epoch 29/30\n",
      "4161/4161 [==============================] - 0s 49us/step - loss: 0.0368 - val_loss: 0.0378\n",
      "Epoch 30/30\n",
      "4161/4161 [==============================] - 0s 53us/step - loss: 0.0368 - val_loss: 0.0378\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "\n",
    "\n",
    "inp=Input(shape=(en_vectors.shape[1],)) #input is 200-dim\n",
    "outp=Dense(fi_vectors.shape[1])(inp) #Simple linear transformation of the input\n",
    "\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"mse\")\n",
    "hist=model.fit(en_vectors,fi_vectors,batch_size=100,verbose=1,epochs=30,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('inexplicable', 0.9999999403953552)]\n",
      "[('selittämätön', 0.9999999403953552)]\n",
      "[('kauhistuttava', 0.7688993215560913), ('kammottava', 0.7672616839408875), ('selittämätön', 0.7632526159286499), ('hirvittävä', 0.7614887952804565), ('kummallinen', 0.7613801956176758)]\n",
      "\n",
      "\n",
      "[('remove', 1.0)]\n",
      "[('poistaa', 1.0)]\n",
      "[('poistaa', 0.7159953117370605), ('irrottaa', 0.6978674530982971), ('puhdistaa', 0.6715410947799683), ('tyhjentää', 0.6694791913032532), ('pyyhkiä', 0.6537654399871826)]\n",
      "\n",
      "\n",
      "[('fifteenth', 0.9999999403953552)]\n",
      "[('viidestoista', 1.0)]\n",
      "[('kolmastoista', 0.7914077639579773), ('kahdeksas', 0.765669047832489), ('seitsemäs', 0.7635477185249329), ('yhdestoista', 0.7624925374984741), ('kahdestoista', 0.7602266073226929)]\n",
      "\n",
      "\n",
      "[('season', 1.0)]\n",
      "[('kausi', 1.0)]\n",
      "[('kausi', 0.7775886058807373), ('pelipäivä', 0.7005314826965332), ('koitos', 0.698922872543335), ('kisa', 0.697600245475769), ('runkosarja', 0.6963039636611938)]\n",
      "\n",
      "\n",
      "[('growls', 0.9999999403953552)]\n",
      "[('murisee', 0.9999999403953552)]\n",
      "[('murina', 0.6671297550201416), ('vaimeasti', 0.6580097675323486), ('murisee', 0.6570577621459961), ('karjui', 0.6381170749664307), ('äkäinen', 0.6348575353622437)]\n",
      "\n",
      "\n",
      "[('titans', 1.0)]\n",
      "[('titaanien', 1.0)]\n",
      "[('kumppanit', 0.658277153968811), ('jättiläiset', 0.6367853283882141), ('pomot', 0.6321783065795898), ('johtajat', 0.6072019338607788), ('liikemiehet', 0.6057531833648682)]\n",
      "\n",
      "\n",
      "[('beds', 1.0)]\n",
      "[('sängyt', 0.9999999403953552)]\n",
      "[('altaat', 0.7372584939002991), ('sängyt', 0.7045561075210571), ('kylpyhuoneet', 0.6847324371337891), ('lattiat', 0.680713415145874), ('putket', 0.6805099844932556)]\n",
      "\n",
      "\n",
      "[('jealous', 1.0)]\n",
      "[('mustasukkainen', 0.9999999403953552)]\n",
      "[('mustasukkainen', 0.8258968591690063), ('ilkeä', 0.7585337162017822), ('narsisti', 0.742983341217041), ('pikkutyttö', 0.7315921187400818), ('hyväsydäminen', 0.7313790917396545)]\n",
      "\n",
      "\n",
      "[('ended', 1.0)]\n",
      "[('päättyi', 1.0)]\n",
      "[('päättyi', 0.6533002257347107), ('katkesi', 0.6225522756576538), ('puhkesi', 0.6168351769447327), ('keskeytettiin', 0.6137388944625854), ('jatkui', 0.607083261013031)]\n",
      "\n",
      "\n",
      "[('installation', 1.0)]\n",
      "[('asennus', 1.0)]\n",
      "[('valaistus', 0.6825215816497803), ('laitteisto', 0.6774635910987854), ('asennus', 0.6649690270423889), ('käynnistys', 0.6498130559921265), ('suojaus', 0.6425298452377319)]\n",
      "\n",
      "\n",
      "[('weight', 1.0)]\n",
      "[('paino', 1.0)]\n",
      "[('paino', 0.7093613147735596), ('lihasmassa', 0.6850624084472656), ('toleranssi', 0.6671880483627319), ('kuormitus', 0.6648241281509399), ('rasitus', 0.6551242470741272)]\n",
      "\n",
      "\n",
      "[('tool', 1.0)]\n",
      "[('työkalu', 1.0)]\n",
      "[('työkalu', 0.8122896552085876), ('menetelmä', 0.7361913323402405), ('työväline', 0.7301226854324341), ('sovellus', 0.6950177550315857), ('apuväline', 0.693207859992981)]\n",
      "\n",
      "\n",
      "[('roast', 0.9999999403953552)]\n",
      "[('paisti', 1.0)]\n",
      "[('salaatti', 0.8679431676864624), ('paistettu', 0.86751389503479), ('kinkku', 0.8582071661949158), ('paistetut', 0.8338422775268555), ('pekoni', 0.8277485370635986)]\n",
      "\n",
      "\n",
      "[('see', 1.0)]\n",
      "[('nähdä', 1.0)]\n",
      "[('ajatella', 0.7779346704483032), ('nähdä', 0.7140587568283081), ('ihmetellä', 0.711645781993866), ('sanoa', 0.7029680013656616), ('kertoa', 0.6819823980331421)]\n",
      "\n",
      "\n",
      "[('company', 1.0)]\n",
      "[('yhtiö', 1.0)]\n",
      "[('yritys', 0.7418477535247803), ('yhtiö', 0.7258596420288086), ('firma', 0.7184665203094482), ('pörssiyhtiö', 0.6995888948440552), ('operaattori', 0.6906917095184326)]\n",
      "\n",
      "\n",
      "[('soaps', 1.0)]\n",
      "[('saippuat', 0.9999999403953552)]\n",
      "[('saippuat', 0.787786066532135), ('shampoot', 0.7230299711227417), ('pesuaineet', 0.7229901552200317), ('kosmetiikka', 0.7017868161201477), ('voiteet', 0.6816393136978149)]\n",
      "\n",
      "\n",
      "[('touched', 1.0)]\n",
      "[('liikuttunut', 0.9999999403953552)]\n",
      "[('liukui', 0.6953783631324768), ('heilahti', 0.6892490983009338), ('valahti', 0.6825238466262817), ('lensi', 0.6823866367340088), ('liikahti', 0.6808254718780518)]\n",
      "\n",
      "\n",
      "[('harassed', 1.0)]\n",
      "[('kiusattu', 1.0)]\n",
      "[('uhkaillut', 0.6802712678909302), ('pahoinpidelty', 0.6681487560272217), ('pahoinpiteli', 0.6492868065834045), ('uhkaili', 0.6249199509620667), ('väkivaltaisesti', 0.621490478515625)]\n",
      "\n",
      "\n",
      "[('loosen', 1.0)]\n",
      "[('löysätä', 1.0)]\n",
      "[('kiristää', 0.7455934286117554), ('suoristaa', 0.6833102703094482), ('venyttää', 0.6767717003822327), ('tiukentaa', 0.674799919128418), ('löysätä', 0.6505422592163086)]\n",
      "\n",
      "\n",
      "[('cook', 1.0)]\n",
      "[('kokki', 1.0)]\n",
      "[('keittää', 0.772815465927124), ('puuro', 0.7481524348258972), ('keitto', 0.7288483381271362), ('salaatti', 0.7265571355819702), ('kypsentää', 0.7251547574996948)]\n",
      "\n",
      "\n",
      "[('management', 1.0)]\n",
      "[('johto', 1.0)]\n",
      "[('hallinnointi', 0.761670708656311), ('hallinta', 0.7514382600784302), ('taloushallinto', 0.723235011100769), ('liiketoiminta', 0.7142549157142639), ('johtaminen', 0.7133292555809021)]\n",
      "\n",
      "\n",
      "[('stance', 1.0)]\n",
      "[('asenne', 0.9999998807907104)]\n",
      "[('näkökanta', 0.7018848657608032), ('kritiikki', 0.68327796459198), ('retoriikka', 0.6684877276420593), ('asennoituminen', 0.650660514831543), ('ajattelutapa', 0.6457762718200684)]\n",
      "\n",
      "\n",
      "[('rounded', 0.9999999403953552)]\n",
      "[('pyöristetty', 0.9999999403953552)]\n",
      "[('taivutettu', 0.655480146408081), ('liimattu', 0.649358332157135), ('varsi', 0.6270628571510315), ('kaareva', 0.6183600425720215), ('ohut', 0.6174759864807129)]\n",
      "\n",
      "\n",
      "[('society', 0.9999998807907104)]\n",
      "[('yhteiskunta', 0.9999999403953552)]\n",
      "[('yhteiskunta', 0.8096385598182678), ('instituutio', 0.7708653211593628), ('kulttuuri', 0.760143518447876), ('ideologia', 0.7566108107566833), ('uskonto', 0.7523729205131531)]\n",
      "\n",
      "\n",
      "[('statutory', 1.0)]\n",
      "[('lakisääteinen', 0.9999999403953552)]\n",
      "[('velvoite', 0.7229288220405579), ('lainsäädännössä', 0.6844882965087891), ('oikeudellinen', 0.6756819486618042), ('juridinen', 0.6679924726486206), ('lakisääteinen', 0.6671240329742432)]\n",
      "\n",
      "\n",
      "[('supports', 1.0)]\n",
      "[('tukee', 1.0)]\n",
      "[('tukee', 0.6973380446434021), ('tukisi', 0.5832498073577881), ('mahdollistaa', 0.5823200345039368), ('edellyttää', 0.5684458017349243), ('painottaa', 0.5667049288749695)]\n",
      "\n",
      "\n",
      "[('bully', 0.9999999403953552)]\n",
      "[('kiusata', 1.0)]\n",
      "[('ilkeä', 0.7279965877532959), ('sekopää', 0.7139871120452881), ('paskiainen', 0.7042977213859558), ('mielipuoli', 0.6987508535385132), ('pelkuri', 0.6966197490692139)]\n",
      "\n",
      "\n",
      "[('canceled', 1.0)]\n",
      "[('peruttu', 1.0)]\n",
      "[('peruttiin', 0.6963734030723572), ('peruuntui', 0.6712815761566162), ('peruutettu', 0.6611995697021484), ('keskeytetty', 0.6416419148445129), ('peruuntuu', 0.6403080224990845)]\n",
      "\n",
      "\n",
      "[('Olympics', 1.0)]\n",
      "[('olympialaiset', 1.0)]\n",
      "[('olympialaiset', 0.6816437840461731), ('mm-kisat', 0.6355319023132324), ('kisat', 0.6206352710723877), ('MM-kisat', 0.6084803938865662), ('maraton', 0.6059747934341431)]\n",
      "\n",
      "\n",
      "[('issues', 0.9999998807907104)]\n",
      "[('kysymykset', 1.0)]\n",
      "[('näkökulmat', 0.7323461771011353), ('kysymykset', 0.7046411037445068), ('haasteet', 0.6699285507202148), ('ristiriidat', 0.6681143641471863), ('yhteiskunnalliset', 0.6663190126419067)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_en,val_fi,_=hist.validation_data #This we saw before - the validation data\n",
    "predicted_fi=model.predict(val_en) #Transform the English vectors in the validation data\n",
    "for en,fi,pred_fi in list(zip(val_en,val_fi,predicted_fi))[:30]:\n",
    "    print(model_english.similar_by_vector(en,topn=1)) #This is the original English word\n",
    "    print(model_finnish.similar_by_vector(fi,topn=1)) #This is the target Finnish word\n",
    "    print(model_finnish.similar_by_vector(pred_fi,topn=5)) # Top five closest hits to the transformed vector\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating more formally\n",
    "\n",
    "* Eyeballing the data is a moving target\n",
    "* Ideally, we'd have a more solid metric\n",
    "* Let us try top-1, top-5, and top-10 for the proportion of words which got the correct translation among top-N candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 28.725701943844495 percent correct\n",
      "Top5 55.291576673866096 percent correct\n",
      "Top10 63.498920086393085 percent correct\n"
     ]
    }
   ],
   "source": [
    "def eval(src_model,tgt_model,src_vecs,tgt_vecs,predicted_vecs):\n",
    "    top1,top5,top10,total=0,0,0,0\n",
    "    for src_v,tgt_v,pred_v in zip(src_vecs,tgt_vecs,predicted_vecs):\n",
    "        src_word=src_model.similar_by_vector(src_v)[0][0]\n",
    "        tgt_word=tgt_model.similar_by_vector(tgt_v)[0][0]\n",
    "        hits=list(w for w,sim in tgt_model.similar_by_vector(pred_v,topn=10))\n",
    "        total+=1\n",
    "        if tgt_word==hits[0]:\n",
    "            top1+=1\n",
    "        if tgt_word in hits[:5]:\n",
    "            top5+=1\n",
    "        if tgt_word in hits[:10]:\n",
    "            top10+=1\n",
    "    print(\"Top1\",top1/total*100,\"percent correct\")\n",
    "    print(\"Top5\",top5/total*100,\"percent correct\")\n",
    "    print(\"Top10\",top10/total*100,\"percent correct\")\n",
    "eval(model_english,model_finnish,val_en,val_fi,predicted_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* We have seen the vectors have interesting properties\n",
    "* In particular, spaces can be mapped onto each other\n",
    "* We have seen how this can be achieved with a simple linear transformation\n",
    "* Optimal transformation has a closed-form solution, but we were lazy and trained it with Keras quite successfully\n",
    "* This demonstrates how Keras can be used also for more generic tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word> dog\n",
      "    WARNING: this word was seen during training\n",
      "   kissa    0.8157461881637573\n",
      "   kani    0.8142617344856262\n",
      "   elukka    0.7629107236862183\n",
      "   koira    0.7623995542526245\n",
      "   aasi    0.7615612745285034\n",
      "   kisu    0.7585018873214722\n",
      "   apina    0.7502449750900269\n",
      "   katti    0.7497345209121704\n",
      "   marsu    0.7484365701675415\n",
      "   hamsteri    0.7365100979804993\n",
      "\n",
      "word> paddle\n",
      "   kajakki    0.7616260647773743\n",
      "   kanootti    0.761273980140686\n",
      "   vene    0.7354230284690857\n",
      "   potkuri    0.7057974338531494\n",
      "   vaijeri    0.6996952295303345\n",
      "   letku    0.6934450268745422\n",
      "   kelkka    0.6847034096717834\n",
      "   köysi    0.6828714609146118\n",
      "   kauha    0.6778427362442017\n",
      "   ruuvi    0.6689082980155945\n",
      "\n",
      "word> sand\n",
      "    WARNING: this word was seen during training\n",
      "   hiekka    0.8602405786514282\n",
      "   sadevesi    0.7444363832473755\n",
      "   kivet    0.7328224182128906\n",
      "   sora    0.7224504351615906\n",
      "   vesi    0.7146394848823547\n",
      "   savi    0.713476836681366\n",
      "   hiekkaa    0.7121126651763916\n",
      "   savea    0.7019836902618408\n",
      "   joki    0.6950616836547852\n",
      "   mutaa    0.6941952705383301\n",
      "\n",
      "word> widow\n",
      "    WARNING: this word was seen during training\n",
      "   sisar    0.8609901666641235\n",
      "   isoisä    0.8604618310928345\n",
      "   vaimo    0.8536527156829834\n",
      "   tytär    0.8469667434692383\n",
      "   isä    0.8383760452270508\n",
      "   veli    0.825786828994751\n",
      "   serkku    0.8231544494628906\n",
      "   leski    0.8223350048065186\n",
      "   eno    0.8192851543426514\n",
      "   isäpuoli    0.8078306317329407\n",
      "\n",
      "word> mobile\n",
      "    WARNING: this word was seen during training\n",
      "   tukiasema    0.7296322584152222\n",
      "   reititin    0.7252651453018188\n",
      "   matkapuhelin    0.7016017436981201\n",
      "   ethernet    0.6969561576843262\n",
      "   wlan    0.6901788711547852\n",
      "   laajakaista    0.6873946785926819\n",
      "   tiedonsiirto    0.6868842244148254\n",
      "   laitteet    0.679649293422699\n",
      "   langattomat    0.6793148517608643\n",
      "   verkkoyhteys    0.6739251613616943\n",
      "\n",
      "word> tree\n",
      "   omenapuu    0.771744966506958\n",
      "   kukka    0.761925220489502\n",
      "   pensas    0.7353760004043579\n",
      "   oksa    0.7265721559524536\n",
      "   lintu    0.7242708206176758\n",
      "   puu    0.7227506041526794\n",
      "   niitty    0.7093478441238403\n",
      "   nurmikko    0.7037678360939026\n",
      "   ruukku    0.7034120559692383\n",
      "   ruoho    0.697083055973053\n",
      "\n",
      "word> end\n"
     ]
    }
   ],
   "source": [
    "# Extra stuff - a function to query the translations, so we can play around\n",
    "def top_n(word,source_model,target_model,transformation_model,topn=5):\n",
    "    try:\n",
    "        source_idx=source_model.vocab[word].index\n",
    "    except:\n",
    "        print(\"Cannot retrieve vector for\",word)\n",
    "        return None\n",
    "    mapped=transformation_model.predict(source_model.vectors[source_idx,:].reshape(1,-1))\n",
    "    return target_model.similar_by_vector(mapped[0])\n",
    "    \n",
    "seen_words=set(en for fi,en in common) #These words were seen during training or validation\n",
    "while True:\n",
    "    wrd=input(\"word> \")\n",
    "    if wrd==\"end\":\n",
    "        break\n",
    "    if wrd in seen_words:\n",
    "        print(\"    WARNING: this word was seen during training\")\n",
    "    hits=top_n(wrd,model_english,model_finnish,model)\n",
    "    for word,sim in hits:\n",
    "        print(\"  \",word,\"  \",sim)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "venv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
