{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting properties of embeddings\n",
    "\n",
    "* We will look at some properties the embeddings have\n",
    "* Test them ourselves in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case you forgot, this is how one can load the embeddings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load embedding model from a file\n",
    "# binary: True if saved as binary file (.bin), False if saved as text file (.vectors or .txt for example)\n",
    "# limit: How many words to read from the model\n",
    "model_english=KeyedVectors.load_word2vec_format(\"data/gigaword-and-wikipedia.bin\", binary=True, limit=100000)\n",
    "model_finnish=KeyedVectors.load_word2vec_format(\"data/pb34_wf_200_v2_skgram.bin\", binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words for 'locomotive':\n",
      "[('locomotives', 0.9040292501449585), ('railcar', 0.8419736623764038), ('railcars', 0.806695282459259), ('locos', 0.7832419872283936), ('steam', 0.7655093669891357), ('bogie', 0.764204740524292), ('shunting', 0.7400067448616028), ('steam-powered', 0.7359079122543335), ('carriages', 0.7352656126022339), ('diesel-electric', 0.7294440269470215)]\n",
      "\n",
      "Most similar words for 'veturi':\n",
      "[('vaunu', 0.7221510410308838), ('veturin', 0.6715301275253296), ('raide', 0.6576318144798279), ('juna', 0.6463427543640137), ('kuorma-auto', 0.6372989416122437), ('perävaunu', 0.6346830725669861), ('veturina', 0.623757004737854), ('traktori', 0.6218146085739136), ('linja-auto', 0.6097179651260376), ('kiskoilla', 0.6076352596282959)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Most similar words for 'locomotive':\")\n",
    "print(model_english.most_similar(\"locomotive\",topn=10))\n",
    "print()\n",
    "print(\"Most similar words for 'veturi':\")\n",
    "print(model_finnish.most_similar(\"veturi\",topn=10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping spaces\n",
    "\n",
    "* One of the more famous properties of the embeddings\n",
    "* Learn a **linear** mapping from one language to another\n",
    "* Can we replicate this?\n",
    "* Learn a network: English vector in -- Finnish vector out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "* Need English-Finnish pairs of words to train\n",
    "* ...google translate, maybe?\n",
    "* Googling around finds this https://github.com/ssut/py-googletrans\n",
    "* ...unofficial API, will get your IP banned if overused, so let's be careful!\n",
    "* official API needs registration, etc.\n",
    "\n",
    "`pip3 install --user googletrans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin= locomotive text= veturi\n",
      "origin= milk text= maito\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator=Translator()\n",
    "translations=translator.translate([\"locomotive\",\"milk\"],src=\"en\",dest=\"fi\")\n",
    "for t in translations:\n",
    "    print(\"origin=\",t.origin,\"text=\",t.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seems to work fine!\n",
    "* Let's grab some translations\n",
    "* The docs say \"max 16000 characters per request\"\n",
    "* Maybe we could translate 2000 words at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab 100000\n",
      "<class 'dict'>\n",
      "Vocab(count:99365, index:635)\n",
      "[('</s>', <gensim.models.keyedvectors.Vocab object at 0x7fc40458db70>), (',', <gensim.models.keyedvectors.Vocab object at 0x7fc40458de10>), ('the', <gensim.models.keyedvectors.Vocab object at 0x7fc40458def0>), ('.', <gensim.models.keyedvectors.Vocab object at 0x7fc40452f048>), ('of', <gensim.models.keyedvectors.Vocab object at 0x7fc40452f0b8>)]\n",
      "Freq sorted ['</s>', ',', 'the', '.', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(\"English vocab\",len(model_english.vocab))\n",
    "print(model_english.vocab.__class__)\n",
    "print(model_english.vocab[\"car\"])\n",
    "#We need a list, in order of frequency\n",
    "words=sorted(model_english.vocab.items(),key=lambda word_dim:word_dim[1].count,reverse=True)\n",
    "print(words[:5])\n",
    "words_freq_sorted=[w for w,_ in words]\n",
    "print(\"Freq sorted\",words_freq_sorted[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have things like `</s>` and `.` in the vocabulary, those we don't want to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_freq_sorted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-70940746a3c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menglish_word_re\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^[a-zA-Z]+$\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#about as stupid simplification as you can get!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfinal_word_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_freq_sorted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0menglish_word_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfinal_word_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words_freq_sorted' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "english_word_re=re.compile(\"^[a-zA-Z]+$\") #about as stupid simplification as you can get!\n",
    "final_word_list=[]\n",
    "for w in words_freq_sorted:\n",
    "    if english_word_re.match(w):\n",
    "        final_word_list.append(w)\n",
    "print(final_word_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Finnish ['ja', 'on', 'ei', 'että', 'se', 'oli', 'mutta', 'tai', 'kun', 'myös', 'ovat', 'ole', 'niin', 'jos', 'kuin'] ... ['seurakunnan', 'selvä', 'tulleet', 'seuraavaan', 'sijasta', 'kuollut', 'I', 'asioihin', 'loput', 'luona', 'talven', 'per', 'ihanan', 'palvelua', 'tietokoneen']\n",
      "Final English ['the', 'of', 'to', 'and', 'in', 'a', 'for', 'The', 'is', 'that', 'was', 'on', 'with', 'said', 'as'] ... ['Greece', 'rally', 'democracy', 'revenue', 'add', 'criticism', 'offices', 'Hussein', 'kids', 'relief', 'promised', 'advance', 'talking', 'boost', 'dispute']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#nicely packed into a function\n",
    "def clean_vocab(gensim_model,regexp):\n",
    "    words=sorted(gensim_model.vocab.items(),key=lambda word_dim:word_dim[1].count,reverse=True)\n",
    "    words_freq_sorted=[w for w,_ in words]\n",
    "    word_re=re.compile(regexp)\n",
    "    final_word_list=[]\n",
    "    for w in words_freq_sorted:\n",
    "        if word_re.match(w):\n",
    "            final_word_list.append(w)\n",
    "    return final_word_list\n",
    "\n",
    "finnish_vocab=clean_vocab(model_finnish,'^[a-zA-ZäöåÖÄÅ]+$')\n",
    "english_vocab=clean_vocab(model_english,'^[a-zA-Z]+$')\n",
    "print(\"Final Finnish\",finnish_vocab[:15],\"...\",finnish_vocab[2000:2015])\n",
    "print(\"Final English\",english_vocab[:15],\"...\",english_vocab[2000:2015])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en -> fi batch at 0 ....OK\n",
      "en -> fi batch at 20 ....OK\n",
      "en -> fi batch at 40 ....OK\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def translate(words,src,dest,batch_size=1000):\n",
    "    result=[] #[(\"dog\",\"koira\"),....]\n",
    "    translator=Translator()\n",
    "    for idx in range(0,len(words),batch_size):\n",
    "        batch=words[idx:idx+batch_size]\n",
    "        try:\n",
    "            translations=translator.translate(batch,src=src,dest=dest)\n",
    "            for t in translations:\n",
    "                result.append((t.origin,t.text))\n",
    "            time.sleep(0.2) #sleep between batches\n",
    "            print(src,\"->\",dest,\"batch at\",idx,\"....OK\")\n",
    "        except:\n",
    "            print(src,\"->\",dest,\"batch at\",idx,\"....FAILED\")\n",
    "            time.sleep(61) #sleep a little longer so Google is not angry\n",
    "            print(src,\"->\",dest,\"...RESTARTING\")\n",
    "            \n",
    "    return result\n",
    "\n",
    "x=translate(english_vocab[:50],\"en\",\"fi\",20) # a small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', ''), ('of', 'of'), ('to', 'että'), ('and', 'ja'), ('in', 'sisään'), ('a', ''), ('for', 'varten'), ('The', ''), ('is', 'on'), ('that', 'että'), ('was', 'oli'), ('on', 'päällä'), ('with', 'kanssa'), ('said', 'sanoi'), ('as', 'kuten'), ('by', 'mennessä'), ('at', 'at'), ('from', 'alkaen'), ('he', 'hän'), ('his', 'hänen'), ('it', 'se'), ('be', 'olla'), ('are', 'olemme'), ('an', ''), ('has', 'on'), ('have', 'omistaa'), ('were', 'olivat'), ('not', 'ei'), ('who', 'Kuka'), ('had', 'oli'), ('which', 'joka'), ('will', 'tahtoa'), ('or', 'tai'), ('their', 'heidän'), ('but', 'mutta'), ('its', 'sen'), ('In', 'Sisään'), ('this', 'Tämä'), ('they', 'ne'), ('been', 'ollut'), ('I', 'minä'), ('also', 'myös'), ('would', 'olisi'), ('one', 'yksi'), ('He', 'Hän'), ('after', 'jälkeen'), ('more', 'lisää'), ('two', 'kaksi'), ('first', 'ensimmäinen'), ('about', 'noin')]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* looks okay\n",
    "* let's run this and save the result for later use, so we don't get banned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en -> fi batch at 0 ....OK\n",
      "en -> fi batch at 150 ....OK\n",
      "en -> fi batch at 300 ....OK\n",
      "en -> fi batch at 450 ....OK\n",
      "en -> fi batch at 600 ....OK\n",
      "en -> fi batch at 750 ....OK\n",
      "en -> fi batch at 900 ....OK\n",
      "en -> fi batch at 1050 ....FAILED\n",
      "en -> fi ...RESTARTING\n",
      "en -> fi batch at 1200 ....FAILED\n",
      "en -> fi ...RESTARTING\n",
      "en -> fi batch at 1350 ....FAILED\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-44b7f1a72387>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(words, src, dest, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mtranslations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.5/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    318\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6860c2a42a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0men_fi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_fi_transl.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_fi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfi_en\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinnish_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-44b7f1a72387>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(words, src, dest, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"batch at\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"....FAILED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m61\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sleep a little longer so Google is not angry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"...RESTARTING\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "en_fi=translate(english_vocab,\"en\",\"fi\",batch_size=150)\n",
    "with open(\"en_fi_transl.json\",\"wt\") as f:\n",
    "    json.dump(en_fi,f)\n",
    "fi_en=translate(finnish_vocab,\"fi\",\"en\",batch_size=150)\n",
    "with open(\"fi_en_transl.json\",\"wt\") as f:\n",
    "    json.dump(fi_en,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* well, we got banned :D\n",
    "* Let's just translate as text files\n",
    "* (quality time manually feeding these into Google translate --- I could have used the official API :)\n",
    "* ...but now it's done, so who cares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_files(words,fname,batch_size):\n",
    "    for idx in range(0,len(words),batch_size):\n",
    "        batch=words[idx:idx+batch_size]\n",
    "        with open(\"trdata/{}_batch_{}.txt\".format(fname,idx),\"wt\") as f:\n",
    "            print(\"\\n\".join(batch),file=f)\n",
    "\n",
    "build_files(english_vocab,\"en-fi-source\",10000)\n",
    "build_files(finnish_vocab,\"fi-en-source\",10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I built manually four files like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trdata/enfi_source_all.txt\n",
      "trdata/enfi_target_all.txt\n",
      "trdata/fien_source_all.txt\n",
      "trdata/fien_target_all.txt\n",
      "  95275 trdata/fien_source_all.txt\n",
      "  95275 trdata/fien_target_all.txt\n",
      "  83618 trdata/enfi_source_all.txt\n",
      "  83618 trdata/enfi_target_all.txt\n",
      " 357786 total\n",
      "FI -> EN\n",
      "ja\tand\n",
      "on\tis\n",
      "ei\tNo\n",
      "että\tthat\n",
      "se\tit\n",
      "oli\twas\n",
      "mutta\tbut\n",
      "tai\tor\n",
      "kun\twhen\n",
      "myös\talso\n",
      "EN -> FI\n",
      "the\t\n",
      "of\tof\n",
      "to\tettä\n",
      "and\tja\n",
      "in\tsisään\n",
      "a\t\n",
      "for\tvarten\n",
      "The\t\n",
      "is\ton\n",
      "that\tettä\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls trdata/fien_* trdata/enfi_*\n",
    "wc -l trdata/fien_* trdata/enfi_*\n",
    "echo \"FI -> EN\"\n",
    "paste trdata/fien_source_all.txt trdata/fien_target_all.txt  | head -n 10\n",
    "echo \"EN -> FI\"\n",
    "paste trdata/enfi_source_all.txt trdata/enfi_target_all.txt  | head -n 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read in and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len fien 95275\n",
      "Len enfi 83610\n",
      "Len common 7100\n",
      "[('vuokrata', 'rent'), ('Jenkins', 'Jenkins'), ('Roses', 'Roses'), ('Beth', 'Beth'), ('TR', 'TR'), ('taantuma', 'downturn'), ('maskotti', 'mascot'), ('Antonin', 'Antonin'), ('kroonisesti', 'chronically'), ('kompromisseja', 'compromises'), ('tunnollisesti', 'scrupulously'), ('tuntea', 'feel'), ('Linus', 'Linus'), ('innoissaan', 'excited'), ('Napoli', 'Naples'), ('haitata', 'hinder'), ('koskematon', 'pristine'), ('Fiona', 'Fiona'), ('muistio', 'memorandum'), ('muslimit', 'Muslims'), ('safari', 'safari'), ('kiima', 'rut'), ('lopulta', 'eventually'), ('nämä', 'these'), ('mekanismit', 'mechanisms'), ('matkamuistoja', 'souvenirs'), ('Nigerian', 'Nigerian'), ('sari', 'sari'), ('petojen', 'beasts'), ('kutistuu', 'shrinking'), ('Camilla', 'Camilla'), ('Lajunen', 'Lajunen'), ('Joe', 'Joe'), ('tuote', 'product'), ('juhlapäivät', 'holidays'), ('XP', 'XP'), ('hyppii', 'hopping'), ('opaskirja', 'guidebook'), ('Dalai', 'Dalai'), ('Melissa', 'Melissa'), ('keskittyminen', 'concentration'), ('ystävät', 'friends'), ('runoilijat', 'poets'), ('vaatimaton', 'modest'), ('kasinot', 'casinos'), ('syy', 'reason'), ('tottuneesti', 'expertly'), ('Jaffa', 'Jaffa'), ('enimmäkseen', 'mostly'), ('Suzuki', 'Suzuki'), ('maailmanloppu', 'apocalypse'), ('Bon', 'Bon'), ('mennyt', 'gone'), ('tykkää', 'likes'), ('vapaaehtoisesti', 'voluntarily'), ('Thatcher', 'Thatcher'), ('voittaja', 'winner'), ('herkkyys', 'sensitivity'), ('jättäen', 'leaving'), ('järjestetty', 'organized'), ('syvä', 'deep'), ('Merkel', 'Merkel'), ('Einar', 'Einar'), ('mac', 'mac'), ('saaret', 'islands'), ('terrori', 'terror'), ('neuvotella', 'negotiate'), ('kierrosta', 'rounds'), ('hengitys', 'respiratory'), ('oivallus', 'realization'), ('siisti', 'neat'), ('analysoidaan', 'analyzed'), ('kohdanneet', 'encountered'), ('sushi', 'sushi'), ('Toshiba', 'Toshiba'), ('duo', 'duo'), ('Ronin', 'Ronin'), ('Frans', 'Frans'), ('bonus', 'bonus'), ('Feng', 'Feng'), ('Bach', 'Bach'), ('tuloaan', 'joining'), ('jo', 'already'), ('Ignatius', 'Ignatius'), ('erikoistuneet', 'specialize'), ('AEG', 'AEG'), ('että', 'that'), ('sivistynyt', 'civilized'), ('ku', 'ku'), ('WD', 'WD'), ('myös', 'also'), ('luterilaisuus', 'Lutheranism'), ('juoma', 'beverage'), ('kirjallinen', 'written'), ('antelias', 'generous'), ('rotta', 'rat'), ('Massachusetts', 'Massachusetts'), ('kaappaa', 'captures'), ('postikulut', 'postage'), ('kalmari', 'squid'), ('tutkia', 'explore'), ('intohimoisesti', 'passionately'), ('Luca', 'Luca'), ('Marek', 'Marek'), ('perinteinen', 'traditional'), ('lukija', 'reader'), ('myrsky', 'storm'), ('raivo', 'rage'), ('epäonnistuu', 'fails'), ('kurdien', 'Kurdish'), ('kitistä', 'whine'), ('interaktiivinen', 'interactive'), ('erottaa', 'distinguish'), ('ilmoitti', 'announced'), ('vallitseva', 'prevalent'), ('Emmy', 'Emmy'), ('läpikuultava', 'translucent'), ('merkittävästi', 'significantly'), ('käytettävyys', 'usability'), ('muotoinen', 'shaped'), ('malaria', 'malaria'), ('sairaalat', 'hospitals'), ('osasto', 'department'), ('olennaiseen', 'essentials'), ('Forza', 'Forza'), ('pyöräily', 'cycling'), ('toimitukset', 'deliveries'), ('murre', 'dialect'), ('Armin', 'Armin'), ('Kia', 'Kia'), ('kiiltävä', 'shiny'), ('teki', 'did'), ('juoksu', 'running'), ('hylätä', 'reject'), ('maksettava', 'due'), ('Liz', 'Liz'), ('pyyhe', 'towel'), ('verkko', 'network'), ('keskikenttäpelaaja', 'midfielder'), ('pehmeästi', 'softly'), ('kuninkaallinen', 'royal'), ('ja', 'and'), ('rapeita', 'crunchy'), ('ikuisuus', 'eternity'), ('Thorp', 'Thorp'), ('kehyksiä', 'frames'), ('työnantaja', 'employer'), ('kuisti', 'porch'), ('Palms', 'Palms'), ('Netflix', 'Netflix'), ('NBC', 'NBC'), ('marsalkka', 'marshal'), ('kohdun', 'uterine'), ('itsenäisesti', 'independently'), ('mehut', 'juices'), ('erehtyä', 'err'), ('traagisesti', 'tragically'), ('monni', 'catfish'), ('par', 'par'), ('TNS', 'TNS'), ('julkkikset', 'celebrities'), ('tekijät', 'factors'), ('juoksi', 'ran'), ('Steve', 'Steve'), ('lisääntyy', 'increases'), ('IBM', 'IBM'), ('Rami', 'Rami'), ('RK', 'RK'), ('Viktoria', 'Viktoria'), ('Sherman', 'Sherman'), ('puutarhuri', 'gardener'), ('legendaarinen', 'legendary'), ('katkeruus', 'bitterness'), ('pelata', 'play'), ('oravia', 'squirrels'), ('tuhonnut', 'destroyed'), ('Atria', 'Atria'), ('päättelee', 'concludes'), ('riittävyys', 'adequacy'), ('harjoitellaan', 'practicing'), ('DE', 'DE'), ('nostalginen', 'nostalgic'), ('Sisko', 'Sister'), ('lämpö', 'heat'), ('valaiseva', 'illustrative'), ('kylpytakki', 'bathrobe'), ('monimuotoisuus', 'diversity'), ('paitsi', 'except'), ('remmi', 'thong'), ('monikko', 'plural'), ('nuo', 'those'), ('Spielberg', 'Spielberg'), ('pyöristää', 'round'), ('des', 'des'), ('Nino', 'Nino'), ('vaarallinen', 'dangerous'), ('välttämättömyys', 'necessity'), ('Hugo', 'Hugo'), ('hajoaminen', 'decomposition'), ('Patrik', 'Patrik'), ('postilaatikko', 'mailbox'), ('laukku', 'bag'), ('terveiset', 'regards'), ('häpeä', 'shame'), ('tuntematon', 'unknown'), ('olemassaolo', 'existence'), ('hankkeet', 'projects'), ('ahkera', 'diligent'), ('sovittaminen', 'adaptation'), ('pimeys', 'darkness'), ('alas', 'down'), ('ammunta', 'shooting'), ('opastus', 'guidance'), ('PT', 'PT'), ('ti', 'ti'), ('Pavel', 'Pavel'), ('Monroe', 'Monroe'), ('jatkuva', 'continuous'), ('Kira', 'Kira'), ('kHz', 'kHz'), ('Jokinen', 'Jokinen'), ('college', 'college'), ('Lehtinen', 'Lehtinen'), ('rakenne', 'structure'), ('suoristaa', 'straighten'), ('puuha', 'chore'), ('kanssa', 'with'), ('liikunta', 'exercise'), ('jyrkkä', 'steep'), ('toteutukset', 'implementations'), ('kiusata', 'bully'), ('tuottajia', 'producers'), ('luokiteltu', 'classified'), ('mainonta', 'advertising'), ('kytkökset', 'affiliations'), ('tönäisi', 'poking'), ('Benin', 'Benin'), ('pelastaja', 'savior'), ('toistaa', 'repeat'), ('apteekki', 'pharmacy'), ('opetukset', 'teachings'), ('komposti', 'compost'), ('lajikkeet', 'varieties'), ('Jeremy', 'Jeremy'), ('Christina', 'Christina'), ('kuoleva', 'moribund'), ('koulunkäynti', 'schooling'), ('adrenaliini', 'adrenaline'), ('kestävä', 'resistant'), ('kissa', 'cat'), ('kohota', 'rise'), ('ässä', 'ace'), ('Frida', 'Frida'), ('kiitollisuus', 'gratitude'), ('Milla', 'Milla'), ('Jennifer', 'Jennifer'), ('valvoi', 'oversaw'), ('PDF', 'PDF'), ('liittimet', 'connectors'), ('välillä', 'between'), ('terva', 'tar'), ('töykeä', 'rude'), ('Vince', 'Vince'), ('Wesley', 'Wesley'), ('tarkkuus', 'accuracy'), ('valokuvaajat', 'photographers'), ('nimellisesti', 'nominally'), ('verkkotunnuksen', 'domain'), ('käänteinen', 'reverse'), ('lounaat', 'lunches'), ('Anaheim', 'Anaheim'), ('Ivan', 'Ivan'), ('Gavin', 'Gavin'), ('WSOP', 'WSOP'), ('Eddie', 'Eddie'), ('vaaleanpunainen', 'pink'), ('kaksinkertaistunut', 'doubled'), ('ajatus', 'thought'), ('äidin', 'maternal'), ('karma', 'karma'), ('vankila', 'prison'), ('uhmaa', 'defying'), ('kuuluva', 'audible'), ('Vincent', 'Vincent'), ('kuninkaat', 'kings'), ('vastauksia', 'answers'), ('lievä', 'mild'), ('useat', 'several'), ('pirullinen', 'devilish'), ('Drew', 'Drew'), ('KTM', 'KTM'), ('määritellä', 'define'), ('TK', 'TK'), ('erityisesti', 'specially'), ('Palma', 'Palma'), ('Bieber', 'Bieber'), ('HDMI', 'HDMI'), ('Parker', 'Parker'), ('Acer', 'Acer'), ('Sierra', 'Sierra')]\n"
     ]
    }
   ],
   "source": [
    "fien=[] #list of (fin,eng) pairs obtained from the fin -> eng direction\n",
    "enfi=[] #list of (fin,eng) pairs, this time obtained from  the eng->fin direction\n",
    "with open(\"trdata/fien_source_all.txt\") as fi_file, open(\"trdata/fien_target_all.txt\") as en_file:\n",
    "    for fi,en in zip(fi_file,en_file):\n",
    "        fi=fi.strip()\n",
    "        en=en.strip()\n",
    "        if fi and en:\n",
    "            fien.append((fi,en))\n",
    "\n",
    "with open(\"trdata/enfi_target_all.txt\") as fi_file, open(\"trdata/enfi_source_all.txt\") as en_file:\n",
    "    for fi,en in zip(fi_file,en_file):\n",
    "        fi=fi.strip()\n",
    "        en=en.strip()\n",
    "        if fi and en:\n",
    "            enfi.append((fi,en))\n",
    "\n",
    "fien_set=set(fien)\n",
    "enfi_set=set(enfi)\n",
    "common=fien_set&enfi_set #keep only pairs which are shared\n",
    "print(\"Len fien\",len(fien_set))\n",
    "print(\"Len enfi\",len(enfi_set))\n",
    "print(\"Len common\",len(common))\n",
    "print(list(common)[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ouch - we lost 90% of the stuff, but such is life\n",
    "* what we got looks good, though :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7100\n",
      "7100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(set(finnish_vocab)&set(fi for fi,en in common)))\n",
    "print(len(set(english_vocab)&set(en for fi,en in common)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices: [17761, 188, 23345, 47640, 45212, 28892, 5850, 8010, 29244, 4137] [63218, 581, 53536, 84987, 24072, 21261, 5298, 1512, 53463, 7604]\n",
      "English model.vectors shape: (100000, 200)\n",
      "Finnish model.vectors shape: (100000, 200)\n",
      "English selected vectors shape: (4624, 200)\n",
      "Finnish selected vectors shape: (4624, 200)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "pairs=[(fi,en) for fi,en in common if fi!=en]\n",
    "random.shuffle(pairs)\n",
    "\n",
    "#Now we need to grab the vectors for the words in question\n",
    "en_indices=[model_english.vocab[en].index for fi,en in pairs]\n",
    "fi_indices=[model_finnish.vocab[fi].index for fi,en in pairs]\n",
    "print(\"Indices:\",en_indices[:10],fi_indices[:10])\n",
    "print(\"English model.vectors shape:\",model_english.vectors.shape)\n",
    "print(\"Finnish model.vectors shape:\",model_finnish.vectors.shape)\n",
    "en_vectors=model_english.vectors[en_indices]\n",
    "fi_vectors=model_finnish.vectors[fi_indices]\n",
    "print(\"English selected vectors shape:\",en_vectors.shape)\n",
    "print(\"Finnish selected vectors shape:\",fi_vectors.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now `en_vectors` is vectors for the 7100 English words in our translation pairs\n",
    "* `fi_vectors` is same for Finnish\n",
    "* ...our training data is done\n",
    "\n",
    "## Learning transformation from Finnish to English\n",
    "\n",
    "* 200-dim vector in, 200-dim vector out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 200)               40200     \n",
      "=================================================================\n",
      "Total params: 40,200\n",
      "Trainable params: 40,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4161 samples, validate on 463 samples\n",
      "Epoch 1/30\n",
      "4161/4161 [==============================] - 1s 336us/step - loss: 0.0716 - val_loss: 0.0556\n",
      "Epoch 2/30\n",
      "4161/4161 [==============================] - 0s 32us/step - loss: 0.0494 - val_loss: 0.0467\n",
      "Epoch 3/30\n",
      "4161/4161 [==============================] - 0s 32us/step - loss: 0.0431 - val_loss: 0.0432\n",
      "Epoch 4/30\n",
      "4161/4161 [==============================] - 0s 33us/step - loss: 0.0404 - val_loss: 0.0417\n",
      "Epoch 5/30\n",
      "4161/4161 [==============================] - 0s 32us/step - loss: 0.0390 - val_loss: 0.0409\n",
      "Epoch 6/30\n",
      "4161/4161 [==============================] - 0s 32us/step - loss: 0.0383 - val_loss: 0.0405\n",
      "Epoch 7/30\n",
      "4161/4161 [==============================] - 0s 33us/step - loss: 0.0378 - val_loss: 0.0403\n",
      "Epoch 8/30\n",
      "4161/4161 [==============================] - 0s 64us/step - loss: 0.0376 - val_loss: 0.0402\n",
      "Epoch 9/30\n",
      "4161/4161 [==============================] - 0s 32us/step - loss: 0.0374 - val_loss: 0.0401\n",
      "Epoch 10/30\n",
      "4161/4161 [==============================] - 0s 35us/step - loss: 0.0373 - val_loss: 0.0400\n",
      "Epoch 11/30\n",
      "4161/4161 [==============================] - 0s 33us/step - loss: 0.0372 - val_loss: 0.0401\n",
      "Epoch 12/30\n",
      "4161/4161 [==============================] - 0s 32us/step - loss: 0.0372 - val_loss: 0.0401\n",
      "Epoch 13/30\n",
      "4161/4161 [==============================] - 0s 32us/step - loss: 0.0371 - val_loss: 0.0401\n",
      "Epoch 14/30\n",
      "4161/4161 [==============================] - 0s 31us/step - loss: 0.0371 - val_loss: 0.0401\n",
      "Epoch 15/30\n",
      "4161/4161 [==============================] - 0s 31us/step - loss: 0.0371 - val_loss: 0.0401\n",
      "Epoch 16/30\n",
      "4161/4161 [==============================] - 0s 31us/step - loss: 0.0371 - val_loss: 0.0401\n",
      "Epoch 17/30\n",
      "4161/4161 [==============================] - 0s 31us/step - loss: 0.0371 - val_loss: 0.0402\n",
      "Epoch 18/30\n",
      "4161/4161 [==============================] - 0s 32us/step - loss: 0.0371 - val_loss: 0.0401\n",
      "Epoch 19/30\n",
      "4161/4161 [==============================] - 0s 32us/step - loss: 0.0370 - val_loss: 0.0401\n",
      "Epoch 20/30\n",
      "4161/4161 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0402\n",
      "Epoch 21/30\n",
      "4161/4161 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0402\n",
      "Epoch 22/30\n",
      "4161/4161 [==============================] - 0s 34us/step - loss: 0.0370 - val_loss: 0.0401\n",
      "Epoch 23/30\n",
      "4161/4161 [==============================] - 0s 33us/step - loss: 0.0370 - val_loss: 0.0402\n",
      "Epoch 24/30\n",
      "4161/4161 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0402\n",
      "Epoch 25/30\n",
      "4161/4161 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0401\n",
      "Epoch 26/30\n",
      "4161/4161 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0402\n",
      "Epoch 27/30\n",
      "4161/4161 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0402\n",
      "Epoch 28/30\n",
      "4161/4161 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0402\n",
      "Epoch 29/30\n",
      "4161/4161 [==============================] - 0s 31us/step - loss: 0.0371 - val_loss: 0.0402\n",
      "Epoch 30/30\n",
      "4161/4161 [==============================] - 0s 32us/step - loss: 0.0371 - val_loss: 0.0402\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "\n",
    "\n",
    "inp=Input(shape=(en_vectors.shape[1],)) #input is 200-dim\n",
    "outp=Dense(fi_vectors.shape[1])(inp) #Simple linear transformation of the input\n",
    "\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"nadam\",loss=\"mse\")\n",
    "hist=model.fit(en_vectors,fi_vectors,batch_size=100,verbose=1,epochs=30,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jamaican', 1.0)]\n",
      "[('Jamaikan', 1.0)]\n",
      "[('irlantilainen', 0.7817846536636353), ('unkarilainen', 0.7307029366493225), ('meksikolainen', 0.7274668216705322), ('brasilialainen', 0.716988205909729), ('skotlantilainen', 0.7149544358253479)]\n",
      "\n",
      "\n",
      "[('hard', 1.0)]\n",
      "[('kova', 1.0)]\n",
      "[('hankala', 0.7053122520446777), ('hyvä', 0.6723480224609375), ('turhauttavaa', 0.6515717506408691), ('kova', 0.6421034932136536), ('inhottava', 0.6195943355560303)]\n",
      "\n",
      "\n",
      "[('person', 0.9999998807907104)]\n",
      "[('henkilö', 1.0)]\n",
      "[('ihminen', 0.7712231874465942), ('henkilö', 0.7443149089813232), ('lähimmäinen', 0.6883211135864258), ('yksilö', 0.6754346489906311), ('aikuinen', 0.6527293920516968)]\n",
      "\n",
      "\n",
      "[('fortunately', 0.9999999403953552)]\n",
      "[('onneksi', 1.0)]\n",
      "[('mutta', 0.7171201705932617), ('siltikin', 0.6973428726196289), ('kuitenkin', 0.6922833919525146), ('kumminkin', 0.691504955291748), ('silti', 0.6911643147468567)]\n",
      "\n",
      "\n",
      "[('stifle', 1.0)]\n",
      "[('tukahduttaa', 0.9999999403953552)]\n",
      "[('tukahduttaa', 0.6876461505889893), ('horjuttaa', 0.6798622608184814), ('vaientaa', 0.6655131578445435), ('kontrolloida', 0.6646119356155396), ('estää', 0.6492694020271301)]\n",
      "\n",
      "\n",
      "[('warned', 1.0)]\n",
      "[('varoitti', 1.0)]\n",
      "[('huomautti', 0.7373246550559998), ('varoitti', 0.7298937439918518), ('väitti', 0.7101077437400818), ('totesi', 0.7004302740097046), ('varoitteli', 0.6979320049285889)]\n",
      "\n",
      "\n",
      "[('ethics', 1.0)]\n",
      "[('etiikka', 0.9999999403953552)]\n",
      "[('etiikka', 0.7667968273162842), ('tiede', 0.702309250831604), ('moraali', 0.6935773491859436), ('eettinen', 0.6895419359207153), ('tieteellinen', 0.6872122287750244)]\n",
      "\n",
      "\n",
      "[('city', 1.0)]\n",
      "[('kaupunki', 1.0)]\n",
      "[('kaupunki', 0.807502269744873), ('kaupunginosa', 0.7648006677627563), ('suurkaupunki', 0.7465440034866333), ('pääkaupunki', 0.7208887338638306), ('kylä', 0.7093544602394104)]\n",
      "\n",
      "\n",
      "[('suffers', 1.0)]\n",
      "[('kärsii', 1.0)]\n",
      "[('kärsii', 0.6894080638885498), ('oireilee', 0.6864587068557739), ('sairaus', 0.6391280889511108), ('oire', 0.6375911235809326), ('masennus', 0.6366496682167053)]\n",
      "\n",
      "\n",
      "[('phenomenal', 1.0)]\n",
      "[('ilmiömäinen', 1.0)]\n",
      "[('huikea', 0.715241551399231), ('huima', 0.6778572201728821), ('ilmiömäinen', 0.670992910861969), ('vahva', 0.6684696674346924), ('hurja', 0.6646061539649963)]\n",
      "\n",
      "\n",
      "[('weeks', 1.0)]\n",
      "[('viikkoa', 1.0)]\n",
      "[('viikkoa', 0.7591757774353027), ('kuukautta', 0.7533406019210815), ('puolivuotta', 0.7349545955657959), ('päivää', 0.7224348783493042), ('kuukausi', 0.700255811214447)]\n",
      "\n",
      "\n",
      "[('overweight', 1.0)]\n",
      "[('ylipainoinen', 0.9999999403953552)]\n",
      "[('ylipainoinen', 0.8000369071960449), ('alipainoinen', 0.7395967841148376), ('sairas', 0.7267540097236633), ('sairaalloisen', 0.7212255001068115), ('ylipainoisia', 0.7162489295005798)]\n",
      "\n",
      "\n",
      "[('growls', 0.9999999403953552)]\n",
      "[('murisee', 0.9999999403953552)]\n",
      "[('murina', 0.6682778000831604), ('murisee', 0.6601071953773499), ('vaimeasti', 0.6506008505821228), ('lauluääni', 0.6449493169784546), ('äkäinen', 0.6363429427146912)]\n",
      "\n",
      "\n",
      "[('healing', 1.0)]\n",
      "[('paranemista', 1.0)]\n",
      "[('hengellinen', 0.6902869343757629), ('kärsimys', 0.6756868362426758), ('ruumiillinen', 0.6700075268745422), ('rukoileminen', 0.6685524582862854), ('rukous', 0.6678076982498169)]\n",
      "\n",
      "\n",
      "[('witch', 1.0)]\n",
      "[('noita', 1.0)]\n",
      "[('murhaaja', 0.7074743509292603), ('hirviö', 0.7016023397445679), ('väkivaltainen', 0.6909950971603394), ('julma', 0.6884417533874512), ('viaton', 0.688107967376709)]\n",
      "\n",
      "\n",
      "[('Ulysses', 1.0)]\n",
      "[('Odysseus', 1.0)]\n",
      "[('Edvard', 0.6070593595504761), ('eversti', 0.5991891622543335), ('Rikhard', 0.5740542411804199), ('Ilmari', 0.5704573392868042), ('Krohnin', 0.5704061985015869)]\n",
      "\n",
      "\n",
      "[('discoverer', 1.0)]\n",
      "[('löytäjä', 0.9999999403953552)]\n",
      "[('kemisti', 0.6575525403022766), ('tiedemies', 0.6330363750457764), ('arkeologi', 0.6278406381607056), ('biologi', 0.6231499314308167), ('keksijä', 0.6061349511146545)]\n",
      "\n",
      "\n",
      "[('psychiatric', 1.0)]\n",
      "[('psykiatrinen', 1.0)]\n",
      "[('psykiatrinen', 0.7817597389221191), ('lääketieteellinen', 0.7437269687652588), ('psykoterapia', 0.7186416387557983), ('diagnoosi', 0.7019033432006836), ('lääkehoito', 0.697333574295044)]\n",
      "\n",
      "\n",
      "[('instrumentation', 1.0)]\n",
      "[('instrumentit', 1.0)]\n",
      "[('akustinen', 0.7633490562438965), ('äänentoisto', 0.7029043436050415), ('äänimaailma', 0.699722170829773), ('sointi', 0.6868582367897034), ('akustiset', 0.6656726002693176)]\n",
      "\n",
      "\n",
      "[('grind', 1.0000001192092896)]\n",
      "[('jauhaa', 1.0)]\n",
      "[('pureskella', 0.6597300171852112), ('sahata', 0.6224485635757446), ('puristaa', 0.6131750345230103), ('nostella', 0.6131485104560852), ('kiskoa', 0.6085071563720703)]\n",
      "\n",
      "\n",
      "[('empire', 1.0)]\n",
      "[('imperiumi', 1.0)]\n",
      "[('imperiumi', 0.8066945672035217), ('sivilisaatio', 0.690842866897583), ('monarkia', 0.6712270975112915), ('hallitsija', 0.6667708158493042), ('kansakunta', 0.6643348336219788)]\n",
      "\n",
      "\n",
      "[('learned', 0.9999999403953552)]\n",
      "[('oppinut', 1.0)]\n",
      "[('oppinut', 0.6227972507476807), ('puhunut', 0.6085976958274841), ('selittänyt', 0.6085847020149231), ('opettanut', 0.6054564118385315), ('kertonut', 0.6052663922309875)]\n",
      "\n",
      "\n",
      "[('so', 1.0)]\n",
      "[('niin', 1.0)]\n",
      "[('niin', 0.8027597665786743), ('mutta', 0.7888758182525635), ('silti', 0.7722792625427246), ('vaikka', 0.7642935514450073), ('siltikin', 0.7535256147384644)]\n",
      "\n",
      "\n",
      "[('altered', 1.0)]\n",
      "[('muuttunut', 1.0)]\n",
      "[('muunneltu', 0.6278818845748901), ('parannettu', 0.5996224880218506), ('laajennettu', 0.5993558764457703), ('uudistettu', 0.5885714888572693), ('vanhentunut', 0.5806221961975098)]\n",
      "\n",
      "\n",
      "[('surrender', 1.0)]\n",
      "[('antautuminen', 0.9999999403953552)]\n",
      "[('liittolainen', 0.6645520925521851), ('kukistaa', 0.6493602991104126), ('surmata', 0.6417148113250732), ('armeija', 0.6378630995750427), ('murhata', 0.6347488164901733)]\n",
      "\n",
      "\n",
      "[('euthanasia', 1.0)]\n",
      "[('eutanasia', 1.0)]\n",
      "[('abortti', 0.739433765411377), ('homous', 0.7328349351882935), ('homoseksuaalisuuden', 0.7301200032234192), ('homoseksuaalisuus', 0.7189474105834961), ('abortin', 0.7164715528488159)]\n",
      "\n",
      "\n",
      "[('axiom', 1.0)]\n",
      "[('selviö', 0.9999999403953552)]\n",
      "[('määritelmä', 0.7377861738204956), ('logiikka', 0.7371050119400024), ('oletus', 0.7037422060966492), ('metodi', 0.6992441415786743), ('teoria', 0.6850974559783936)]\n",
      "\n",
      "\n",
      "[('toddler', 1.0)]\n",
      "[('taapero', 1.0)]\n",
      "[('pikkutyttö', 0.8024106025695801), ('tyttö', 0.7827569246292114), ('lapsi', 0.7431641817092896), ('äiti', 0.7400320768356323), ('vauva', 0.7338100075721741)]\n",
      "\n",
      "\n",
      "[('pictures', 1.0)]\n",
      "[('kuvat', 1.0)]\n",
      "[('filmit', 0.7016283273696899), ('kuvat', 0.6812454462051392), ('animaatiot', 0.6661088466644287), ('valokuvat', 0.6646617650985718), ('videot', 0.6607128977775574)]\n",
      "\n",
      "\n",
      "[('sessions', 1.0)]\n",
      "[('istuntoja', 1.0)]\n",
      "[('istunnot', 0.6739488840103149), ('harjoitukset', 0.6561952829360962), ('tapaamiset', 0.6372936964035034), ('treenit', 0.622071385383606), ('oppitunnit', 0.6189417839050293)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_en,val_fi,_=hist.validation_data\n",
    "predicted_fi=model.predict(val_en)\n",
    "for en,fi,pred_fi in list(zip(val_en,val_fi,predicted_fi))[:30]:\n",
    "    print(model_english.similar_by_vector(en,topn=1))\n",
    "    print(model_finnish.similar_by_vector(fi,topn=1))\n",
    "    print(model_finnish.similar_by_vector(pred_fi,topn=5))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 32.18142548596112 percent correct\n",
      "Top5 52.05183585313174 percent correct\n",
      "Top10 59.395248380129594 percent correct\n"
     ]
    }
   ],
   "source": [
    "def eval(src_model,tgt_model,src_vecs,tgt_vecs,predicted_vecs):\n",
    "    top1,top5,top10,total=0,0,0,0\n",
    "    for src_v,tgt_v,pred_v in zip(src_vecs,tgt_vecs,predicted_vecs):\n",
    "        src_word=src_model.similar_by_vector(src_v)[0][0]\n",
    "        tgt_word=tgt_model.similar_by_vector(tgt_v)[0][0]\n",
    "        hits=list(w for w,sim in tgt_model.similar_by_vector(pred_v,topn=10))\n",
    "        total+=1\n",
    "        if tgt_word==hits[0]:\n",
    "            top1+=1\n",
    "        if tgt_word in hits[:5]:\n",
    "            top5+=1\n",
    "        if tgt_word in hits[:10]:\n",
    "            top10+=1\n",
    "    print(\"Top1\",top1/total*100,\"percent correct\")\n",
    "    print(\"Top5\",top5/total*100,\"percent correct\")\n",
    "    print(\"Top10\",top10/total*100,\"percent correct\")\n",
    "eval(model_english,model_finnish,val_en,val_fi,predicted_fi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "venv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
