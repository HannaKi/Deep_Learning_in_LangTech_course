{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Copy of rnn_text_generation.ipynb","provenance":[{"file_id":"https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/rnn_text_generation.ipynb","timestamp":1585820641027}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jiEluYamRoX0","colab_type":"text"},"source":["# RNN text generation\n","\n","In this notebook, we'll have a look at simple character-level text generation with recurrent neural networks in Keras.\n","\n","This is largely a simplified version of the Tensorflow tutorial [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation), which in turn follows in part Andrej Karpathy's\n"," [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."]},{"cell_type":"markdown","metadata":{"id":"c6YG4cB7RoX3","colab_type":"text"},"source":["## Load Shakespeare dataset\n","\n","We'll use a dataset of texts from Shakespeare provided by Google. The approach would work with any reasonably-sized plain text file."]},{"cell_type":"code","metadata":{"id":"yoTT3VsdRoX7","colab_type":"code","outputId":"4fe5b2e2-cdfc-4d88-ce78-914ef1895323","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1585820912921,"user_tz":-180,"elapsed":3181,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n","!wget -nc https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-04-02 09:48:32--  https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.112.128, 2607:f8b0:4001:c12::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.112.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘shakespeare.txt’\n","\n","\rshakespeare.txt       0%[                    ]       0  --.-KB/s               \rshakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n","\n","2020-04-02 09:48:32 (80.0 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZgIQm8bGRoYS","colab_type":"text"},"source":["This is a simple plain text file, so we'll just read it in directly."]},{"cell_type":"code","metadata":{"id":"CfXAvvN8RoYU","colab_type":"code","outputId":"2a113048-94c8-487e-f61f-bed16fc132da","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1585820936266,"user_tz":-180,"elapsed":635,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["with open('shakespeare.txt') as f:\n","    text = f.read()\n","\n","print(len(text))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["1115394\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FbIpD-mGRoYh","colab_type":"text"},"source":["Note that the text includes punctuation and newline characters."]},{"cell_type":"code","metadata":{"id":"P_xnI2r6RoYj","colab_type":"code","outputId":"e4765c6c-b3e0-4c5d-afaf-309f8725d3d7","colab":{"base_uri":"https://localhost:8080/","height":313},"executionInfo":{"status":"ok","timestamp":1585820991820,"user_tz":-180,"elapsed":682,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["print(repr(text[:200]))\n","print(text[:200])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'\n","First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A_wD3-iCRoYs","colab_type":"text"},"source":["## Vectorize text\n","\n","First, find the set of unique characters in the text:"]},{"cell_type":"code","metadata":{"id":"3j9UtC8_RoYu","colab_type":"code","outputId":"83bf6cb2-3ba4-4a48-f862-116791ec6580","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1585821042584,"user_tz":-180,"elapsed":644,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["characters = sorted(set(text))\n","\n","# What did we get?\n","print(len(characters))\n","print(characters)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["65\n","['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mooIIumtRoY8","colab_type":"text"},"source":["Create a mapping from characters to integers and the inverse mapping from those integers back to the characters."]},{"cell_type":"code","metadata":{"id":"Fl9jfGMGRoY_","colab_type":"code","colab":{}},"source":["char_to_int = { c: i for i, c in enumerate(characters) }\n","# no padding needed so 0 is ok\n","int_to_char = { i: c for c, i in char_to_int.items() }"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q1ODX7a0RoZM","colab_type":"text"},"source":["Let's have a look at that mapping."]},{"cell_type":"code","metadata":{"id":"hn-13k6dRoZN","colab_type":"code","outputId":"a79bfe4a-490d-4127-ed8c-e3e4af0242aa","colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"status":"ok","timestamp":1585821156941,"user_tz":-180,"elapsed":828,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["from pprint import pprint    # pretty-printer\n","\n","\n","def truncate_dict(d, count=10):\n","    # Returns at most count items from the given dictionary.  \n","    return dict(i for i, _ in zip(d.items(), range(count)))\n","\n","\n","pprint(truncate_dict(char_to_int, 20))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["{'\\n': 0,\n"," ' ': 1,\n"," '!': 2,\n"," '$': 3,\n"," '&': 4,\n"," \"'\": 5,\n"," ',': 6,\n"," '-': 7,\n"," '.': 8,\n"," '3': 9,\n"," ':': 10,\n"," ';': 11,\n"," '?': 12,\n"," 'A': 13,\n"," 'B': 14,\n"," 'C': 15,\n"," 'D': 16,\n"," 'E': 17,\n"," 'F': 18,\n"," 'G': 19}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"inC6pF9QRoZT","colab_type":"text"},"source":["Map text characters to the (arbitrary) indices defined above:"]},{"cell_type":"code","metadata":{"id":"gcwPVaHoRoZV","colab_type":"code","outputId":"197e770c-32bc-426a-f4e2-e6e974b46279","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1585821220405,"user_tz":-180,"elapsed":739,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["data = [char_to_int[c] for c in text] # kirjaimet numeroiksi\n","\n","print(data[:100])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nDe_aX-PRoZf","colab_type":"text"},"source":["Next, let's split that data into fixed-length parts for training.\n","\n","(The +1 is here because we'll be shifting the outputs by one character.)"]},{"cell_type":"code","metadata":{"id":"-lAzusaHRoZg","colab_type":"code","outputId":"29189324-7083-4e2e-889d-392201ed8639","colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1585821272731,"user_tz":-180,"elapsed":638,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["# many input to many output\n","sequence_length = 100\n","\n","sequences = []\n","for i in range(0, len(data), sequence_length+1):\n","    sequences.append(data[i:i+sequence_length+1])\n","sequences.pop()    # drop (likely) incomplete last sequence\n","\n","# jokainen tulosteen rivi 100 merkkiä (sequence_length) \n","for i in range(5):\n","    print(repr(''.join(int_to_char[c] for c in sequences[i])))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l7524KX2RoZo","colab_type":"text"},"source":["Split into paired inputs and outputs so that each output matches the text of its input, shifted to the right by one character."]},{"cell_type":"code","metadata":{"id":"PTW7jwcERoZq","colab_type":"code","outputId":"e09e41fc-c41f-4c9c-f0e2-9d9abf5abae5","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1585821515064,"user_tz":-180,"elapsed":817,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["inputs, outputs = [], []\n","for sequence in sequences:\n","    inputs.append(sequence[:-1])\n","    outputs.append(sequence[1:])\n","    \n","    \n","print(repr(''.join(int_to_char[c] for c in inputs[0]))) # input \n","# jokaisen lisämerkin perusteella yritetään ennustaa seuraava F: ennusta i, Fi: ennusta r, Fir: ennusta s\n","print(repr(''.join(int_to_char[c] for c in outputs[0])))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WMJ3cAt5RoZ0","colab_type":"text"},"source":["## Build model\n","\n","We build a straightforward RNN model where each RNN state outputs unnormalized predictions (\"logits\"):\n","\n","* input: sequence of `sequence_length` integers corresponding to characters\n","* embedding: randomly initialized mapping to `embedding_dim`-dimensional vectors\n","* rnn: recurrent neural network with `rnn_units`-dimensional state\n","* output: `num_classes`-dimensional fully connected layer with softmax activation\n"]},{"cell_type":"code","metadata":{"id":"5F9-VTqURoZ1","colab_type":"code","outputId":"0cd932a4-dae5-4df8-83a7-440f137ed044","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"ok","timestamp":1585821997750,"user_tz":-180,"elapsed":10016,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, GRU, Dense\n","\n","\n","vocab_size = len(characters)\n","embedding_dim = 256 #values from google example\n","rnn_units = 1024\n","\n","\n","input_ = Input(shape=(sequence_length,))\n","embedding = Embedding(vocab_size, embedding_dim)(input_)\n","# Note: return_sequences=True\n","rnn = GRU(rnn_units, return_sequences=True)(embedding) # gated recurrent unit\n","# Note: no activation function (e.g. softmax)\n","# return_sequences=True THIS is the difference between one to one from many to many\n","# Jokainen 100 RNN-yksikkö tuottaa oman outputin (hidden staten lisäksi)\n","output = Dense(vocab_size)(rnn)\n","model = Model(inputs=[input_], outputs=[output])\n","\n","print(model.summary())"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 100)]             0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 100, 256)          16640     \n","_________________________________________________________________\n","gru (GRU)                    (None, 100, 1024)         3938304   \n","_________________________________________________________________\n","dense (Dense)                (None, 100, 65)           66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S3-WcRDjXyTx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"55d41290-a611-465b-8af3-e0aeb769ab25","executionInfo":{"status":"ok","timestamp":1585822422955,"user_tz":-180,"elapsed":649,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["print(sequence_length)\n","print(vocab_size)\n","print(vocab_size*embedding_dim) # size of the input matrix\n","\n","# logits = log probabilities, raw outputs"],"execution_count":11,"outputs":[{"output_type":"stream","text":["100\n","65\n","16640\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fyjl53rLRoZ_","colab_type":"text"},"source":["Define an appropriate loss function and compile the model."]},{"cell_type":"code","metadata":{"id":"EE9j9n9tRoaB","colab_type":"code","colab":{}},"source":["from tensorflow.keras.losses import sparse_categorical_crossentropy\n","\n","# Since we are not using softmax we have raw output and need to tune \n","# loss function:\n","\n","# This is just 'sparse_categorical_crossentropy' where from_logits=True\n","# specifies that the values are unnormalized (no softmax)\n","def sparse_categorical_crossentropy_with_logits(labels, logits):\n","    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","\n","model.compile(\n","    optimizer='adam',\n","    loss=sparse_categorical_crossentropy_with_logits,\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T3MP8zkXRoaP","colab_type":"text"},"source":["Cast inputs and outputs as numpy arrays"]},{"cell_type":"code","metadata":{"id":"d-Dtg8faRoaR","colab_type":"code","outputId":"ede2ad12-d02e-4a25-ea80-5a54d7a28a01","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1585822512348,"user_tz":-180,"elapsed":753,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["import numpy as np\n","\n","X, Y = np.array(inputs), np.array(outputs)\n","print(len(inputs), len(inputs[0]))\n","print(X.shape)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["11043 100\n","(11043, 100)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZaLK2713Roab","colab_type":"text"},"source":["Train the model. (This will take a while unless running on a GPU.)"]},{"cell_type":"code","metadata":{"id":"W3M3lifPRoac","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":384},"outputId":"5b9cadb2-cc51-4824-9115-f2774ea97455","executionInfo":{"status":"ok","timestamp":1585823119770,"user_tz":-180,"elapsed":574224,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["epochs = 10\n","batch_size = 16\n","\n","\n","history = model.fit(\n","    X,\n","    Y,\n","    epochs=epochs,\n","    batch_size=batch_size\n",")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","691/691 [==============================] - 57s 83ms/step - loss: 2.0904\n","Epoch 2/10\n","691/691 [==============================] - 56s 81ms/step - loss: 1.5259\n","Epoch 3/10\n","691/691 [==============================] - 56s 81ms/step - loss: 1.3863\n","Epoch 4/10\n","691/691 [==============================] - 56s 82ms/step - loss: 1.3078\n","Epoch 5/10\n","691/691 [==============================] - 57s 82ms/step - loss: 1.2444\n","Epoch 6/10\n","691/691 [==============================] - 56s 81ms/step - loss: 1.1853\n","Epoch 7/10\n","691/691 [==============================] - 56s 81ms/step - loss: 1.1260\n","Epoch 8/10\n","691/691 [==============================] - 57s 82ms/step - loss: 1.0663\n","Epoch 9/10\n","691/691 [==============================] - 57s 82ms/step - loss: 1.0073\n","Epoch 10/10\n","691/691 [==============================] - 56s 81ms/step - loss: 0.9519\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pI5oGUOGRoan","colab_type":"text"},"source":["## Generate text with trained model\n","\n","Note that as we set a fixed `sequence_length` to keep things simple, we won't actually be using the RNN state here; we're instead simply always giving the RNN the catenation of the initial seed text and previously generated characters  as input."]},{"cell_type":"code","metadata":{"id":"vPSZ03MuRoao","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":384},"outputId":"dcd545a6-7939-4b4e-d2f8-94eb20d00a1f","executionInfo":{"status":"ok","timestamp":1585823485751,"user_tz":-180,"elapsed":8579,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["import tensorflow as tf\n","\n","\n","def predict_one_character(input_, temperature=1.0):\n","    input_ = ' ' * (sequence_length-len(input_)) + input_   # pad to sequence_length\n","    input_ = input_[-sequence_length:]    # truncate if longer\n","    X = np.array([[char_to_int[c] for c in input_]])    # note batch dim\n","    model.reset_states()    # forget state\n","    Y = model(X)\n","    Y = tf.squeeze(Y, 0)    # drop batch dim\n","    Y = Y / temperature   # log probabilities: dividing with constant flattens the distribution\n","    # if temp = 1.0, model predicts the most probable outputs\n","    # if temp = high, for exampe 3.0, model predicts jibberish\n","    # Sample from categorical distribution \n","    pred_id = tf.random.categorical(Y, num_samples=1)[-1,0].numpy()\n","    return int_to_char[pred_id]\n","\n","\n","seed = 'ROMEO'\n","string = seed\n","generated = []\n","for i in range(500):\n","    # Lower temperature gives more likely predictions, higher more surprising\n","    char = predict_one_character(string, temperature=1.0) # change param value here to see what happens\n","    generated.append(char)\n","    string += char\n","\n","\n","print(seed + ''.join(generated))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["ROMEO:\n","Good morrow with him and all.\n","\n","PETRUCHIO:\n","Rather the fatal sweeter.\n","\n","ROMEO:\n","I am very alone; sure, I speak; my noble father,\n","To execute this fair amight to Barblone.\n","To the desire and the house of judging sith,\n","When like presence of changing joints forth\n","Against their husbands; but noble looks on his\n","death; but he hath sulper'd the Titch and Marcius.\n","The precious Warwick, take her formort.\n","\n","KING LEWIS XI:\n","To me, what news, who would he's myself?\n","\n","First Servingman:\n","I would myself besides my min\n"],"name":"stdout"}]}]}