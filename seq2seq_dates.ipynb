{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Copy of Copy of seq2seq_dates.ipynb","provenance":[{"file_id":"1Dnfon-Uz8lUIQE6FebZKovnY6o8OB15R","timestamp":1586686722186},{"file_id":"https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/seq2seq_dates.ipynb","timestamp":1586423639801}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"nD6RSra6N5ES","colab_type":"text"},"source":["# Sequence-to-sequence RNN\n","\n","This notebook shows an example of a character-level sequence-to-sequence recurrent neural network model using Keras.\n","\n","The implementation draws on [A ten-minute introduction to sequence-to-sequence learning in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) and [English to Katakana using a Sequence-to-Sequence model](https://github.com/wanasit/katakana)"]},{"cell_type":"markdown","metadata":{"id":"5hqzhQIKN5EU","colab_type":"text"},"source":["## Configuration\n","\n","The following variables configure a few aspects of the data, model, and training process. To adapt this example to a different dataset, you'll probably want to change these to match."]},{"cell_type":"code","metadata":{"id":"9AAEZKcUN5EY","colab_type":"code","colab":{}},"source":["# Maximum number of examples to read\n","MAX_EXAMPLES = 100000\n","\n","# Maximum length of input sequence in characters\n","INPUT_LENGTH = 35\n","\n","# Maximum length of output sequence in characters, including start symbol\n","OUTPUT_LENGTH = 11\n","\n","# Number of epochs to train for\n","EPOCHS = 3\n","\n","# Training batch size\n","BATCH_SIZE = 64\n","\n","# Size of character embeddings\n","EMBEDDING_DIM = 100\n","\n","# Size of RNN states\n","RNN_UNITS = 100"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u5ciOSFWN5Eo","colab_type":"text"},"source":["## Download dataset\n","\n","We'll be using an automatically generated dataset of freeform and standardized dates."]},{"cell_type":"code","metadata":{"id":"CLukh0EjN5Ez","colab_type":"code","outputId":"0b276dda-853a-4b56-b7e4-d440d22edddb","colab":{"base_uri":"https://localhost:8080/","height":222},"executionInfo":{"status":"ok","timestamp":1586426225154,"user_tz":-180,"elapsed":7468,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["!wget -nc https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/generated_dates.txt"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2020-04-09 09:57:01--  https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/generated_dates.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2872063 (2.7M) [text/plain]\n","Saving to: ‘generated_dates.txt’\n","\n","\rgenerated_dates.txt   0%[                    ]       0  --.-KB/s               \rgenerated_dates.txt  68%[============>       ]   1.88M  9.39MB/s               \rgenerated_dates.txt 100%[===================>]   2.74M  10.7MB/s    in 0.3s    \n","\n","2020-04-09 09:57:02 (10.7 MB/s) - ‘generated_dates.txt’ saved [2872063/2872063]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aGNBkZ-kN5E-","colab_type":"text"},"source":["## Load data"]},{"cell_type":"code","metadata":{"id":"dPfwgGKqN5FA","colab_type":"code","outputId":"463f3fa7-2b1b-482a-ce06-b3322c796edf","colab":{"base_uri":"https://localhost:8080/","height":147},"executionInfo":{"status":"ok","timestamp":1586426225939,"user_tz":-180,"elapsed":8241,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["import random\n","\n","\n","def load_data(fn, separator='\\t', has_header_line=False, max_examples=None):\n","    data = []\n","    with open(fn) as f:\n","        if has_header_line:\n","            next(f)    # skip header\n","        for line in f:\n","            if max_examples is not None and len(data) >= max_examples:\n","                break\n","            line = line.rstrip('\\n')\n","            input_text, output_text = line.split(separator)\n","            data.append([input_text, output_text])\n","    return data\n","\n","\n","data = load_data('generated_dates.txt', max_examples=MAX_EXAMPLES)\n","\n","random.seed(1234)    # make random.shuffle() repeatable\n","random.shuffle(data)\n","\n","input_texts = [input_text for input_text, output_text in data]\n","output_texts = [output_text for input_text, output_text in data]\n","\n","\n","# Have a look at the source data\n","print('max input length', max(len(t) for t in input_texts))\n","print('max output length', max(len(t) for t in output_texts))\n","for i in range(5):\n","    print(input_texts[i], '→', output_texts[i])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["max input length 35\n","max output length 10\n","ELOKUUN 23. 1977 → 23.08.1977\n","23.10.1972 → 23.10.1972\n","2006/12/22 → 22.12.2006\n","1990/11/28 → 28.11.1990\n","syyskuun 5. 2015 → 05.09.2015\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wyhLuuLwN5FI","colab_type":"text"},"source":["## Vectorize text"]},{"cell_type":"markdown","metadata":{"id":"QmSRmQC_N5FK","colab_type":"text"},"source":["Tokenize on the character level with lowercasing. To allow different input and output alphabets, create separate tokenizers for each.\n","\n","We're slightly abusing the tokenizer's out-of-vocabulary item support here to introduce a special `<START>` token into the mapping. As we're not restricting the number of words, this token will never be output by the tokenizer, so we're free to use it for our purposes."]},{"cell_type":"code","metadata":{"id":"tpU07Gw-N5FN","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","\n","input_tokenizer = Tokenizer(lower=True, char_level=True)\n","output_tokenizer = Tokenizer(lower=True, char_level=True, oov_token='<START>')\n","\n","input_tokenizer.fit_on_texts(input_texts)\n","output_tokenizer.fit_on_texts(output_texts)\n","\n","# Remember these\n","INPUT_VOCAB_SIZE = max(input_tokenizer.word_index.values()) + 1\n","OUTPUT_VOCAB_SIZE = max(output_tokenizer.word_index.values()) + 1\n","START_INDEX = output_tokenizer.word_index['<START>']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q1qd1kTaN5FV","colab_type":"text"},"source":["Let's have a look at those mappings."]},{"cell_type":"code","metadata":{"id":"deZLEH82N5FX","colab_type":"code","outputId":"f161175f-ad5b-4a04-895e-c9826ca96f95","colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"status":"ok","timestamp":1586426230277,"user_tz":-180,"elapsed":1494,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["from pprint import pprint    # pretty-printer\n","\n","\n","def truncate_dict(d, count=10):\n","    # Returns at most count items from the given dictionary.  \n","    return dict(i for i, _ in zip(d.items(), range(count)))\n","\n","\n","print('Number of entries in input mapping:', len(input_tokenizer.word_index))\n","pprint(truncate_dict(input_tokenizer.word_index))\n","print('Number of entries in output mapping:', len(output_tokenizer.word_index))\n","pprint(truncate_dict(output_tokenizer.word_index))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Number of entries in input mapping: 31\n","{' ': 2,\n"," '.': 8,\n"," '/': 9,\n"," '0': 4,\n"," '1': 1,\n"," '2': 6,\n"," '9': 5,\n"," 'n': 7,\n"," 'u': 3,\n"," 'ä': 10}\n","Number of entries in output mapping: 12\n","{'.': 2,\n"," '0': 3,\n"," '1': 4,\n"," '2': 5,\n"," '3': 9,\n"," '5': 10,\n"," '7': 8,\n"," '8': 7,\n"," '9': 6,\n"," '<START>': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rD-QpNtpN5Fh","colab_type":"text"},"source":["Map input and output texts to integer sequences using the tokenizers, then pad and truncate the sequences to desired input and output lengths."]},{"cell_type":"code","metadata":{"id":"ZMogYnmLN5Fi","colab_type":"code","outputId":"a500e033-f92b-42c5-9094-cacb10fe9b8c","colab":{"base_uri":"https://localhost:8080/","height":242},"executionInfo":{"status":"ok","timestamp":1586426236660,"user_tz":-180,"elapsed":3096,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","def vectorize(texts, tokenizer, maxlen, quiet=False):\n","    # This bit does the work\n","    sequences = tokenizer.texts_to_sequences(texts)\n","    padded = pad_sequences(sequences, maxlen=maxlen, padding='post')\n","    \n","    # This just prints out the first input and its vectorized versions\n","    if not quiet:\n","        print('Text:', texts[0])\n","        print('Sequence:', sequences[0])\n","        print('Padded:', padded[0])\n","        print('Mapped back:', [tokenizer.index_word.get(i, '-') for i in padded[0]])\n","    \n","    return padded\n","\n","\n","encoder_X = vectorize(input_texts, input_tokenizer, INPUT_LENGTH)\n","decoder_Y = vectorize(output_texts, output_tokenizer, OUTPUT_LENGTH)\n","\n","# This creates numpy arrays:\n","print('type(encoder_X):', type(encoder_X))\n","print('encoder_X.shape:', encoder_X.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Text: ELOKUUN 23. 1977\n","Sequence: [27, 25, 16, 11, 3, 3, 7, 2, 6, 18, 8, 2, 1, 5, 13, 13]\n","Padded: [27 25 16 11  3  3  7  2  6 18  8  2  1  5 13 13  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0]\n","Mapped back: ['e', 'l', 'o', 'k', 'u', 'u', 'n', ' ', '2', '3', '.', ' ', '1', '9', '7', '7', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n","Text: 23.08.1977\n","Sequence: [5, 9, 2, 3, 7, 2, 4, 6, 8, 8]\n","Padded: [5 9 2 3 7 2 4 6 8 8 0]\n","Mapped back: ['2', '3', '.', '0', '8', '.', '1', '9', '7', '7', '-']\n","type(encoder_X): <class 'numpy.ndarray'>\n","encoder_X.shape: (100000, 35)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iuM_SvheN5Fq","colab_type":"text"},"source":["In prediction, the decoder will receive its last output as input at each timestep. During training, we'll use _teacher forcing_, where the decoder is instead given the correct previous output. To implement this, the output sequence (`decoder_Y`) is shifted one character forward, and the special start symbol is placed first in the sequence."]},{"cell_type":"code","metadata":{"id":"jnTqURk4N5Fr","colab_type":"code","outputId":"4038e696-5382-4b5a-d359-29fa24b3140e","colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"status":"ok","timestamp":1586426248780,"user_tz":-180,"elapsed":926,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["import numpy as np\n","\n","\n","decoder_X = np.zeros_like(decoder_Y)\n","decoder_X[:,1:] = decoder_Y[:,:-1]\n","decoder_X[:,0] = START_INDEX\n","\n","print('decoder_X:', decoder_X[0])\n","print('Mapped back:', [output_tokenizer.index_word.get(i, '-') for i in decoder_X[0]])\n","print('decoder_Y:', decoder_Y[0])\n","print('Mapped back:', [output_tokenizer.index_word.get(i, '-') for i in decoder_Y[0]])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["decoder_X: [1 5 9 2 3 7 2 4 6 8 8]\n","Mapped back: ['<START>', '2', '3', '.', '0', '8', '.', '1', '9', '7', '7']\n","decoder_Y: [5 9 2 3 7 2 4 6 8 8 0]\n","Mapped back: ['2', '3', '.', '0', '8', '.', '1', '9', '7', '7', '-']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TQfVE_crN5F3","colab_type":"text"},"source":["## Build model\n","\n","Note the following:\n","\n","* `mask_zero=True` for the embedding makes the model ignore padding (see [Masking and padding with Keras](https://www.tensorflow.org/guide/keras/masking_and_padding))\n","* `return_state=True` for the encoder RNN returns the state of the last timestep in addition to output (see https://keras.io/layers/recurrent/).\n","* As we're using an LSTM, we get two separate state values, _h_ and _c_ (see below)\n","* `return_sequences=True` for the decoder RNN returns the output from each time step, not only the last\n","\n","<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" style=\"width: 50%\">\n","\n","(LSTM illustration from https://colah.github.io/posts/2015-08-Understanding-LSTMs/ )\n"]},{"cell_type":"code","metadata":{"id":"tW-dryikN5F6","colab_type":"code","outputId":"8da5f6e0-0407-4ecb-9e3d-b0508b4cf768","colab":{"base_uri":"https://localhost:8080/","height":520},"executionInfo":{"status":"ok","timestamp":1586426819013,"user_tz":-180,"elapsed":2488,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["from tensorflow.keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense\n","from tensorflow.keras.models import Model, load_model\n","\n","\n","def build_seq2seq_model(input_length, output_length,\n","                        input_vocab_size, output_vocab_size,\n","                        embedding_dim=EMBEDDING_DIM, rnn_units=RNN_UNITS):\n","    encoder_input = Input(shape=(input_length,))\n","    encoder_embedding = Embedding(input_vocab_size, embedding_dim, mask_zero=True)(encoder_input)\n","    encoder_output, encoder_h, encoder_c = LSTM(\n","        rnn_units,\n","        return_sequences=False,\n","        return_state=True # LSTM:n h ja c, encoder-decoderin c\n","    )(encoder_embedding)\n","    encoder_states = [encoder_h, encoder_c]\n","\n","    print(encoder_h.shape)\n","    print(encoder_c.shape)\n","\n","    decoder_input = Input(shape=(output_length,))\n","    decoder_embedding = Embedding(output_vocab_size, embedding_dim, mask_zero=True)(decoder_input)\n","    decoder_rnn = LSTM(rnn_units, return_sequences=True)(\n","        decoder_embedding,\n","        initial_state=encoder_states\n","    )\n","    decoder_output = TimeDistributed(Dense(output_vocab_size, activation=\"softmax\"))(decoder_rnn)\n","\n","    model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n","    return model\n","\n","\n","model = build_seq2seq_model(\n","    input_length=INPUT_LENGTH,\n","    output_length=OUTPUT_LENGTH,\n","    input_vocab_size=INPUT_VOCAB_SIZE,\n","    output_vocab_size=OUTPUT_VOCAB_SIZE\n",")\n","\n","print(model.summary())"],"execution_count":9,"outputs":[{"output_type":"stream","text":["(None, 100)\n","(None, 100)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 35)]         0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 11)]         0                                            \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 35, 100)      3200        input_3[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_3 (Embedding)         (None, 11, 100)      1300        input_4[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   [(None, 100), (None, 80400       embedding_2[0][0]                \n","__________________________________________________________________________________________________\n","lstm_3 (LSTM)                   (None, 11, 100)      80400       embedding_3[0][0]                \n","                                                                 lstm_2[0][1]                     \n","                                                                 lstm_2[0][2]                     \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 11, 13)       1313        lstm_3[0][0]                     \n","==================================================================================================\n","Total params: 166,613\n","Trainable params: 166,613\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YuFAm0KlN5GC","colab_type":"text"},"source":["## Training\n","\n","As our output values here are (as usual) integer values standing for our vocabulary items (characters), we'll use `sparse_categorical_crossentropy` loss."]},{"cell_type":"code","metadata":{"id":"lw859ks2N5GD","colab_type":"code","colab":{}},"source":["model.compile(\n","    optimizer='adam',\n","    loss='sparse_categorical_crossentropy'\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TL5H3XWKN5GK","colab_type":"code","outputId":"3b3c552d-9d9c-400c-e112-6ceaa1ace01b","colab":{"base_uri":"https://localhost:8080/","height":129},"executionInfo":{"status":"ok","timestamp":1586428324419,"user_tz":-180,"elapsed":304112,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["history = model.fit(\n","    x=[encoder_X, decoder_X], # input syötteenä edellinen tila h\n","    y=[decoder_Y],\n","    batch_size=BATCH_SIZE,\n","    epochs=EPOCHS,\n","    validation_split=0.2\n",")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","1250/1250 [==============================] - 101s 80ms/step - loss: 0.5450 - val_loss: 0.1158\n","Epoch 2/3\n","1250/1250 [==============================] - 99s 79ms/step - loss: 0.0343 - val_loss: 0.0077\n","Epoch 3/3\n","1250/1250 [==============================] - 99s 79ms/step - loss: 0.0067 - val_loss: 0.0017\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xvw7JMMgN5GS","colab_type":"text"},"source":["Plot the training history:"]},{"cell_type":"code","metadata":{"id":"OOk9ujsTN5GU","colab_type":"code","outputId":"265be80f-71c3-4733-e3fa-672a3aa3461d","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1586428765760,"user_tz":-180,"elapsed":715,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","\n","def plot_history(history):\n","    plt.plot(history.history['loss'],label=\"Training set loss\")\n","    plt.plot(history.history['val_loss'],label=\"Validation set loss\")\n","    plt.legend()\n","    plt.show()\n","\n","\n","plot_history(history)"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUkr5CE3oSinlIzShK6WUj/j/uJo4m85Et6MAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"jCScYnrWN5Gi","colab_type":"text"},"source":["Seems to no longer be improving."]},{"cell_type":"markdown","metadata":{"id":"gg-nlNQwN5Gi","colab_type":"text"},"source":["## Prediction\n","\n","For prediction, we'll vectorize the given text normally, and initialize the decoder inputs to a vector with an initial start symbol and zeros otherwise.\n","\n","We'll then predict outputs, take the first new predicted character, place that back into the decoder inputs, and iterate."]},{"cell_type":"code","metadata":{"id":"9R2CXLjIN5Gk","colab_type":"code","colab":{}},"source":["def predict(text):\n","    encoder_X = vectorize([text], input_tokenizer, INPUT_LENGTH, quiet=True)\n","    decoder_X = np.zeros(shape=(1, OUTPUT_LENGTH))\n","    decoder_X[0,0] = START_INDEX\n","    predictions = []\n","    # ennusta aikaisempien merkkien perusteella seuraava\n","    for i in range(1, OUTPUT_LENGTH): # start symbol in position 0\n","        prediction = model.predict([encoder_X, decoder_X])[0][i].argmax() # [0][i].argmax() poimi output, jolla suurin tn. (softmax)\n","        predictions.append(prediction)\n","        decoder_X[0,i] = prediction\n","    pred_chars = []\n","    for i in predictions:\n","        if i == 0:\n","            break\n","        pred_chars += output_tokenizer.index_word[i]\n","    return ''.join(pred_chars)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZT5UombVN5Gr","colab_type":"text"},"source":["Test with a few cases:"]},{"cell_type":"code","metadata":{"id":"Z4Dq3ioaN5Gs","colab_type":"code","outputId":"e03dbb32-8b53-4d07-c669-0745a47e77e0","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1586428776201,"user_tz":-180,"elapsed":3692,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["test_inputs = ['1. helmikuuta 2003', 'Toukokuun 7. päivä 1995', '9. päivä huhtikuuta 2020']\n","for text in test_inputs:\n","    print(text, '→', predict(text))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["1. helmikuuta 2003 → 01.02.2003\n","Toukokuun 7. päivä 1995 → 07.05.1995\n","9. päivä huhtikuuta 2020 → 09.04.2020\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W-oRn080N5G1","colab_type":"text"},"source":["The coverage of the training data is not comprehensive:"]},{"cell_type":"code","metadata":{"id":"25fdyXZzN5G2","colab_type":"code","outputId":"63d5cafb-7bd7-44e6-a905-dbcfd4cdfc71","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1586428900161,"user_tz":-180,"elapsed":1305,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["test_inputs = ['1. tammikuuta vuonna 800', 'vuoden 2020 ensimmäinen päivä']\n","for text in test_inputs:\n","    print(text, '→', predict(text))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["1. tammikuuta vuonna 800 → 01.05.2000\n","vuoden 2020 ensimmäinen päivä → 09.06.2004\n"],"name":"stdout"}]}]}