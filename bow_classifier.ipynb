{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words document classification\n",
    "\n",
    "BoW is the simplest way to do classification: Feature vector goes in, decision falls out.\n",
    "\n",
    "Feature vector: a vector with as many dimensions as we have unique features, and a non-zero value set for every feature present in our example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'This movie has a lot of comedy, not dark and Gordon Liu shines in this one. He displays his comical side and it was really weird seeing him get beat up. His training is \\\\unorthodox\\\\\" and who would\\'ve thought knot tying could be so deadly?? Lots of great stunts and choreography. Very creative!  Add Johnny Wang in the mix and you\\'ve got an awesome final showdown! Don\\'t mess with Manchu thugs; they\\'re ruthless!\"', 'class': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"data/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) #play it safe!\n",
    "print(data[0]) #Every item is a dictionary with `text` and `class` keys, here's one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn this data, we will need a few steps:\n",
    "\n",
    "* Build a data matrix with dimensionality (number of examples, number of possible features)\n",
    "* Build a vector (number of examples,) with the correct labels for the examples\n",
    "\n",
    "It is quite useless to do all this ourselves, so we will use ready-made classes and functions mostly from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This movie has a lot of comedy, not dark and Gordon Liu shines in this one. He displays his comical side and it was really weird seeing him get beat up. His training is \\\\unorthodox\\\\\" and who would\\'ve thought knot tying could be so deadly?? Lots of great stunts and choreography. Very creative!  Add Johnny Wang in the mix and you\\'ve got an awesome final showdown! Don\\'t mess with Manchu thugs; they\\'re ruthless!\"', \"This mini series, also based on a book by Alex Haley as was `Queen', tried to use similar formulas, that is, constructing a long history following the lives of a family over many years. Whereas in `Queen' the result was masterful, here in Mama Flora the inspiration was lacking. Firstly perhaps in the book itself, and most certainly in this TV production. Too much is put in with too much haste over the years, such that the unfolding saga is shallow, superficial, not nearly so authentic as in `Queen'. Full marks for the scenification in the earlier parts of the film, which was prepared with great care, but as the film progressed it seemed to degenerate into a kind of dallasian-forsythian unpalatable mix in the last third of its three hours or so duration. I had hoped for more; but evidently Haley was less inspired with this tale than his near-biographical `Queen', and Peter Werner III is no match for John Erman. Only recommendable for those who have an appetite for these lengthy tales of generations growing up.\"]\n",
      "['pos', 'neg']\n"
     ]
    }
   ],
   "source": [
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape= (25000, 100000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,2))\n",
    "feature_matrix=vectorizer.fit_transform(texts)\n",
    "print(\"shape=\",feature_matrix.shape)\n",
    "#print(feature_matrix.todense())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature matrix done! Next thing we need is the class labels to be predicted in one-hot encoding. This means:\n",
    "\n",
    "* one row for every example\n",
    "* one column for every possible class label\n",
    "* exactly one column has 1 for every example, corresponding to the desired class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_numbers shape= (25000,)\n",
      "class_numbers [1 0 1 ... 0 0 0]\n",
      "class labels ['neg' 'pos']\n",
      "classes_1hot [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "one_hot_encoder=OneHotEncoder(sparse=False) #Turns class integers into one-hot encoding\n",
    "class_numbers=label_encoder.fit_transform(labels)\n",
    "print(\"class_numbers shape=\",class_numbers.shape)\n",
    "print(\"class_numbers\",class_numbers)\n",
    "print(\"class labels\",label_encoder.classes_)\n",
    "#And now yet the one-hot encoding\n",
    "classes_1hot=one_hot_encoder.fit_transform(class_numbers.reshape(-1,1))\n",
    "print(\"classes_1hot\",classes_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/20\n",
      "22500/22500 [==============================] - 100s 4ms/step - loss: 0.5417 - acc: 0.7896 - val_loss: 0.4432 - val_acc: 0.8452\n",
      "Epoch 2/20\n",
      "22500/22500 [==============================] - 98s 4ms/step - loss: 0.3962 - acc: 0.8575 - val_loss: 0.3698 - val_acc: 0.8608\n",
      "Epoch 3/20\n",
      "22500/22500 [==============================] - 97s 4ms/step - loss: 0.3336 - acc: 0.8791 - val_loss: 0.3356 - val_acc: 0.8656\n",
      "Epoch 4/20\n",
      "22500/22500 [==============================] - 92s 4ms/step - loss: 0.2947 - acc: 0.8928 - val_loss: 0.3097 - val_acc: 0.8732\n",
      "Epoch 5/20\n",
      "22500/22500 [==============================] - 94s 4ms/step - loss: 0.2658 - acc: 0.9049 - val_loss: 0.2948 - val_acc: 0.8760\n",
      "Epoch 6/20\n",
      "22500/22500 [==============================] - 96s 4ms/step - loss: 0.2424 - acc: 0.9143 - val_loss: 0.2840 - val_acc: 0.8800\n",
      "Epoch 7/20\n",
      "22500/22500 [==============================] - 123s 5ms/step - loss: 0.2229 - acc: 0.9225 - val_loss: 0.2787 - val_acc: 0.8852\n",
      "Epoch 8/20\n",
      "22500/22500 [==============================] - 121s 5ms/step - loss: 0.2058 - acc: 0.9304 - val_loss: 0.2710 - val_acc: 0.8884\n",
      "Epoch 9/20\n",
      "22500/22500 [==============================] - 118s 5ms/step - loss: 0.1905 - acc: 0.9391 - val_loss: 0.2678 - val_acc: 0.8896\n",
      "Epoch 10/20\n",
      "22500/22500 [==============================] - 119s 5ms/step - loss: 0.1769 - acc: 0.9457 - val_loss: 0.2626 - val_acc: 0.8904\n",
      "Epoch 11/20\n",
      "22500/22500 [==============================] - 117s 5ms/step - loss: 0.1646 - acc: 0.9516 - val_loss: 0.2599 - val_acc: 0.8912\n",
      "Epoch 12/20\n",
      "22500/22500 [==============================] - 117s 5ms/step - loss: 0.1532 - acc: 0.9559 - val_loss: 0.2584 - val_acc: 0.8932\n",
      "Epoch 13/20\n",
      "22500/22500 [==============================] - 119s 5ms/step - loss: 0.1430 - acc: 0.9607 - val_loss: 0.2558 - val_acc: 0.8920\n",
      "Epoch 14/20\n",
      "22500/22500 [==============================] - 101s 4ms/step - loss: 0.1335 - acc: 0.9648 - val_loss: 0.2548 - val_acc: 0.8928\n",
      "Epoch 15/20\n",
      "22500/22500 [==============================] - 90s 4ms/step - loss: 0.1248 - acc: 0.9690 - val_loss: 0.2544 - val_acc: 0.8940\n",
      "Epoch 16/20\n",
      "22500/22500 [==============================] - 90s 4ms/step - loss: 0.1167 - acc: 0.9723 - val_loss: 0.2558 - val_acc: 0.8964\n",
      "Epoch 17/20\n",
      "22500/22500 [==============================] - 91s 4ms/step - loss: 0.1093 - acc: 0.9753 - val_loss: 0.2538 - val_acc: 0.8952\n",
      "Epoch 18/20\n",
      "22500/22500 [==============================] - 90s 4ms/step - loss: 0.1025 - acc: 0.9775 - val_loss: 0.2540 - val_acc: 0.8964\n",
      "Epoch 19/20\n",
      "22500/22500 [==============================] - 90s 4ms/step - loss: 0.0961 - acc: 0.9805 - val_loss: 0.2542 - val_acc: 0.8964\n",
      "Epoch 20/20\n",
      "22500/22500 [==============================] - 90s 4ms/step - loss: 0.0903 - acc: 0.9828 - val_loss: 0.2555 - val_acc: 0.8984\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "example_count,feature_count=feature_matrix.shape\n",
    "example_count,class_count=classes_1hot.shape\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(200,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "hist=model.fit(feature_matrix,classes_1hot,batch_size=100,verbose=1,epochs=20,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8451999950408936, 0.8608000111579895, 0.8656000018119812, 0.8732000017166137, 0.8759999990463256, 0.88, 0.8851999974250794, 0.888400001525879, 0.8896000027656555, 0.8904000043869018, 0.8912000036239625, 0.8932000064849853, 0.892000002861023, 0.8928000068664551, 0.8940000057220459, 0.8964000034332276, 0.8952000069618226, 0.8964000058174133, 0.8964000058174133, 0.8984000039100647]\n"
     ]
    }
   ],
   "source": [
    "print(hist.history[\"val_acc\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
