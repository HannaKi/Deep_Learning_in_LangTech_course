{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "bow_classifier.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/Deep_Learning_in_LangTech_course/blob/master/bow_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e385K-r6UkT0",
        "colab_type": "text"
      },
      "source": [
        "# NN\n",
        "\n",
        "* We will cover the essentials of NN during the lecture\n",
        "* Those absent, try eg. [this tutorial](https://www.cs.toronto.edu/~jlucas/teaching/csc411/lectures/tut5_handout.pdf)\n",
        "* For everyone interested, [here](https://gombru.github.io/2018/05/23/cross_entropy_loss/) is some reading about the loss functions\n",
        "* And [here](https://github.com/Jaewan-Yun/optimizer-visualization) is the visualization of different optimizers I showed\n",
        "* We also walked through several optimization techniques, if you missed the lecture on that, I suggest you watch these: [Momentum](https://www.youtube.com/watch?v=N18Km9YIIug) [RMSProp](https://www.youtube.com/watch?v=XhZahXzEuNo) and [Adam](https://www.youtube.com/watch?v=JXQT_vxqwIs). These videos are by Hinton and Ng, who are trustable sources. The slides are [here](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf). Further reading (from a trustable source) is also to be found [here](https://ruder.io/optimizing-gradient-descent/index.html#adagrad). The [Adam paper](https://arxiv.org/pdf/1412.6980.pdf) is for those especially interested in the topic.\n",
        "\n",
        "# More deeply about optimization\n",
        "\n",
        "* Adam combines SGD and Momentum\n",
        "\n",
        "* SGD:\n",
        "  * w(t+1) = w(t) - epsilon(gradient)\n",
        "    * epsilon = learning rate\n",
        "    * gradient = derivative of the loss at point wi\n",
        "  * small learning rate: optimization might get stuck in local minimum\n",
        "  * too large learning rate: expensive zig-zag, takes long time\n",
        "  * massive large learning rate: will diverge, not able to descent to minimum\n",
        "* Momentum:\n",
        "  * sama kuin fysiikassa, liikkeen muuttumiselle on vastavoima: optimization function can not make jyrkkä changes: less zig-zagging, faster divergence to minimum\n",
        "  * V(t+1) = aV(t) + epsilon(gradient)\n",
        "    * tämä on vektori, jolla on suunta ja nopeus\n",
        "  * a < 1, jarruttava tekijä, jotta opitimointi pysähtyy\n",
        "* If gradient (derivative of loss function) is big, it would be beneficial to have small learning rate and vice versa\n",
        "* Problem: same learning rate for all the weights! \n",
        "  * Solution: learning rate needs to be adjusted for all the weights: running average of gragients with respece to time and weight:\n",
        "  * G(t+1,wi) = bG(t,wi)- epsilon(gradient^2)\n",
        "  * b < 1\n",
        "  * technical problem: we need to store the adjusted learning rates of every weight somewhere: memory consumption is doubled if you use for example Adam!\n",
        "\n",
        "  * Setting the learning rate is most important parameter to tune in Adam. If loss is acting wierdly do hyperparameter optimization (exponential grid!)\n",
        "\n",
        "# Bag-of-words document classification\n",
        "\n",
        "* BoW is the simplest way to do classification: Feature vector goes in, decision falls out.\n",
        "\n",
        "* Feature vector: a vector with as many dimensions as we have unique features, and a non-zero value set for every feature present in our example\n",
        "* Binary features: 1/0\n",
        "\n",
        "In the following we work with the IMDB data, have a look on [how to read it in](read_imdb.ipynb). Here we just read the ready data in.\n",
        "\n",
        "# IMDB data\n",
        "\n",
        "* Movie review sentiment positive/negative\n",
        "* Some 25,000 examples, 50:50 split\n",
        "* Current state-of-the-art is about 95% accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWfVke4sPYSw",
        "colab_type": "code",
        "outputId": "e241c4f4-a8d7-4a27-905a-5ef71c8e32eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "# to run with old tf with which the code was made\n",
        "# The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOt1VsNYUkT5",
        "colab_type": "code",
        "outputId": "c63723ca-434b-4c0b-dc55-cffe1567a7a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/imdb_train.json"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-06 13:26:07--  https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/imdb_train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33944099 (32M) [text/plain]\n",
            "Saving to: ‘imdb_train.json.1’\n",
            "\n",
            "\rimdb_train.json.1     0%[                    ]       0  --.-KB/s               \rimdb_train.json.1   100%[===================>]  32.37M   165MB/s    in 0.2s    \n",
            "\n",
            "2020-04-06 13:26:07 (165 MB/s) - ‘imdb_train.json.1’ saved [33944099/33944099]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Uxnn06MUkUM",
        "colab_type": "code",
        "outputId": "ae211eaa-5f9c-49ae-8749-6f0e7f744640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import json\n",
        "import random\n",
        "with open(\"imdb_train.json\") as f:\n",
        "    data=json.load(f)\n",
        "random.shuffle(data) #play it safe!\n",
        "# # Tärkeää, koska järjestys vaikuttaa malleihin! --> jos luokka 1 ekana, \n",
        "# mallia opetetaan aluksi sillä, eikä tasaisesti kaikilla jne.\n",
        "print(data[0]) #Every item is a dictionary with `text` and `class` keys, here's the first one:"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'class': 'neg', 'text': \"......in a horror movie that is. Alright first off , lets start with Kate. Her main goals include getting laid by George Clooney, looking good and last but not least screwing everyone over. Gotta love her. She had about 3 amazingly good chances to finish off this sicko but ..... instead she ran. I mean she didn't wanna bring Guy out for 10 minutes and when she did it was too late. I mean the guy tried to rape her. I cant get into these movies where the main character is a sad idiot. I mean who honestly would have any sympathy for a guy who finishes off everyone she has meet in a night. The movie kept going on. And as a result lost all its credibility.\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k32onsrUkUX",
        "colab_type": "text"
      },
      "source": [
        "To learn on this data, we will need a few steps:\n",
        "\n",
        "* Build a data matrix with dimensionality (number of examples, number of possible features), and a value for each feature, 0/1 for binary features\n",
        "* Build a class label matrix (number of examples, number of classes) with the correct labels for the examples, setting 1 for the correct class, and 0 for others\n",
        "\n",
        "It is quite useless to do all this ourselves, so we will use ready-made classes and functions mostly from scikit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwdY1IEiUkUZ",
        "colab_type": "code",
        "outputId": "aa4cac3b-3b5d-4058-bcba-17882cca5701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# We need to gather the texts, into a list\n",
        "texts=[one_example[\"text\"] for one_example in data] # features\n",
        "labels=[one_example[\"class\"] for one_example in data]\n",
        "print(texts[:2])\n",
        "print(labels[:2])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"......in a horror movie that is. Alright first off , lets start with Kate. Her main goals include getting laid by George Clooney, looking good and last but not least screwing everyone over. Gotta love her. She had about 3 amazingly good chances to finish off this sicko but ..... instead she ran. I mean she didn't wanna bring Guy out for 10 minutes and when she did it was too late. I mean the guy tried to rape her. I cant get into these movies where the main character is a sad idiot. I mean who honestly would have any sympathy for a guy who finishes off everyone she has meet in a night. The movie kept going on. And as a result lost all its credibility.\", 'Some moron who read or saw some reference to angels coming to Earth, decided to disregard what he\\'d heard about the offspring of humans and angels being larger than normal humans. Reinventing them as mythical giants that were 40 feet tall, is beyond ridiculous. There was some historical references to housing and furniture in parts of the world, that were much larger than would be needed for standard humans. These were supposedly built on a scale that would lend itself to a 10 to 14 foot human, somewhat supporting the \\\\David and Goliath\\\\\" tale from the bible. There is no mention in any historical references to buildings or artifacts that would support the idea of a 40 foot tall being. If I was rating this movie on my own scale, it would have been a negative value instead of a one...\"']\n",
            "['neg', 'neg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHHFtcwFUkUl",
        "colab_type": "code",
        "outputId": "8d42253d-30e3-4fbc-c627-5159127b5b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# prepare data for bag of words: sanat featureiksi, jotka on vektoreissa\n",
        "# Kaikkien tekstien sanat poimitaan erikseen ja sille annetaan arvo vektoriin esiintyyko\n",
        "# se tekstissa. Tsekkaa CountVectorizer:in dokumentointi \n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\n",
        "feature_matrix=vectorizer.fit_transform(texts)\n",
        "print(\"shape=\",feature_matrix.shape)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape= (25000, 74849)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ooa1s99raMDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(feature_matrix[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjx9gQSkUkUu",
        "colab_type": "text"
      },
      "source": [
        "Now we have the feature matrix done! Next thing we need is the class labels to be predicted in one-hot encoding. This means:\n",
        "\n",
        "* one row for every example\n",
        "* one column for every possible class label\n",
        "* exactly one column has 1 for every example, corresponding to the desired class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdwhkJndUkUx",
        "colab_type": "code",
        "outputId": "f65db0c5-e358-46b3-d26e-8bff29aece38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder=LabelEncoder() #Turns class labels into integers\n",
        "class_numbers=label_encoder.fit_transform(labels)\n",
        "\n",
        "print(\"class_numbers shape=\",class_numbers.shape)\n",
        "print(\"class labels\",label_encoder.classes_) #this will let us translate back from indices to labels"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class_numbers shape= (25000,)\n",
            "class labels ['neg' 'pos']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD4M2-LZUkU7",
        "colab_type": "text"
      },
      "source": [
        "* The data is ready, we need to build the network now\n",
        "* Input\n",
        "* Hidden Dense layer with some kind of non-linearity, and a suitable number of nodes\n",
        "* Output Dense layer with the softmax activation (normalizes output to distribution) and as many nodes as there are classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb_c6_zvUkU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import keras\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Input, Dense\n",
        "\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input, Dense\n",
        "\n",
        "example_count,feature_count=feature_matrix.shape\n",
        "class_count=len(label_encoder.classes_)\n",
        "\n",
        "inp=Input(shape=(feature_count,)) # tuple\n",
        "hidden=Dense(200,activation=\"tanh\")(inp) # taalla kaytetty tanh. Relu suositumpi? \n",
        "# # Jos mitaan funktiota ei anneta, tulee syotteen ja kertoimien lineaarinen matriisitulo \n",
        "outp=Dense(class_count,activation=\"softmax\")(hidden) # softmax: tuottaa luokkien jakauman\n",
        "model=Model(inputs=[inp], outputs=[outp])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKP-DO9nVr36",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb4163a4-b3f3-46c9-8a77-4885c9f67590"
      },
      "source": [
        "model # mallin \"resepti\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.training.Model at 0x7f701a42bc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc472hx7UkVI",
        "colab_type": "text"
      },
      "source": [
        "...it's **this** simple...!\n",
        "\n",
        "Once the model is constructed it needs to be compiled, for that we need to know:\n",
        "* which optimizer we want to use (sgd is fine to begin with)\n",
        "* what is the loss (categorial_crossentropy for multiclass of the kind we have is the right choice)\n",
        "* which metrics to measure, accuracy is an okay choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FzSDEhQVzC3",
        "colab_type": "text"
      },
      "source": [
        "* Optimaizer = algoritmi, joka etsii minimia haittafunktiosta\n",
        "* Loss (multiclass classification): cross entropy: oikeiden ja ennustettujen jakaumien vertailu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKJVRgPuUkVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efSRBEVOUkVT",
        "colab_type": "text"
      },
      "source": [
        "A compiled model can be fitted on data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiIXq-PvUkVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch_size kuinka monta inputtia kerralla sisaan. jokaisen batchin jalkeen paivitetaan painokertoimet gradientien keskiarvolla\n",
        "# epochs kuinka monta kertaa mennaan lapi koko data\n",
        "# validation_split: kuinka paljon dataa kaytetaan accuracyn laskemiseen\n",
        "\n",
        "hist=model.fit(feature_matrix,class_numbers,batch_size=100,verbose=1,epochs=5,validation_split=0.1)\n",
        "\n",
        "# tulosteessa:\n",
        "# Malli putkauttaa ulos: loss\n",
        "# accuracy: training data accuracy\n",
        "# val_acc: accuracy with new data\n",
        "# jos näyttää sille, että mallin oppiminen paranisi vaikka malli on jo treenattu (val_acc kehittyy paremmaksi), lisaa epocheja\n",
        "# jos val_acc alkaa laskea epochista toiseen, malli alkaa ylifittaamaan (overfitting)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKjhx8H5UkVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(hist.history[\"val_acc\"]) # val_accuracy vanhassa Keras-versiosa. Tarkista tämä jos koodi kaatuu."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiGKwEo8UkVn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* We ran for 10 epochs of training\n",
        "* Made it to a decent accuracy on the validation data\n",
        "\n",
        "* But we do not have the model saved, so let's fix that and get the whole thing done\n",
        "* What constitutes a model (ie what we need to run the model on new data)\n",
        "  - The feature dictionary in the vectorizer\n",
        "  - The list of classes in their correct order\n",
        "  - The structure of the network\n",
        "  - The weights the network learned\n",
        "\n",
        "* Do all these things, and run again. This time we also increase the number of epochs to 100, see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-we56vZhUkVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mallin tallentaminen\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "def save_model(file_name,model,label_encoder,vectorizer):\n",
        "    \"\"\"Saves model structure and vocabularies\"\"\"\n",
        "    model_json = model.to_json() # mallin rakenne ilman painoja\n",
        "    with open(file_name+\".model.json\", \"w\") as f:\n",
        "        print(model_json,file=f)\n",
        "    with open(file_name+\".encoders.pickle\",\"wb\") as f: # pickle vaatii avaamista binaarimuodosa\n",
        "        pickle.dump((label_encoder,vectorizer),f)\n",
        "        \n",
        "example_count,feature_count=feature_matrix.shape\n",
        "class_count=len(label_encoder.classes_)\n",
        "\n",
        "inp=Input(shape=(feature_count,))\n",
        "hidden=Dense(200,activation=\"tanh\")(inp)\n",
        "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
        "model=Model(inputs=[inp], outputs=[outp])\n",
        "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
        "\n",
        "# Save model and vocabularies, can be done before training\n",
        "save_model(\"models/imdb_bow\",model,label_encoder,vectorizer)\n",
        "# Callback function to save weights during training, if validation loss goes down\n",
        "save_cb=ModelCheckpoint(filepath=\"models/imdb_bow.weights.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "# Callback to stop training when no improvement\n",
        "stop_cb=EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "hist=model.fit(feature_matrix,class_numbers,batch_size=100,verbose=1,epochs=100,validation_split=0.1,callbacks=[save_cb,stop_cb])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__JcOdNVUkVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#Validation data used during training:\n",
        "val_instances,val_labels,_=hist.validation_data\n",
        "\n",
        "# jakaumat\n",
        "print(\"Network output=\",model.predict(val_instances))\n",
        "predictions=numpy.argmax(model.predict(val_instances),axis=1)\n",
        "print(\"Maximum class for each example=\",predictions)\n",
        "conf_matrix=confusion_matrix(list(val_labels),list(predictions))\n",
        "print(\"Confusion matrix=\\n\",conf_matrix)\n",
        "gold_labels=label_encoder.inverse_transform(list(val_labels))\n",
        "predicted_labels=label_encoder.inverse_transform(list(predictions))\n",
        "print(classification_report(gold_labels,predicted_labels))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g97xemuLUkV6",
        "colab_type": "text"
      },
      "source": [
        "# Learning progress\n",
        "\n",
        "* The history object we get lets us inspect the accuracy during training\n",
        "* Remarks:\n",
        "  - Accuracy on training data keeps going up\n",
        "  - Accuracy on validation (test) data flattens out after a but over 10 epochs, we are learning very little past that point\n",
        "  - What we see is the network keeps overfitting on the training data to the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjbNP6jJUkV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.ylim(0.85,1.0)\n",
        "plt.plot(hist.history[\"val_accuracy\"],label=\"Validation set accuracy\")\n",
        "plt.plot(hist.history[\"accuracy\"],label=\"Training set accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faD4RxrxUkWD",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "* We put together a program to train a neural network classifier for sentiment detector\n",
        "* We learned the necessary code/techniques to save models, and feed the training with data in just the right format\n",
        "* We observed the training across epochs\n",
        "* We saw how the classifier can be applied to various text classification problems\n",
        "* The IMDB sentiment classifier ended up at nearly 90% accuracy, the state of the art is about 95%, we got surprisingly far in few lines of code\n"
      ]
    }
  ]
}