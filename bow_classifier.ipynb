{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words document classification\n",
    "\n",
    "BoW is the simplest way to do classification: Feature vector goes in, decision falls out.\n",
    "\n",
    "Feature vector: a vector with as many dimensions as we have unique features, and a non-zero value set for every feature present in our example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'This movie has a lot of comedy, not dark and Gordon Liu shines in this one. He displays his comical side and it was really weird seeing him get beat up. His training is \\\\unorthodox\\\\\" and who would\\'ve thought knot tying could be so deadly?? Lots of great stunts and choreography. Very creative!  Add Johnny Wang in the mix and you\\'ve got an awesome final showdown! Don\\'t mess with Manchu thugs; they\\'re ruthless!\"', 'class': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"data/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) #play it safe!\n",
    "print(data[0]) #Every item is a dictionary with `text` and `class` keys, here's one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn this data, we will need a few steps:\n",
    "\n",
    "* Build a data matrix with dimensionality (number of examples, number of possible features)\n",
    "* Build a vector (number of examples,) with the correct labels for the examples\n",
    "\n",
    "It is quite useless to do all this ourselves, so we will use ready-made classes and functions mostly from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This movie has a lot of comedy, not dark and Gordon Liu shines in this one. He displays his comical side and it was really weird seeing him get beat up. His training is \\\\unorthodox\\\\\" and who would\\'ve thought knot tying could be so deadly?? Lots of great stunts and choreography. Very creative!  Add Johnny Wang in the mix and you\\'ve got an awesome final showdown! Don\\'t mess with Manchu thugs; they\\'re ruthless!\"', \"This mini series, also based on a book by Alex Haley as was `Queen', tried to use similar formulas, that is, constructing a long history following the lives of a family over many years. Whereas in `Queen' the result was masterful, here in Mama Flora the inspiration was lacking. Firstly perhaps in the book itself, and most certainly in this TV production. Too much is put in with too much haste over the years, such that the unfolding saga is shallow, superficial, not nearly so authentic as in `Queen'. Full marks for the scenification in the earlier parts of the film, which was prepared with great care, but as the film progressed it seemed to degenerate into a kind of dallasian-forsythian unpalatable mix in the last third of its three hours or so duration. I had hoped for more; but evidently Haley was less inspired with this tale than his near-biographical `Queen', and Peter Werner III is no match for John Erman. Only recommendable for those who have an appetite for these lengthy tales of generations growing up.\"]\n",
      "['pos', 'neg']\n"
     ]
    }
   ],
   "source": [
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape= (25000, 100000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,2))\n",
    "feature_matrix=vectorizer.fit_transform(texts)\n",
    "print(\"shape=\",feature_matrix.shape)\n",
    "#print(feature_matrix.todense())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature matrix done! Next thing we need is the class labels to be predicted in one-hot encoding. This means:\n",
    "\n",
    "* one row for every example\n",
    "* one column for every possible class label\n",
    "* exactly one column has 1 for every example, corresponding to the desired class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_numbers shape= (25000,)\n",
      "class_numbers [1 0 1 ... 0 0 0]\n",
      "class labels ['neg' 'pos']\n",
      "classes_1hot [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "one_hot_encoder=OneHotEncoder(sparse=False) #Turns class integers into one-hot encoding\n",
    "class_numbers=label_encoder.fit_transform(labels)\n",
    "print(\"class_numbers shape=\",class_numbers.shape)\n",
    "print(\"class_numbers\",class_numbers)\n",
    "print(\"class labels\",label_encoder.classes_)\n",
    "#And now yet the one-hot encoding\n",
    "classes_1hot=one_hot_encoder.fit_transform(class_numbers.reshape(-1,1))\n",
    "print(\"classes_1hot\",classes_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 93s 5ms/step - loss: 0.5521 - acc: 0.7808 - val_loss: 0.4617 - val_acc: 0.8444\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 88s 4ms/step - loss: 0.4077 - acc: 0.8566 - val_loss: 0.3859 - val_acc: 0.8594\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 94s 5ms/step - loss: 0.3429 - acc: 0.8775 - val_loss: 0.3477 - val_acc: 0.8678\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 89s 4ms/step - loss: 0.3031 - acc: 0.8907 - val_loss: 0.3252 - val_acc: 0.8732\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 92s 5ms/step - loss: 0.2732 - acc: 0.9020 - val_loss: 0.3087 - val_acc: 0.8774\n",
      "Epoch 6/10\n",
      " 9800/20000 [=============>................] - ETA: 44s - loss: 0.2496 - acc: 0.9130"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "example_count,feature_count=feature_matrix.shape\n",
    "example_count,class_count=classes_1hot.shape\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(200,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "hist=model.fit(feature_matrix,classes_1hot,batch_size=100,verbose=1,epochs=10,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.history[\"val_acc\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
