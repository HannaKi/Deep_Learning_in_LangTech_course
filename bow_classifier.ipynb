{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words document classification\n",
    "\n",
    "BoW is the simplest way to do classification: Feature vector goes in, decision falls out.\n",
    "\n",
    "Feature vector: a vector with as many dimensions as we have unique features, and a non-zero value set for every feature present in our example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"data/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) #play it safe!\n",
    "print(data[0]) #Every item is a dictionary with `text` and `class` keys, here's one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn this data, we will need a few steps:\n",
    "\n",
    "* Build a data matrix with dimensionality (number of examples, number of possible features)\n",
    "* Build a vector (number of examples,) with the correct labels for the examples\n",
    "\n",
    "It is quite useless to do all this ourselves, so we will use ready-made classes and functions mostly from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,2))\n",
    "feature_matrix=vectorizer.fit_transform(texts)\n",
    "print(\"shape=\",feature_matrix.shape)\n",
    "#print(feature_matrix.todense())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature matrix done! Next thing we need is the class labels to be predicted in one-hot encoding. This means:\n",
    "\n",
    "* one row for every example\n",
    "* one column for every possible class label\n",
    "* exactly one column has 1 for every example, corresponding to the desired class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "one_hot_encoder=OneHotEncoder(sparse=False) #Turns class integers into one-hot encoding\n",
    "class_numbers=label_encoder.fit_transform(labels)\n",
    "print(\"class_numbers shape=\",class_numbers.shape)\n",
    "print(\"class_numbers\",class_numbers)\n",
    "print(\"class labels\",label_encoder.classes_)\n",
    "#And now yet the one-hot encoding\n",
    "classes_1hot=one_hot_encoder.fit_transform(class_numbers.reshape(-1,1))\n",
    "print(\"classes_1hot\",classes_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "example_count,feature_count=feature_matrix.shape\n",
    "example_count,class_count=classes_1hot.shape\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(200,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "hist=model.fit(feature_matrix,classes_1hot,batch_size=100,verbose=1,epochs=20,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.history[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def save_model(file_name,model,label_encoder,vectorizer):\n",
    "    \"\"\"Saves model structure and vocabularies\"\"\"\n",
    "    model_json = model.to_json()\n",
    "    with open(file_name+\".model.json\", \"w\") as f:\n",
    "        print(model_json,file=f)\n",
    "    with open(file_name+\".vocabularies.json\",\"w\") as f:\n",
    "        classes=list(label_encoder.classes_)\n",
    "        vocab=dict(((str(w),int(idx)) for w,idx in vectorizer.vocabulary_.items()))\n",
    "        json.dump((classes,vocab),f,indent=2)\n",
    "        \n",
    "example_count,feature_count=feature_matrix.shape\n",
    "example_count,class_count=classes_1hot.shape\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(200,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# Save model and vocabularies\n",
    "save_model(\"models/imdb_bow\",model,label_encoder,vectorizer)\n",
    "# Callback function to save weights during training\n",
    "save_cb=ModelCheckpoint(filepath=\"models/imdb_bow.weights.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "hist=model.fit(feature_matrix,classes_1hot,batch_size=100,verbose=1,epochs=30,validation_split=0.1,callbacks=[save_cb])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "venv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
