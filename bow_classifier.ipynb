{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words document classification\n",
    "\n",
    "* BoW is the simplest way to do classification: Feature vector goes in, decision falls out.\n",
    "\n",
    "* Feature vector: a vector with as many dimensions as we have unique features, and a non-zero value set for every feature present in our example\n",
    "* Binary features: 1/0\n",
    "\n",
    "In the following we work with the IMDB data, have a look on [how to read it in](read_imdb.ipynb). Here we just read the ready data in.\n",
    "\n",
    "# IMDB data\n",
    "\n",
    "* Movie review sentiment positive/negative\n",
    "* Some 25,000 examples, 50:50 split\n",
    "* Current state-of-the-art is about 95% accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': 'neg', 'text': '*** Contains Spoilers ***  I did not like this movie at all.  I found it amazingly boring and rather superficially made, irrespective of the importance and depth of the proposed themes: given that eventually we have to die, how should we approach life? In a \\\\light\\\\\" way, like Tomas; in a \\\\\"heavy\\\\\" way like Tereza; or should we find ways not to face that question, like Sabina? How much is fidelity important in a relationship? How much of the professional life can be mutilated for the sake of our loved ones? How much do we have to be involved in the political life and the social issues of our Country?  Unfortunately, I haven\\'t read Kundera\\'s novel but after having being let down by the movie I certainly will: I want to understand if the story was ruined by the movie adaptation (which is my guess) or if it was dull from the beginning.  I disagree with most of the positive comments that defined the movie as a masterpiece. I simply don\\'t see the reasons why. What I see are many flaws, and a sample of them follows.  1) The three main characters are thrown at you and it\\'s very hard to understand what drives them when making their choices.  2) The \\\\\"secondary\\\\\" characters are there just to fill the gaps but they don\\'t add nothing to the story and you wonder if they are really necessary.  3) I did not like how Tomas was impersonated. Nothing is good for him. He is so self-centered and selfish. He is not human, in some sense. But when his self-confidence fails and he realizes that he depends on others and is emotionally linked to someone, I did not find the interpretation credible.  4) It\\'s very unlikely that an artist like Sabina could afford her lifestyle in a communist country in 1968. On top of that, the three main characters are all very successful in their respective professions, which sounds strange to me. a) how can Tereza become effortlessly such a good photographer? b) how can they do so well in a country lacking all the economic incentives that usually motivate people to succeed?  5) The fake accents of the English spoken by the actors are laughable. And I am not even mother tongue. Moreover, the letter that Sabina receives while in the US is written in Czech, which I found very inconsistent.  6) Many comments praised the movie saying that Prague was beautifully rendered: I guess that most of the movie was shot on location, so it\\'s not difficult to give the movie a Eastern European feeling, and given the intrinsic beauty of Prague is not even difficult to make it look good.  7) I found the ending sort of trivial. Tereza and Tomas, finally happy in the countryside, far away from the temptations of the \\\\\"metropoly\\\\\", distant from the social struggles their fellow citizens are living, detached from their professional lives, die in a car accident. But they die after having realized that they are happy, indeed. So what? Had they died unhappy, would the message of the movie have been different? I don\\'t think so. I considered it sort of a cheap trick to please the audience.  8) The only thing in the movie which is unbearably light is the way the director has portrayed the characters. You see them for almost three hours, but in the end you are left with nothing. You don\\'t feel empathy, you don\\'t relate to them, you are left there in your couch watching a sequence of events and scenes that have very little to say.  9) I hated the \\\\\"stop the music in the restaurant\\\\\" scene (which some comments praised a lot). Why Sabina has got such a strong reaction? Why Franz agrees with her? I really don\\'t see the point. The only thing you learn is that Sabina has got a very bad temper and quite a strong personality. That\\'s it. What\\'s so special and unique about it?  After all these negative comments, let me point tout that there are two scenes that I liked a lot (that\\'s why I gave it a two).  The \\\\\"Naked women Photoshoot\\\\\", where the envy, the jealousy, and the insecurities of Sabina and Tereza are beautifully presented.  The other scene is the one representing the investigations after the occupation of Prague by the Russians. Tereza pictures, taken to let the world know about what is going on in Prague, are used to identify the people taking part to the riots. I found it quite original and Tereza\\'s sense of despair and guilt are nicely portrayed.  Finally, there is a tiny possibility that the movie was intentionally \\\\\"designed\\\\\" in such a way that \\\\\"Tomas types\\\\\" are going to like it and \\\\\"Tereza ones\\\\\" are going to hate it. If this is the case (I strongly doubt it, though) then my comment should be revised drastically.\"'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"data/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) #play it safe!\n",
    "print(data[0]) #Every item is a dictionary with `text` and `class` keys, here's the first one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn on this data, we will need a few steps:\n",
    "\n",
    "* Build a data matrix with dimensionality (number of examples, number of possible features), and a value for each feature, 0/1 for binary features\n",
    "* Build a class label matrix (number of examples, number of classes) with the correct labels for the examples, setting 1 for the correct class, and 0 for others\n",
    "\n",
    "It is quite useless to do all this ourselves, so we will use ready-made classes and functions mostly from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*** Contains Spoilers ***  I did not like this movie at all.  I found it amazingly boring and rather superficially made, irrespective of the importance and depth of the proposed themes: given that eventually we have to die, how should we approach life? In a \\\\light\\\\\" way, like Tomas; in a \\\\\"heavy\\\\\" way like Tereza; or should we find ways not to face that question, like Sabina? How much is fidelity important in a relationship? How much of the professional life can be mutilated for the sake of our loved ones? How much do we have to be involved in the political life and the social issues of our Country?  Unfortunately, I haven\\'t read Kundera\\'s novel but after having being let down by the movie I certainly will: I want to understand if the story was ruined by the movie adaptation (which is my guess) or if it was dull from the beginning.  I disagree with most of the positive comments that defined the movie as a masterpiece. I simply don\\'t see the reasons why. What I see are many flaws, and a sample of them follows.  1) The three main characters are thrown at you and it\\'s very hard to understand what drives them when making their choices.  2) The \\\\\"secondary\\\\\" characters are there just to fill the gaps but they don\\'t add nothing to the story and you wonder if they are really necessary.  3) I did not like how Tomas was impersonated. Nothing is good for him. He is so self-centered and selfish. He is not human, in some sense. But when his self-confidence fails and he realizes that he depends on others and is emotionally linked to someone, I did not find the interpretation credible.  4) It\\'s very unlikely that an artist like Sabina could afford her lifestyle in a communist country in 1968. On top of that, the three main characters are all very successful in their respective professions, which sounds strange to me. a) how can Tereza become effortlessly such a good photographer? b) how can they do so well in a country lacking all the economic incentives that usually motivate people to succeed?  5) The fake accents of the English spoken by the actors are laughable. And I am not even mother tongue. Moreover, the letter that Sabina receives while in the US is written in Czech, which I found very inconsistent.  6) Many comments praised the movie saying that Prague was beautifully rendered: I guess that most of the movie was shot on location, so it\\'s not difficult to give the movie a Eastern European feeling, and given the intrinsic beauty of Prague is not even difficult to make it look good.  7) I found the ending sort of trivial. Tereza and Tomas, finally happy in the countryside, far away from the temptations of the \\\\\"metropoly\\\\\", distant from the social struggles their fellow citizens are living, detached from their professional lives, die in a car accident. But they die after having realized that they are happy, indeed. So what? Had they died unhappy, would the message of the movie have been different? I don\\'t think so. I considered it sort of a cheap trick to please the audience.  8) The only thing in the movie which is unbearably light is the way the director has portrayed the characters. You see them for almost three hours, but in the end you are left with nothing. You don\\'t feel empathy, you don\\'t relate to them, you are left there in your couch watching a sequence of events and scenes that have very little to say.  9) I hated the \\\\\"stop the music in the restaurant\\\\\" scene (which some comments praised a lot). Why Sabina has got such a strong reaction? Why Franz agrees with her? I really don\\'t see the point. The only thing you learn is that Sabina has got a very bad temper and quite a strong personality. That\\'s it. What\\'s so special and unique about it?  After all these negative comments, let me point tout that there are two scenes that I liked a lot (that\\'s why I gave it a two).  The \\\\\"Naked women Photoshoot\\\\\", where the envy, the jealousy, and the insecurities of Sabina and Tereza are beautifully presented.  The other scene is the one representing the investigations after the occupation of Prague by the Russians. Tereza pictures, taken to let the world know about what is going on in Prague, are used to identify the people taking part to the riots. I found it quite original and Tereza\\'s sense of despair and guilt are nicely portrayed.  Finally, there is a tiny possibility that the movie was intentionally \\\\\"designed\\\\\" in such a way that \\\\\"Tomas types\\\\\" are going to like it and \\\\\"Tereza ones\\\\\" are going to hate it. If this is the case (I strongly doubt it, though) then my comment should be revised drastically.\"', \"I would rather have 20 root canals than go through this film again. The Prince of Annoying, Myles Berkowitz, has wasted nearly two hours of my life with this piece of cynical pseudo-cool. The only amusing thing in this whole mess is Mr. Samaha's obnoxious off camera patter about tits and ass. Berkowitz takes a great concept and grinds it into the dust. It is choppy and badly done, in an apparent effort to make it seem edgy or funny. It is neither. I seldom feel that a movie was a waste of film (or tape), but this one qualifies for that distinction. If a date suggests seeing this film, run.\"]\n",
      "['neg', 'neg']\n"
     ]
    }
   ],
   "source": [
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape= (25000, 74849)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\n",
    "feature_matrix=vectorizer.fit_transform(texts)\n",
    "print(\"shape=\",feature_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature matrix done! Next thing we need is the class labels to be predicted in one-hot encoding. This means:\n",
    "\n",
    "* one row for every example\n",
    "* one column for every possible class label\n",
    "* exactly one column has 1 for every example, corresponding to the desired class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_numbers shape= (25000,)\n",
      "class labels ['neg' 'pos']\n",
      "classes_1hot [[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "one_hot_encoder=OneHotEncoder(sparse=False) #Turns class integers into one-hot encoding\n",
    "class_numbers=label_encoder.fit_transform(labels)\n",
    "print(\"class_numbers shape=\",class_numbers.shape)\n",
    "print(\"class labels\",label_encoder.classes_) #this will let us translate back from indices to labels\n",
    "#And now yet the one-hot encoding\n",
    "classes_1hot=one_hot_encoder.fit_transform(class_numbers.reshape(-1,1)) #running without reshape tells you to reshape\n",
    "print(\"classes_1hot\",classes_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data is ready, we need to build the network now\n",
    "* Input\n",
    "* Hidden Dense layer with some kind of non-linearity, and a suitable number of nodes\n",
    "* Output Dense layer with the softmax activation (normalizes output to distribution) and as many nodes as there are classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ginter/venv-jupyter/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "example_count,feature_count=feature_matrix.shape\n",
    "example_count2,class_count=classes_1hot.shape\n",
    "assert example_count==example_count2 #sanity check\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(200,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...it's **this** simple...!\n",
    "\n",
    "Once the model is constructed it needs to be compiled, for that we need to know:\n",
    "* which optimizer we want to use (sgd is fine to begin with)\n",
    "* what is the loss (categorial_crossentropy for multiclass of the kind we have is the right choice)\n",
    "* which metrics to measure, accuracy is an okay choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A compiled model can be fitted on data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/10\n",
      "22500/22500 [==============================] - 490s 22ms/step - loss: 0.5519 - acc: 0.7796 - val_loss: 0.4692 - val_acc: 0.8240\n",
      "Epoch 2/10\n",
      "22500/22500 [==============================] - 81s 4ms/step - loss: 0.4230 - acc: 0.8414 - val_loss: 0.3974 - val_acc: 0.8452\n",
      "Epoch 3/10\n",
      "22500/22500 [==============================] - 81s 4ms/step - loss: 0.3685 - acc: 0.8598 - val_loss: 0.3629 - val_acc: 0.8536\n",
      "Epoch 4/10\n",
      "22500/22500 [==============================] - 80s 4ms/step - loss: 0.3367 - acc: 0.8700 - val_loss: 0.3414 - val_acc: 0.8628\n",
      "Epoch 5/10\n",
      "22500/22500 [==============================] - 251s 11ms/step - loss: 0.3138 - acc: 0.8782 - val_loss: 0.3228 - val_acc: 0.8696\n",
      "Epoch 6/10\n",
      "22500/22500 [==============================] - 588s 26ms/step - loss: 0.2963 - acc: 0.8846 - val_loss: 0.3112 - val_acc: 0.8700\n",
      "Epoch 7/10\n",
      "22500/22500 [==============================] - 639s 28ms/step - loss: 0.2820 - acc: 0.8923 - val_loss: 0.3022 - val_acc: 0.8732\n",
      "Epoch 8/10\n",
      "22500/22500 [==============================] - 637s 28ms/step - loss: 0.2696 - acc: 0.8959 - val_loss: 0.2951 - val_acc: 0.8744\n",
      "Epoch 9/10\n",
      "22500/22500 [==============================] - 690s 31ms/step - loss: 0.2589 - acc: 0.9010 - val_loss: 0.2901 - val_acc: 0.8756\n",
      "Epoch 10/10\n",
      "22500/22500 [==============================] - 665s 30ms/step - loss: 0.2493 - acc: 0.9056 - val_loss: 0.2850 - val_acc: 0.8772\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(feature_matrix,classes_1hot,batch_size=100,verbose=1,epochs=10,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8239999985694886, 0.8452000021934509, 0.8535999989509583, 0.8627999997138978, 0.8696000027656555, 0.8699999976158143, 0.8731999969482422, 0.8743999981880188, 0.8755999970436096, 0.8771999955177308]\n"
     ]
    }
   ],
   "source": [
    "print(hist.history[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We ran for 10 epochs of training\n",
    "* Made it to a decent accuracy on the validation data\n",
    "\n",
    "* But we do not have the model saved, so let's fix that and get the whole thing done\n",
    "* What constitutes a model (ie what we need to run the model on new data)\n",
    "  - The feature dictionary in the vectorizer\n",
    "  - The list of classes in their correct order\n",
    "  - The structure of the network\n",
    "  - The weights the network learned\n",
    "\n",
    "* Do all these things, and run again. This time we also increase the number of epochs to 100, see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/100\n",
      "22500/22500 [==============================] - 606s 27ms/step - loss: 0.5612 - acc: 0.7734 - val_loss: 0.4770 - val_acc: 0.8236\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47701, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 2/100\n",
      "22500/22500 [==============================] - 696s 31ms/step - loss: 0.4303 - acc: 0.8395 - val_loss: 0.4033 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47701 to 0.40333, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 3/100\n",
      "22500/22500 [==============================] - 694s 31ms/step - loss: 0.3740 - acc: 0.8581 - val_loss: 0.3654 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.40333 to 0.36543, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 4/100\n",
      "22500/22500 [==============================] - 647s 29ms/step - loss: 0.3406 - acc: 0.8678 - val_loss: 0.3418 - val_acc: 0.8548\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36543 to 0.34178, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 5/100\n",
      "22500/22500 [==============================] - 727s 32ms/step - loss: 0.3170 - acc: 0.8773 - val_loss: 0.3282 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.34178 to 0.32816, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 6/100\n",
      "22500/22500 [==============================] - 735s 33ms/step - loss: 0.2993 - acc: 0.8837 - val_loss: 0.3135 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.32816 to 0.31350, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 7/100\n",
      "22500/22500 [==============================] - 780s 35ms/step - loss: 0.2844 - acc: 0.8898 - val_loss: 0.3046 - val_acc: 0.8700\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.31350 to 0.30458, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 8/100\n",
      "22500/22500 [==============================] - 844s 38ms/step - loss: 0.2721 - acc: 0.8955 - val_loss: 0.3001 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.30458 to 0.30006, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 9/100\n",
      "22500/22500 [==============================] - 835s 37ms/step - loss: 0.2612 - acc: 0.9011 - val_loss: 0.2938 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.30006 to 0.29380, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 10/100\n",
      "22500/22500 [==============================] - 803s 36ms/step - loss: 0.2515 - acc: 0.9053 - val_loss: 0.2886 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.29380 to 0.28858, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 11/100\n",
      "22500/22500 [==============================] - 749s 33ms/step - loss: 0.2427 - acc: 0.9096 - val_loss: 0.2818 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.28858 to 0.28177, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 12/100\n",
      "22500/22500 [==============================] - 804s 36ms/step - loss: 0.2347 - acc: 0.9128 - val_loss: 0.2778 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.28177 to 0.27777, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 13/100\n",
      "22500/22500 [==============================] - 775s 34ms/step - loss: 0.2273 - acc: 0.9158 - val_loss: 0.2761 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.27777 to 0.27610, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 14/100\n",
      "22500/22500 [==============================] - 834s 37ms/step - loss: 0.2204 - acc: 0.9196 - val_loss: 0.2737 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.27610 to 0.27370, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 15/100\n",
      "22500/22500 [==============================] - 700s 31ms/step - loss: 0.2142 - acc: 0.9219 - val_loss: 0.2710 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.27370 to 0.27096, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 16/100\n",
      "22500/22500 [==============================] - 738s 33ms/step - loss: 0.2078 - acc: 0.9242 - val_loss: 0.2699 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.27096 to 0.26992, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 17/100\n",
      "22500/22500 [==============================] - 705s 31ms/step - loss: 0.2023 - acc: 0.9276 - val_loss: 0.2705 - val_acc: 0.8864\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/100\n",
      "22500/22500 [==============================] - 773s 34ms/step - loss: 0.1966 - acc: 0.9307 - val_loss: 0.2679 - val_acc: 0.8848\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.26992 to 0.26790, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 19/100\n",
      "22500/22500 [==============================] - 748s 33ms/step - loss: 0.1912 - acc: 0.9323 - val_loss: 0.2672 - val_acc: 0.8864\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.26790 to 0.26721, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 20/100\n",
      "22500/22500 [==============================] - 793s 35ms/step - loss: 0.1865 - acc: 0.9355 - val_loss: 0.2714 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/100\n",
      "22500/22500 [==============================] - 357s 16ms/step - loss: 0.1815 - acc: 0.9372 - val_loss: 0.2668 - val_acc: 0.8852\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.26721 to 0.26677, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 22/100\n",
      "22500/22500 [==============================] - 263s 12ms/step - loss: 0.1771 - acc: 0.9396 - val_loss: 0.2679 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/100\n",
      "22500/22500 [==============================] - 252s 11ms/step - loss: 0.1729 - acc: 0.9415 - val_loss: 0.2670 - val_acc: 0.8864\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/100\n",
      "22500/22500 [==============================] - 118s 5ms/step - loss: 0.1684 - acc: 0.9428 - val_loss: 0.2685 - val_acc: 0.8848\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/100\n",
      "22500/22500 [==============================] - 142s 6ms/step - loss: 0.1643 - acc: 0.9441 - val_loss: 0.2673 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/100\n",
      "22500/22500 [==============================] - 14s 610us/step - loss: 0.1605 - acc: 0.9467 - val_loss: 0.2678 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/100\n",
      "22500/22500 [==============================] - 25s 1ms/step - loss: 0.1566 - acc: 0.9476 - val_loss: 0.2743 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/100\n",
      "22500/22500 [==============================] - 14s 608us/step - loss: 0.1530 - acc: 0.9497 - val_loss: 0.2724 - val_acc: 0.8852\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/100\n",
      "22500/22500 [==============================] - 17s 734us/step - loss: 0.1494 - acc: 0.9516 - val_loss: 0.2698 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/100\n",
      "22500/22500 [==============================] - 14s 605us/step - loss: 0.1461 - acc: 0.9526 - val_loss: 0.2718 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/100\n",
      "22500/22500 [==============================] - 13s 595us/step - loss: 0.1424 - acc: 0.9548 - val_loss: 0.2734 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/100\n",
      "22500/22500 [==============================] - 14s 602us/step - loss: 0.1389 - acc: 0.9566 - val_loss: 0.2732 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n",
      "22500/22500 [==============================] - 13s 599us/step - loss: 0.1362 - acc: 0.9571 - val_loss: 0.2743 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/100\n",
      "22500/22500 [==============================] - 19s 839us/step - loss: 0.1330 - acc: 0.9590 - val_loss: 0.2759 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/100\n",
      "22500/22500 [==============================] - 14s 643us/step - loss: 0.1299 - acc: 0.9609 - val_loss: 0.2770 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/100\n",
      "22500/22500 [==============================] - 20s 871us/step - loss: 0.1272 - acc: 0.9618 - val_loss: 0.2777 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/100\n",
      "22500/22500 [==============================] - 27s 1ms/step - loss: 0.1244 - acc: 0.9633 - val_loss: 0.2812 - val_acc: 0.8852\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/100\n",
      "22500/22500 [==============================] - 14s 604us/step - loss: 0.1217 - acc: 0.9645 - val_loss: 0.2800 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/100\n",
      "22500/22500 [==============================] - 13s 600us/step - loss: 0.1188 - acc: 0.9653 - val_loss: 0.2830 - val_acc: 0.8856\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500/22500 [==============================] - 13s 597us/step - loss: 0.1163 - acc: 0.9670 - val_loss: 0.2828 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/100\n",
      "22500/22500 [==============================] - 23s 1ms/step - loss: 0.1137 - acc: 0.9684 - val_loss: 0.2874 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/100\n",
      "22500/22500 [==============================] - 27s 1ms/step - loss: 0.1113 - acc: 0.9692 - val_loss: 0.2894 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/100\n",
      "22500/22500 [==============================] - 18s 816us/step - loss: 0.1087 - acc: 0.9703 - val_loss: 0.2874 - val_acc: 0.8816\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/100\n",
      "22500/22500 [==============================] - 14s 608us/step - loss: 0.1066 - acc: 0.9709 - val_loss: 0.2909 - val_acc: 0.8804\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/100\n",
      "22500/22500 [==============================] - 14s 604us/step - loss: 0.1044 - acc: 0.9716 - val_loss: 0.2923 - val_acc: 0.8816\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/100\n",
      "22500/22500 [==============================] - 14s 605us/step - loss: 0.1022 - acc: 0.9728 - val_loss: 0.2917 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/100\n",
      "22500/22500 [==============================] - 14s 607us/step - loss: 0.1001 - acc: 0.9737 - val_loss: 0.2934 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/100\n",
      "22500/22500 [==============================] - 17s 749us/step - loss: 0.0981 - acc: 0.9740 - val_loss: 0.2952 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0959 - acc: 0.9762 - val_loss: 0.2999 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/100\n",
      "22500/22500 [==============================] - 16s 729us/step - loss: 0.0940 - acc: 0.9764 - val_loss: 0.2983 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/100\n",
      "22500/22500 [==============================] - 14s 609us/step - loss: 0.0921 - acc: 0.9778 - val_loss: 0.3009 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/100\n",
      "22500/22500 [==============================] - 14s 602us/step - loss: 0.0902 - acc: 0.9780 - val_loss: 0.3010 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/100\n",
      "22500/22500 [==============================] - 14s 605us/step - loss: 0.0883 - acc: 0.9787 - val_loss: 0.3041 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/100\n",
      "22500/22500 [==============================] - 17s 751us/step - loss: 0.0866 - acc: 0.9796 - val_loss: 0.3055 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0846 - acc: 0.9797 - val_loss: 0.3064 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/100\n",
      "22500/22500 [==============================] - 14s 642us/step - loss: 0.0830 - acc: 0.9808 - val_loss: 0.3109 - val_acc: 0.8756\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/100\n",
      "22500/22500 [==============================] - 14s 605us/step - loss: 0.0813 - acc: 0.9810 - val_loss: 0.3129 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0799 - acc: 0.9824 - val_loss: 0.3182 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/100\n",
      "22500/22500 [==============================] - 17s 774us/step - loss: 0.0782 - acc: 0.9829 - val_loss: 0.3128 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/100\n",
      "22500/22500 [==============================] - 14s 607us/step - loss: 0.0766 - acc: 0.9838 - val_loss: 0.3175 - val_acc: 0.8756\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/100\n",
      "22500/22500 [==============================] - 14s 602us/step - loss: 0.0750 - acc: 0.9833 - val_loss: 0.3174 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/100\n",
      "22500/22500 [==============================] - 17s 764us/step - loss: 0.0735 - acc: 0.9843 - val_loss: 0.3218 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/100\n",
      "22500/22500 [==============================] - 14s 607us/step - loss: 0.0722 - acc: 0.9848 - val_loss: 0.3207 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0708 - acc: 0.9855 - val_loss: 0.3239 - val_acc: 0.8756\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/100\n",
      "22500/22500 [==============================] - 13s 594us/step - loss: 0.0693 - acc: 0.9868 - val_loss: 0.3242 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/100\n",
      "22500/22500 [==============================] - 36s 2ms/step - loss: 0.0679 - acc: 0.9872 - val_loss: 0.3256 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/100\n",
      "22500/22500 [==============================] - 18s 811us/step - loss: 0.0667 - acc: 0.9869 - val_loss: 0.3270 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0654 - acc: 0.9877 - val_loss: 0.3303 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/100\n",
      "22500/22500 [==============================] - 14s 605us/step - loss: 0.0642 - acc: 0.9878 - val_loss: 0.3313 - val_acc: 0.8772\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/100\n",
      "22500/22500 [==============================] - 14s 602us/step - loss: 0.0630 - acc: 0.9882 - val_loss: 0.3331 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/100\n",
      "22500/22500 [==============================] - 30s 1ms/step - loss: 0.0618 - acc: 0.9891 - val_loss: 0.3350 - val_acc: 0.8752\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/100\n",
      "22500/22500 [==============================] - 17s 751us/step - loss: 0.0608 - acc: 0.9894 - val_loss: 0.3378 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/100\n",
      "22500/22500 [==============================] - 37s 2ms/step - loss: 0.0594 - acc: 0.9900 - val_loss: 0.3370 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/100\n",
      "22500/22500 [==============================] - 14s 604us/step - loss: 0.0584 - acc: 0.9902 - val_loss: 0.3389 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0572 - acc: 0.9904 - val_loss: 0.3415 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/100\n",
      "22500/22500 [==============================] - 14s 604us/step - loss: 0.0562 - acc: 0.9915 - val_loss: 0.3422 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/100\n",
      "22500/22500 [==============================] - 17s 747us/step - loss: 0.0551 - acc: 0.9917 - val_loss: 0.3443 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/100\n",
      "22500/22500 [==============================] - 14s 606us/step - loss: 0.0540 - acc: 0.9916 - val_loss: 0.3469 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/100\n",
      "22500/22500 [==============================] - 17s 773us/step - loss: 0.0533 - acc: 0.9921 - val_loss: 0.3478 - val_acc: 0.8756\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/100\n",
      "22500/22500 [==============================] - 40s 2ms/step - loss: 0.0520 - acc: 0.9922 - val_loss: 0.3519 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/100\n",
      "22500/22500 [==============================] - 14s 606us/step - loss: 0.0512 - acc: 0.9924 - val_loss: 0.3508 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/100\n",
      "22500/22500 [==============================] - 14s 609us/step - loss: 0.0503 - acc: 0.9924 - val_loss: 0.3542 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/100\n",
      "22500/22500 [==============================] - 14s 607us/step - loss: 0.0494 - acc: 0.9932 - val_loss: 0.3550 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/100\n",
      "22500/22500 [==============================] - 14s 605us/step - loss: 0.0485 - acc: 0.9932 - val_loss: 0.3567 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/100\n",
      "22500/22500 [==============================] - 14s 601us/step - loss: 0.0476 - acc: 0.9934 - val_loss: 0.3579 - val_acc: 0.8776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0468 - acc: 0.9934 - val_loss: 0.3607 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/100\n",
      "22500/22500 [==============================] - 14s 604us/step - loss: 0.0461 - acc: 0.9940 - val_loss: 0.3611 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0452 - acc: 0.9944 - val_loss: 0.3648 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/100\n",
      "22500/22500 [==============================] - 14s 606us/step - loss: 0.0445 - acc: 0.9943 - val_loss: 0.3651 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/100\n",
      "22500/22500 [==============================] - 14s 604us/step - loss: 0.0438 - acc: 0.9944 - val_loss: 0.3677 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0430 - acc: 0.9946 - val_loss: 0.3738 - val_acc: 0.8716\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/100\n",
      "22500/22500 [==============================] - 14s 604us/step - loss: 0.0422 - acc: 0.9948 - val_loss: 0.3710 - val_acc: 0.8756\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/100\n",
      "22500/22500 [==============================] - 14s 604us/step - loss: 0.0416 - acc: 0.9946 - val_loss: 0.3730 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0408 - acc: 0.9950 - val_loss: 0.3748 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0402 - acc: 0.9955 - val_loss: 0.3788 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/100\n",
      "22500/22500 [==============================] - 14s 608us/step - loss: 0.0394 - acc: 0.9954 - val_loss: 0.3767 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/100\n",
      "22500/22500 [==============================] - 14s 604us/step - loss: 0.0388 - acc: 0.9956 - val_loss: 0.3796 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0381 - acc: 0.9956 - val_loss: 0.3805 - val_acc: 0.8752\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/100\n",
      "22500/22500 [==============================] - 14s 603us/step - loss: 0.0376 - acc: 0.9958 - val_loss: 0.3869 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/100\n",
      "22500/22500 [==============================] - 14s 609us/step - loss: 0.0369 - acc: 0.9959 - val_loss: 0.3845 - val_acc: 0.8756\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def save_model(file_name,model,label_encoder,vectorizer):\n",
    "    \"\"\"Saves model structure and vocabularies\"\"\"\n",
    "    model_json = model.to_json()\n",
    "    with open(file_name+\".model.json\", \"w\") as f:\n",
    "        print(model_json,file=f)\n",
    "    with open(file_name+\".vocabularies.json\",\"w\") as f:\n",
    "        classes=list(label_encoder.classes_)\n",
    "        vocab=dict(((str(w),int(idx)) for w,idx in vectorizer.vocabulary_.items())) #must turn numpy objects to python ones\n",
    "        json.dump((classes,vocab),f,indent=2)\n",
    "        \n",
    "example_count,feature_count=feature_matrix.shape\n",
    "example_count2,class_count=classes_1hot.shape\n",
    "assert example_count==example_count2 #sanity check\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(200,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "# Let's try a different optimizer!\n",
    "model.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# Save model and vocabularies, can be done before training\n",
    "save_model(\"models/imdb_bow\",model,label_encoder,vectorizer)\n",
    "# Callback function to save weights during training, if validation loss goes down\n",
    "save_cb=ModelCheckpoint(filepath=\"models/imdb_bow.weights.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "hist=model.fit(feature_matrix,classes_1hot,batch_size=100,verbose=1,epochs=100,validation_split=0.1,callbacks=[save_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corrected answer to a lecture question** why the training set accuracy is smaller on first epoch than the validation set accuracy.\n",
    "\n",
    "My quick answer was not correct. Given a little more time to think, what happens is this: The training set accuracy is calculated from the beginning, when the network is randomly initialized and doesn't really do anything. The validation accuracy is calculated only after the first epoch, when the network already learned for one full epoch. That is why the number on validation set can be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network output= [[9.99880433e-01 1.19501674e-04]\n",
      " [4.34341507e-10 1.00000000e+00]\n",
      " [7.21750723e-07 9.99999285e-01]\n",
      " ...\n",
      " [9.99235392e-01 7.64620898e-04]\n",
      " [6.95837021e-01 3.04162979e-01]\n",
      " [9.22101676e-01 7.78983310e-02]]\n",
      "Maximum class for each example= [0 1 1 ... 0 0 0]\n",
      "Confusion matrix=\n",
      " [[1109  152]\n",
      " [ 159 1080]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.87      0.88      0.88      1261\n",
      "        pos       0.88      0.87      0.87      1239\n",
      "\n",
      "avg / total       0.88      0.88      0.88      2500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ginter/venv-jupyter/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/ginter/venv-jupyter/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#Validation data used during training:\n",
    "val_instances,val_labels_1hot,_=hist.validation_data\n",
    "\n",
    "print(\"Network output=\",model.predict(val_instances))\n",
    "predictions=numpy.argmax(model.predict(val_instances),axis=1)\n",
    "print(\"Maximum class for each example=\",predictions)\n",
    "gold=numpy.nonzero(val_labels_1hot)[1] #undo 1-hot encoding\n",
    "conf_matrix=confusion_matrix(list(gold),list(predictions))\n",
    "print(\"Confusion matrix=\\n\",conf_matrix)\n",
    "### FIXED VERSION (thanks for reporting the bug during the lecture!)\n",
    "### \n",
    "gold_labels=label_encoder.inverse_transform(list(gold))\n",
    "predicted_labels=label_encoder.inverse_transform(list(predictions))\n",
    "#print(\"Gold labels=\",gold_labels)\n",
    "#print(\"Predicted labels=\",predicted_labels)\n",
    "print(classification_report(gold_labels,predicted_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning progress\n",
    "\n",
    "* The history object we get lets us inspect the accuracy during training\n",
    "* Remarks:\n",
    "  - Accuracy on training data keeps going up\n",
    "  - Accuracy on validation (test) data flattens out after a but over 10 epochs, we are learning very little past that point\n",
    "  - What we see is the network keeps overfitting on the training data to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65247e6208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#print(hist.history[\"val_acc\"])\n",
    "plt.ylim(0.85,1.0)\n",
    "plt.plot(hist.history[\"val_acc\"],label=\"Validation set accuracy\")\n",
    "plt.plot(hist.history[\"acc\"],label=\"Training set accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "* We put together a program to train a neural network classifier for sentiment detector\n",
    "* We learned the necessary code/techniques to save models, and feed the training with data in just the right format\n",
    "* We observed the training across epochs\n",
    "* We saw how the classifier can be applied to various text classification problems\n",
    "* The IMDB sentiment classifier ended up at nearly 90% accuracy, the state of the art is about 95%, we got surprisingly far in few lines of code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "venv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
