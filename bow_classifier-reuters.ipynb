{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words document classification\n",
    "\n",
    "What will happen on Reuters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '&#2;\\nUNITED COMPANIES &lt;UNCF> DECLARES STOCK DIVIDEND\\nBATON ROUGE, La, March 6 - United Companies Financial Corp\\nsaid its board declared a two pct stock dividend payable APril\\neight to holders of record March 17.\\nThe board also declared a regular quarterly cash dividend\\nof 12.5 cts payable April one to holders of record March 16.\\nReuter\\n&#3;', 'class': 'earn'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "random.seed(0)\n",
    "with open(\"data/reuters_51cls.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) #play it safe!\n",
    "print(data[0]) #Every item is a dictionary with `text` and `class` keys, here's one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['&#2;\\nUNITED COMPANIES &lt;UNCF> DECLARES STOCK DIVIDEND\\nBATON ROUGE, La, March 6 - United Companies Financial Corp\\nsaid its board declared a two pct stock dividend payable APril\\neight to holders of record March 17.\\nThe board also declared a regular quarterly cash dividend\\nof 12.5 cts payable April one to holders of record March 16.\\nReuter\\n&#3;', '&#2;\\nCANBRA FOODS SETS SPECIAL FIVE DLR/SHR PAYOUT\\nLETHBRIDGE, Alberta, March 16 - &lt;Canbra Foods Ltd>, earlier\\nreporting a 1986 net profit against a year-ago loss, said it\\ndeclared a special, one-time dividend of five dlrs per common\\nshare, pay March 31, record March 26.\\nCanbra said it set the special payout to allow shareholders\\nto participate in the gain on the sale of unit Stafford Foods\\nLtd in November, 1986, as well as the company\\'s \"unusually\\nprofitable performance\" in 1986.\\nCanbra earlier reported 1986 net earnings of 4.2 mln dlrs,\\nexcluding a 1.3 mln dlr gain on the Stafford sale, compared to\\na year-ago loss of 1.5 mln dlrs.\\nReuter\\n&#3;']\n",
      "['earn', 'earn']\n"
     ]
    }
   ],
   "source": [
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape= (9465, 100000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,2))\n",
    "feature_matrix=vectorizer.fit_transform(texts)\n",
    "print(\"shape=\",feature_matrix.shape)\n",
    "#print(feature_matrix.todense())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature matrix done! Next thing we need is the class labels to be predicted in one-hot encoding. This means:\n",
    "\n",
    "* one row for every example\n",
    "* one column for every possible class label\n",
    "* exactly one column has 1 for every example, corresponding to the desired class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_numbers shape= (9465,)\n",
      "class_numbers [11 11 11 ... 11  0  0]\n",
      "class labels ['acq' 'alum' 'bop' 'carcass' 'cocoa' 'coffee' 'copper' 'cotton' 'cpi'\n",
      " 'crude' 'dlr' 'earn' 'fuel' 'gas' 'gnp' 'gold' 'grain' 'heat' 'housing'\n",
      " 'income' 'instal-debt' 'interest' 'ipi' 'iron-steel' 'jobs' 'lead' 'lei'\n",
      " 'livestock' 'lumber' 'meal-feed' 'money-fx' 'money-supply' 'nat-gas'\n",
      " 'oilseed' 'orange' 'pet-chem' 'potato' 'reserves' 'retail' 'rubber'\n",
      " 'ship' 'silver' 'strategic-metal' 'sugar' 'tea' 'tin' 'trade' 'veg-oil'\n",
      " 'wpi' 'yen' 'zinc']\n",
      "classes_1hot [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "one_hot_encoder=OneHotEncoder(sparse=False) #Turns class integers into one-hot encoding\n",
    "class_numbers=label_encoder.fit_transform(labels)\n",
    "print(\"class_numbers shape=\",class_numbers.shape)\n",
    "print(\"class_numbers\",class_numbers)\n",
    "print(\"class labels\",label_encoder.classes_)\n",
    "#And now yet the one-hot encoding\n",
    "classes_1hot=one_hot_encoder.fit_transform(class_numbers.reshape(-1,1))\n",
    "print(\"classes_1hot\",classes_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8518 samples, validate on 947 samples\n",
      "Epoch 1/30\n",
      "8518/8518 [==============================] - 11s 1ms/step - loss: 2.4822 - acc: 0.5788 - val_loss: 1.7624 - val_acc: 0.6389\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.76240, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 2/30\n",
      "8518/8518 [==============================] - 10s 1ms/step - loss: 1.6365 - acc: 0.6335 - val_loss: 1.4902 - val_acc: 0.6610\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.76240 to 1.49023, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 3/30\n",
      "8518/8518 [==============================] - 11s 1ms/step - loss: 1.4269 - acc: 0.6749 - val_loss: 1.3628 - val_acc: 0.6938\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.49023 to 1.36283, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 4/30\n",
      "8518/8518 [==============================] - 11s 1ms/step - loss: 1.3063 - acc: 0.7026 - val_loss: 1.2770 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36283 to 1.27699, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 5/30\n",
      "8518/8518 [==============================] - 9s 1ms/step - loss: 1.2171 - acc: 0.7265 - val_loss: 1.2087 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.27699 to 1.20868, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 6/30\n",
      "8518/8518 [==============================] - 8s 945us/step - loss: 1.1443 - acc: 0.7468 - val_loss: 1.1521 - val_acc: 0.7445\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.20868 to 1.15206, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 7/30\n",
      "8518/8518 [==============================] - 12s 1ms/step - loss: 1.0823 - acc: 0.7659 - val_loss: 1.1032 - val_acc: 0.7529\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.15206 to 1.10322, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 8/30\n",
      "8518/8518 [==============================] - 9s 1ms/step - loss: 1.0281 - acc: 0.7832 - val_loss: 1.0599 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.10322 to 1.05988, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 9/30\n",
      "8518/8518 [==============================] - 8s 982us/step - loss: 0.9795 - acc: 0.7976 - val_loss: 1.0207 - val_acc: 0.7846\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.05988 to 1.02072, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 10/30\n",
      "8518/8518 [==============================] - 8s 976us/step - loss: 0.9355 - acc: 0.8092 - val_loss: 0.9854 - val_acc: 0.7983\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.02072 to 0.98541, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 11/30\n",
      "8518/8518 [==============================] - 9s 1ms/step - loss: 0.8956 - acc: 0.8194 - val_loss: 0.9533 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.98541 to 0.95334, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 12/30\n",
      "8518/8518 [==============================] - 11s 1ms/step - loss: 0.8587 - acc: 0.8292 - val_loss: 0.9233 - val_acc: 0.8131\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.95334 to 0.92332, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 13/30\n",
      "8518/8518 [==============================] - 9s 1ms/step - loss: 0.8250 - acc: 0.8365 - val_loss: 0.8959 - val_acc: 0.8152\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.92332 to 0.89593, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 14/30\n",
      "8518/8518 [==============================] - 16s 2ms/step - loss: 0.7937 - acc: 0.8436 - val_loss: 0.8700 - val_acc: 0.8247\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.89593 to 0.87003, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 15/30\n",
      "8518/8518 [==============================] - 10s 1ms/step - loss: 0.7646 - acc: 0.8493 - val_loss: 0.8463 - val_acc: 0.8310\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.87003 to 0.84633, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 16/30\n",
      "8518/8518 [==============================] - 13s 2ms/step - loss: 0.7375 - acc: 0.8535 - val_loss: 0.8245 - val_acc: 0.8321\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.84633 to 0.82453, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 17/30\n",
      "8518/8518 [==============================] - 8s 910us/step - loss: 0.7122 - acc: 0.8572 - val_loss: 0.8039 - val_acc: 0.8374\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.82453 to 0.80394, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 18/30\n",
      "8518/8518 [==============================] - 9s 1ms/step - loss: 0.6884 - acc: 0.8605 - val_loss: 0.7846 - val_acc: 0.8384\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.80394 to 0.78459, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 19/30\n",
      "8518/8518 [==============================] - 10s 1ms/step - loss: 0.6661 - acc: 0.8655 - val_loss: 0.7663 - val_acc: 0.8427\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.78459 to 0.76628, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 20/30\n",
      "8518/8518 [==============================] - 10s 1ms/step - loss: 0.6448 - acc: 0.8700 - val_loss: 0.7488 - val_acc: 0.8458\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.76628 to 0.74882, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 21/30\n",
      "8518/8518 [==============================] - 10s 1ms/step - loss: 0.6247 - acc: 0.8731 - val_loss: 0.7328 - val_acc: 0.8522\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.74882 to 0.73280, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 22/30\n",
      "8518/8518 [==============================] - 12s 1ms/step - loss: 0.6058 - acc: 0.8763 - val_loss: 0.7173 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.73280 to 0.71732, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 23/30\n",
      "8518/8518 [==============================] - 9s 1ms/step - loss: 0.5879 - acc: 0.8804 - val_loss: 0.7032 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.71732 to 0.70317, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 24/30\n",
      "8518/8518 [==============================] - 11s 1ms/step - loss: 0.5708 - acc: 0.8845 - val_loss: 0.6892 - val_acc: 0.8617\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.70317 to 0.68925, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 25/30\n",
      "8518/8518 [==============================] - 9s 1ms/step - loss: 0.5545 - acc: 0.8880 - val_loss: 0.6763 - val_acc: 0.8638\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.68925 to 0.67629, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 26/30\n",
      "8518/8518 [==============================] - 10s 1ms/step - loss: 0.5391 - acc: 0.8916 - val_loss: 0.6639 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.67629 to 0.66392, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 27/30\n",
      "8518/8518 [==============================] - 10s 1ms/step - loss: 0.5243 - acc: 0.8948 - val_loss: 0.6519 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.66392 to 0.65192, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 28/30\n",
      "8518/8518 [==============================] - 8s 954us/step - loss: 0.5102 - acc: 0.8989 - val_loss: 0.6407 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.65192 to 0.64075, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 29/30\n",
      "8518/8518 [==============================] - 11s 1ms/step - loss: 0.4967 - acc: 0.9023 - val_loss: 0.6297 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.64075 to 0.62965, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 30/30\n",
      "8518/8518 [==============================] - 9s 1ms/step - loss: 0.4838 - acc: 0.9049 - val_loss: 0.6192 - val_acc: 0.8722\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.62965 to 0.61923, saving model to models/reuters_51cls_bow.weights.h5\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def save_model(file_name,model,label_encoder,vectorizer):\n",
    "    \"\"\"Saves model structure and vocabularies\"\"\"\n",
    "    model_json = model.to_json()\n",
    "    with open(file_name+\".model.json\", \"w\") as f:\n",
    "        print(model_json,file=f)\n",
    "    with open(file_name+\".vocabularies.json\",\"w\") as f:\n",
    "        classes=list(label_encoder.classes_)\n",
    "        vocab=dict(((str(w),int(idx)) for w,idx in vectorizer.vocabulary_.items()))\n",
    "        json.dump((classes,vocab),f,indent=2)\n",
    "        \n",
    "example_count,feature_count=feature_matrix.shape\n",
    "example_count,class_count=classes_1hot.shape\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(200,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# Save model and vocabularies\n",
    "save_model(\"models/reuters_51cls_bow\",model,label_encoder,vectorizer)\n",
    "# Callback function to save weights during training\n",
    "save_cb=ModelCheckpoint(filepath=\"models/reuters_51cls_bow.weights.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "hist=model.fit(feature_matrix,classes_1hot,batch_size=100,verbose=1,epochs=30,validation_split=0.1,callbacks=[save_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network output= [[2.0972650e-01 1.2842861e-02 9.6099349e-03 ... 6.1267391e-03\n",
      "  6.0931095e-03 5.6801271e-03]\n",
      " [1.0681288e-04 3.5428602e-06 3.7730745e-06 ... 2.1082194e-06\n",
      "  1.9002957e-06 1.1053662e-06]\n",
      " [5.9391409e-01 7.4532600e-03 4.1777967e-03 ... 3.3430997e-03\n",
      "  4.1454947e-03 2.9960810e-03]\n",
      " ...\n",
      " [3.2259982e-02 1.8379733e-03 9.8252657e-04 ... 7.1975170e-04\n",
      "  8.0373778e-04 5.8117666e-04]\n",
      " [9.9498838e-01 7.3048315e-05 3.5937352e-05 ... 1.7827695e-05\n",
      "  2.7510325e-05 2.1379099e-05]\n",
      " [9.9085873e-01 1.3929917e-04 7.2065945e-05 ... 3.2477805e-05\n",
      "  5.5748078e-05 4.0438361e-05]]\n",
      "Maximum class for each example= [ 0 11  0 43  0  0 11 11 11  9  0  0 11  0 11  9 11  0  0 11  0 11  9 21\n",
      " 46  9 11 11  0 30 21 11  8 40 46  0 11  0 11 30 11  5 11  0 11 11  0 11\n",
      " 11 11  0  0  9 11 11 21  0  8 11 46  0 11  0 11 11 11 21 30 31 46 37 11\n",
      " 11  0 11 46 11 21  0 11  0 40 11 40  0 21 11  8 11 11 11 11 11  0 11 11\n",
      " 46  0 11  4 11  0 11  0 21 11  0  0 11 11  0 21  0  0  0  0 46 11 46  8\n",
      " 11 46  0  0 46 11  0 21 11 11  8 11  9 11  5 11 24 11  8 11 11 11  0  0\n",
      " 11 11 11 32  0 11 11 43  0 11 21  0 15  0  0  0  0 11 11  0 11 11 46  0\n",
      " 46 11 30 11  0 21  9 11 43 24 11  0 11 21  0 11 11 11 15 11 30 43  0  9\n",
      " 11 47  0 11 11  0 43  0 11  0 11  0 46 11 11 11 27  0  0 11  0 11  9 40\n",
      " 11 11  9 11  0  0 31  9 11  9  0  0 11  0 46 11  9 11  0 11  0  0  0 11\n",
      " 11 11  0  0 11 31  0  0 11 11 11  0 21  9 22 11  0  0 11 11  0  0 11  0\n",
      " 11 11 11 46 46  0 40 11  0  4  9 11 21 11 46 11 46  0 11 11 30  0 11 11\n",
      "  0 11  0 11 46  0  0 14 31  0 11  9  0  4 11  0 11 43  0 11  0  0 11  0\n",
      " 11 15 11 11 11  0 11  0  0  0 15 11  0  0  0  8 11 30 30 11  0  0 11 30\n",
      " 11  9 11 40 11  0 11 11 11  0 11 11  0 11 11  0  0 11  0  0  0 11 11 11\n",
      " 11 11  0  0 11 21  0 46 11 11  0 11  8 11 46  0  0 11  0 46  0  4 11 11\n",
      " 11 11 11  0 11 11 11 46 21 11 11 11 11  0 11 11 11 11 30 11 11 11  6 11\n",
      "  0 24 11  0 11 11 21 40 11  0  0  0  0  0 11 43  9 11 11  0 11 11 11 11\n",
      "  9  4  0 11 11 43 43  0  9 11 31 11  0 11 11 21 30  0  0  0 46 11 11 11\n",
      " 46 30  0 11 11 11 31 11 11 11  0 11 46 21  9  0 11  0  0 30 11  0 11 30\n",
      " 11 11 11 11 43  9  0 31  9  0 11  8 31  9 43 11 11  0 11 11 11  0  0 30\n",
      " 11 11  9  0  0 11 11 11 11 46  0  9 31 11 11 11  0 11  0 11  0  0  9 11\n",
      "  0 31 11  9 11  0  0 11  0  0  0  0  5 30  0 11  0 11 11  0  0 11 11 11\n",
      " 46  0  0 11 11 30 11 11 46 46 11  0 11 11 11 11 11  0 43 11  0 46  0 11\n",
      " 40  9  0 11 11  0 43 31  4  9 11 11 40 11 11  0  0 11  0 11  9  0 11 11\n",
      " 11 43  8 11  0 11 40 31 30  0 22  5 11 11 11 11  8 11  0  9 11  0 31  9\n",
      " 11 11  0 11  0 11 11  0  0  0 11 11 11  0  0  0 40  9  0 11 11 11 21 46\n",
      " 11  0 31 11 11 11  0 11  0  0 11  9 15 11 11 11 11 11 46  8 11 11 11 31\n",
      "  0 24 43 11 11  0 11 11 11 11 43 11 11  0  0 11 11 11 11  0 11  0 11  0\n",
      "  0 43  0 11 11 11 46 11 11 40 11 11 11 11  0 11  0  0 11 40 11 14 11 11\n",
      " 11 11 11 40 37 11  5  4 21 11 11 31  0 30 11  5 11 31 11 21  0  0 11  4\n",
      "  9 40  0  0 31  8  0  0 11  0  0 11 15 11  9 11  0  0  0 40 11 30  0 11\n",
      " 40 21  0  0 11  0 11 11  0 11 11 11 11  8  9  0 46 11 11  0 40  0  0 11\n",
      " 46 11 11 11 40 11 11 11  0 11 11  0 11 30  0  0 11 11  8 11 30  9  0 11\n",
      "  0 30 18 21  0 11 11  0 11  9  5 11 11 11  0  0 21 11  0 11 43  5  0 11\n",
      " 46  9  9  0 11 11 21 11 11 31  0 11 11 21  0 30 11  0 31 11 43 11 11 11\n",
      "  0 11  0 11 11 11 46 11 30 11 46 11 11  0 11  0 11 11  9  0 43  9 11  0\n",
      " 46 11  0  0 43  0 11 11  9 11 11  9  5  0  0 11  9 11 22 11  0 14 24 46\n",
      " 11 11 11 43  0 11 11  9 21 46 11 11  0  0  0 11 30 11  0  0 11 21 11  9\n",
      " 46 21 11  0 11 11  0  0 11  0  0]\n",
      "[[236   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   1   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  1   0   0 ...   0   0   0]]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "            acq       0.89      0.99      0.93       239\n",
      "           alum       0.00      0.00      0.00         4\n",
      "            bop       0.00      0.00      0.00         1\n",
      "        carcass       1.00      0.62      0.76        13\n",
      "          cocoa       0.78      0.78      0.78         9\n",
      "         coffee       1.00      0.17      0.29         6\n",
      "         copper       0.00      0.00      0.00         3\n",
      "         cotton       0.60      0.90      0.72        10\n",
      "            cpi       0.71      0.88      0.79        40\n",
      "          crude       0.00      0.00      0.00         1\n",
      "            dlr       0.97      1.00      0.99       406\n",
      "           earn       0.00      0.00      0.00         1\n",
      "           fuel       0.00      0.00      0.00         2\n",
      "            gas       1.00      0.60      0.75         5\n",
      "            gnp       0.83      0.56      0.67         9\n",
      "           gold       0.00      0.00      0.00         8\n",
      "          grain       0.00      0.00      0.00         2\n",
      "           heat       1.00      0.25      0.40         4\n",
      "        housing       0.00      0.00      0.00         1\n",
      "         income       0.67      0.83      0.74        24\n",
      "    instal-debt       1.00      0.43      0.60         7\n",
      "       interest       0.00      0.00      0.00         7\n",
      "            ipi       0.60      0.75      0.67         4\n",
      "     iron-steel       0.00      0.00      0.00         3\n",
      "           jobs       1.00      0.33      0.50         3\n",
      "           lead       0.00      0.00      0.00         1\n",
      "            lei       0.00      0.00      0.00         1\n",
      "      livestock       0.85      0.88      0.86        25\n",
      "         lumber       0.70      0.93      0.80        15\n",
      "      meal-feed       1.00      0.25      0.40         4\n",
      "       money-fx       0.00      0.00      0.00         2\n",
      "   money-supply       0.00      0.00      0.00         3\n",
      "        nat-gas       0.00      0.00      0.00         3\n",
      "        oilseed       0.00      0.00      0.00         1\n",
      "         orange       1.00      0.40      0.57         5\n",
      "       pet-chem       0.00      0.00      0.00         2\n",
      "         potato       0.00      0.00      0.00         2\n",
      "       reserves       0.47      0.64      0.55        14\n",
      "         retail       0.00      0.00      0.00         3\n",
      "         rubber       0.00      0.00      0.00         2\n",
      "           ship       0.41      0.90      0.56        10\n",
      "         silver       0.00      0.00      0.00         3\n",
      "strategic-metal       0.72      0.94      0.82        33\n",
      "          sugar       1.00      0.50      0.67         2\n",
      "            tea       0.00      0.00      0.00         3\n",
      "            tin       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       0.83      0.87      0.84       947\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ginter/venv-jupyter/lib/python3.5/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 46, does not match size of target_names, 51\n",
      "  .format(len(labels), len(target_names))\n",
      "/home/ginter/venv-jupyter/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#Validation data used during training:\n",
    "val_instances,val_labels_1hot,_=hist.validation_data\n",
    "\n",
    "print(\"Network output=\",model.predict(val_instances))\n",
    "predictions=numpy.argmax(model.predict(val_instances),axis=1)\n",
    "print(\"Maximum class for each example=\",predictions)\n",
    "gold=numpy.nonzero(val_labels_1hot)[1] #undo 1-hot encoding\n",
    "conf_matrix=confusion_matrix(list(gold),list(predictions))\n",
    "print(conf_matrix)\n",
    "print(classification_report(list(gold),list(predictions),target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "venv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
