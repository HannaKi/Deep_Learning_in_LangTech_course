{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words document classification\n",
    "\n",
    "What will happen on Reuters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"&#2;\\nANALYSTS SEE NO OTHER BIDDER FOR PUROLATOR&lt;PCC>\\nNew York, March 2 - Several analysts said they do not\\nbelieve another suitor will top the 265 mln dlr bid for\\nPurolator Courier Corp by E.F. Hutton LBO Inc and a management\\ngroup from Purolator's courier division.\\nThere had been speculation another offer might be\\nforthcoming, but analysts mostly believe the 35 dlrs per share\\nprice being paid by Hutton and the managers' PC Acquisition Inc\\nis fully valued.\\nAnalysts and some Wall Street sources said they doubted\\nanother bidder would emerge since Purolator had been for sale\\nfor sometime before a deal was struck with Hutton Friday.\\nPurolator's stock slipped 3/8 today to close at 34-3/4. It\\nhad been trading slightly higher than the 35 dlr offer on\\nFriday. At least one analyst Friday speculated the company\\nmight fetch 38 to 42 dlrs per share.\\nanalysts and wall street sources doubted a competitive\\noffer would emerge since the company has been for sale for\\nsometime before the deal with Hutton was struck Friday.\\nHutton had been in talks with Purolator's adviser, Dillon,\\nRead and Co since late December, a Hutton spokesman said.\\nHutton is offering 35 dlrs cash per share for 83 pct of the\\nshares. If all shares are tendered, shareholders would receive\\n29 dlrs cash, six dlrs in debentures, and warrants for stock in\\na subsidiary of PC Acquisition containing the Purolator U.S.\\ncourier operation. Hutton values the warrants at two to three\\ndlrs per share.\\nWall Street sources also said today that a rival bidder\\nmight be discouraged by a breakup fee Purolator would have to\\npay if it ends its agreement with Hutton. The sources would not\\nreveal the amount of the fee, which will be noted in documents\\non the transaction to be made public later this week.\\nReuter\\n&#3;\", 'class': 'acq'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"data/reuters_51cls.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) #play it safe!\n",
    "print(data[0]) #Every item is a dictionary with `text` and `class` keys, here's one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"&#2;\\nANALYSTS SEE NO OTHER BIDDER FOR PUROLATOR&lt;PCC>\\nNew York, March 2 - Several analysts said they do not\\nbelieve another suitor will top the 265 mln dlr bid for\\nPurolator Courier Corp by E.F. Hutton LBO Inc and a management\\ngroup from Purolator's courier division.\\nThere had been speculation another offer might be\\nforthcoming, but analysts mostly believe the 35 dlrs per share\\nprice being paid by Hutton and the managers' PC Acquisition Inc\\nis fully valued.\\nAnalysts and some Wall Street sources said they doubted\\nanother bidder would emerge since Purolator had been for sale\\nfor sometime before a deal was struck with Hutton Friday.\\nPurolator's stock slipped 3/8 today to close at 34-3/4. It\\nhad been trading slightly higher than the 35 dlr offer on\\nFriday. At least one analyst Friday speculated the company\\nmight fetch 38 to 42 dlrs per share.\\nanalysts and wall street sources doubted a competitive\\noffer would emerge since the company has been for sale for\\nsometime before the deal with Hutton was struck Friday.\\nHutton had been in talks with Purolator's adviser, Dillon,\\nRead and Co since late December, a Hutton spokesman said.\\nHutton is offering 35 dlrs cash per share for 83 pct of the\\nshares. If all shares are tendered, shareholders would receive\\n29 dlrs cash, six dlrs in debentures, and warrants for stock in\\na subsidiary of PC Acquisition containing the Purolator U.S.\\ncourier operation. Hutton values the warrants at two to three\\ndlrs per share.\\nWall Street sources also said today that a rival bidder\\nmight be discouraged by a breakup fee Purolator would have to\\npay if it ends its agreement with Hutton. The sources would not\\nreveal the amount of the fee, which will be noted in documents\\non the transaction to be made public later this week.\\nReuter\\n&#3;\", \"&#2;\\nSPECTRA-PHYSICS &lt;SPY> BOARD REJECTS TENDER OFFER\\nSAN JOSE, Calif., June 1 - Spectra-Physics Inc said its\\nboard rejected a 32 dlrs per share unsolicited tender offer for\\nthe company's stock from Ciba-Geigy Ltd &lt;CIGZ.Z>, which already\\nholds 18.8 pct of the stock.\\nSpectra-Physics said it also filed a lawsuit in Delaware\\nfederal court this morning seeking to enjoin the offer and\\nalleging, among other things, that the offer vilates federal\\nsecurities laws, certain agreements between Ciba-Geigy and\\nSpectra-Physics, and Ciba-Geigy's fiduciary duties.\\nSpectra-Physics said the two Ciba-Geigy designess to its\\nboard were not present at yesterday's special meeting which\\nvoted to reject the offer as financially inadequate, unfair and\\nnot in the best interests of Spectra-Physics or its\\nstockholders.\\nThe company said the board also authorized a special\\ncommittee of outside directors to take whatever steps it deems\\nnecessary to protect the interests of Spectra-Physics and its\\nstockholders and to investigate all alternatives to maximize\\nthe value of the stock, including talks with third parties.\\nSpectra-Physics said a letter communicating the board's\\nrecommendation and reasons therefore is being mailed to\\nstockholders.\\nIt said Robert Bruce, Reliance Group Holdings Inc's &lt;REL>\\ndesignee on Spectra-Physics' board, resigned his position on\\nMay 29. His letter of resignation said the action was to\\nalleviate Ciba-Geigy's stated justification for making the\\nunsolicited offer that it had not contemplated another\\nsignificant investor having representation on the board when\\nits Spectra-Physics' investment was made.\\nReuter\\n&#3;\"]\n",
      "['acq', 'acq']\n"
     ]
    }
   ],
   "source": [
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape= (9465, 100000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,2))\n",
    "feature_matrix=vectorizer.fit_transform(texts)\n",
    "print(\"shape=\",feature_matrix.shape)\n",
    "#print(feature_matrix.todense())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature matrix done! Next thing we need is the class labels to be predicted in one-hot encoding. This means:\n",
    "\n",
    "* one row for every example\n",
    "* one column for every possible class label\n",
    "* exactly one column has 1 for every example, corresponding to the desired class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_numbers shape= (9465,)\n",
      "class_numbers [ 0  0  0 ... 11 11 46]\n",
      "class labels ['acq' 'alum' 'bop' 'carcass' 'cocoa' 'coffee' 'copper' 'cotton' 'cpi'\n",
      " 'crude' 'dlr' 'earn' 'fuel' 'gas' 'gnp' 'gold' 'grain' 'heat' 'housing'\n",
      " 'income' 'instal-debt' 'interest' 'ipi' 'iron-steel' 'jobs' 'lead' 'lei'\n",
      " 'livestock' 'lumber' 'meal-feed' 'money-fx' 'money-supply' 'nat-gas'\n",
      " 'oilseed' 'orange' 'pet-chem' 'potato' 'reserves' 'retail' 'rubber'\n",
      " 'ship' 'silver' 'strategic-metal' 'sugar' 'tea' 'tin' 'trade' 'veg-oil'\n",
      " 'wpi' 'yen' 'zinc']\n",
      "classes_1hot [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "one_hot_encoder=OneHotEncoder(sparse=False) #Turns class integers into one-hot encoding\n",
    "class_numbers=label_encoder.fit_transform(labels)\n",
    "print(\"class_numbers shape=\",class_numbers.shape)\n",
    "print(\"class_numbers\",class_numbers)\n",
    "print(\"class labels\",label_encoder.classes_)\n",
    "#And now yet the one-hot encoding\n",
    "classes_1hot=one_hot_encoder.fit_transform(class_numbers.reshape(-1,1))\n",
    "print(\"classes_1hot\",classes_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ginter/venv-jupyter/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8518 samples, validate on 947 samples\n",
      "Epoch 1/30\n",
      "8518/8518 [==============================] - 7s 833us/step - loss: 2.5036 - acc: 0.5393 - val_loss: 1.8087 - val_acc: 0.6378\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.80867, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 2/30\n",
      "8518/8518 [==============================] - 7s 808us/step - loss: 1.6587 - acc: 0.6363 - val_loss: 1.5114 - val_acc: 0.6600\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.80867 to 1.51141, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 3/30\n",
      "8518/8518 [==============================] - 6s 736us/step - loss: 1.4479 - acc: 0.6723 - val_loss: 1.3701 - val_acc: 0.6917\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.51141 to 1.37008, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 4/30\n",
      "8518/8518 [==============================] - 5s 636us/step - loss: 1.3242 - acc: 0.6941 - val_loss: 1.2765 - val_acc: 0.7096\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.37008 to 1.27653, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 5/30\n",
      "8518/8518 [==============================] - 5s 634us/step - loss: 1.2332 - acc: 0.7166 - val_loss: 1.2056 - val_acc: 0.7392\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.27653 to 1.20564, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 6/30\n",
      "8518/8518 [==============================] - 5s 645us/step - loss: 1.1598 - acc: 0.7369 - val_loss: 1.1485 - val_acc: 0.7571\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.20564 to 1.14851, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 7/30\n",
      "8518/8518 [==============================] - 5s 637us/step - loss: 1.0971 - acc: 0.7549 - val_loss: 1.1003 - val_acc: 0.7687\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.14851 to 1.10025, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 8/30\n",
      "8518/8518 [==============================] - 5s 636us/step - loss: 1.0426 - acc: 0.7733 - val_loss: 1.0576 - val_acc: 0.7856\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.10025 to 1.05764, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 9/30\n",
      "8518/8518 [==============================] - 7s 784us/step - loss: 0.9936 - acc: 0.7890 - val_loss: 1.0199 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.05764 to 1.01991, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 10/30\n",
      "8518/8518 [==============================] - 6s 673us/step - loss: 0.9492 - acc: 0.8022 - val_loss: 0.9844 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.01991 to 0.98439, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 11/30\n",
      "8518/8518 [==============================] - 5s 637us/step - loss: 0.9084 - acc: 0.8150 - val_loss: 0.9533 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.98439 to 0.95330, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 12/30\n",
      "8518/8518 [==============================] - 6s 678us/step - loss: 0.8710 - acc: 0.8270 - val_loss: 0.9249 - val_acc: 0.8184\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.95330 to 0.92491, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 13/30\n",
      "8518/8518 [==============================] - 5s 637us/step - loss: 0.8364 - acc: 0.8332 - val_loss: 0.8983 - val_acc: 0.8215\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.92491 to 0.89827, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 14/30\n",
      "8518/8518 [==============================] - 5s 636us/step - loss: 0.8044 - acc: 0.8406 - val_loss: 0.8738 - val_acc: 0.8247\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.89827 to 0.87378, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 15/30\n",
      "8518/8518 [==============================] - 5s 636us/step - loss: 0.7745 - acc: 0.8475 - val_loss: 0.8507 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.87378 to 0.85067, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 16/30\n",
      "8518/8518 [==============================] - 5s 636us/step - loss: 0.7464 - acc: 0.8528 - val_loss: 0.8294 - val_acc: 0.8332\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.85067 to 0.82944, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 17/30\n",
      "8518/8518 [==============================] - 8s 956us/step - loss: 0.7203 - acc: 0.8585 - val_loss: 0.8101 - val_acc: 0.8353\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.82944 to 0.81013, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 18/30\n",
      "8518/8518 [==============================] - 6s 660us/step - loss: 0.6955 - acc: 0.8632 - val_loss: 0.7915 - val_acc: 0.8384\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.81013 to 0.79150, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 19/30\n",
      "8518/8518 [==============================] - 6s 694us/step - loss: 0.6723 - acc: 0.8662 - val_loss: 0.7741 - val_acc: 0.8384\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.79150 to 0.77411, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 20/30\n",
      "8518/8518 [==============================] - 6s 747us/step - loss: 0.6501 - acc: 0.8703 - val_loss: 0.7578 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.77411 to 0.75778, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 21/30\n",
      "8518/8518 [==============================] - 6s 675us/step - loss: 0.6293 - acc: 0.8739 - val_loss: 0.7427 - val_acc: 0.8458\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.75778 to 0.74272, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 22/30\n",
      "8518/8518 [==============================] - 5s 639us/step - loss: 0.6097 - acc: 0.8773 - val_loss: 0.7282 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.74272 to 0.72823, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 23/30\n",
      "8518/8518 [==============================] - 5s 639us/step - loss: 0.5910 - acc: 0.8812 - val_loss: 0.7145 - val_acc: 0.8522\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.72823 to 0.71453, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 24/30\n",
      "8518/8518 [==============================] - 5s 637us/step - loss: 0.5733 - acc: 0.8861 - val_loss: 0.7014 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.71453 to 0.70142, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 25/30\n",
      "8518/8518 [==============================] - 6s 661us/step - loss: 0.5562 - acc: 0.8901 - val_loss: 0.6894 - val_acc: 0.8553\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.70142 to 0.68941, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 26/30\n",
      "8518/8518 [==============================] - 5s 638us/step - loss: 0.5402 - acc: 0.8932 - val_loss: 0.6777 - val_acc: 0.8553\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.68941 to 0.67770, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 27/30\n",
      "8518/8518 [==============================] - 7s 819us/step - loss: 0.5248 - acc: 0.8988 - val_loss: 0.6666 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.67770 to 0.66660, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 28/30\n",
      "8518/8518 [==============================] - 5s 643us/step - loss: 0.5100 - acc: 0.9020 - val_loss: 0.6561 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.66660 to 0.65612, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 29/30\n",
      "8518/8518 [==============================] - 5s 644us/step - loss: 0.4960 - acc: 0.9046 - val_loss: 0.6462 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.65612 to 0.64619, saving model to models/reuters_51cls_bow.weights.h5\n",
      "Epoch 30/30\n",
      "8518/8518 [==============================] - 5s 637us/step - loss: 0.4826 - acc: 0.9074 - val_loss: 0.6363 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.64619 to 0.63632, saving model to models/reuters_51cls_bow.weights.h5\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def save_model(file_name,model,label_encoder,vectorizer):\n",
    "    \"\"\"Saves model structure and vocabularies\"\"\"\n",
    "    model_json = model.to_json()\n",
    "    with open(file_name+\".model.json\", \"w\") as f:\n",
    "        print(model_json,file=f)\n",
    "    with open(file_name+\".vocabularies.json\",\"w\") as f:\n",
    "        classes=list(label_encoder.classes_)\n",
    "        vocab=dict(((str(w),int(idx)) for w,idx in vectorizer.vocabulary_.items()))\n",
    "        json.dump((classes,vocab),f,indent=2)\n",
    "        \n",
    "example_count,feature_count=feature_matrix.shape\n",
    "example_count,class_count=classes_1hot.shape\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(200,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# Save model and vocabularies\n",
    "save_model(\"models/reuters_51cls_bow\",model,label_encoder,vectorizer)\n",
    "# Callback function to save weights during training\n",
    "save_cb=ModelCheckpoint(filepath=\"models/reuters_51cls_bow.weights.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "hist=model.fit(feature_matrix,classes_1hot,batch_size=100,verbose=1,epochs=30,validation_split=0.1,callbacks=[save_cb])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "venv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
