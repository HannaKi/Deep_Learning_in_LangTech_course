{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "bowcls_demo_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/Deep_Learning_in_LangTech_course/blob/master/language_classifier_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwOk0plcmZez",
        "colab_type": "text"
      },
      "source": [
        "# Compare SVM and NN with language classification \n",
        "\n",
        "Course data folder language_identification contains data for 5 languages. Based on this data\n",
        "* Train an SVM classifier for language recognition between these 5 languages. (Previously done!)\n",
        "* Implement this same classifier using a simple NN\n",
        "* Compare the results you get with NN and SVM Focus on experimenting with the various NN parameters of learning (learning rate, optimizer, etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwuacC9wl6L5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the data in makes sense to structure a little bit\n",
        "# ratkaise, miten kansio tuodaan omasta GitHubista!\n",
        "# toimii myös luomalla Colabiin kansion (tässä nimeltä \"texts\"), \n",
        "# jonne tiedostot raahaa (kansio katoaa, kun ajo päättyy)\n",
        "\n",
        "import random\n",
        "\n",
        "def read_data_one_lang(lang,part):\n",
        "    \"\"\"Reads one file for one language. Returns data in the form of pairs of (lang,line)\"\"\"\n",
        "    filename=\"texts/{}_{}.txt\".format(lang,part)\n",
        "    result=[] #this will be the list of pairs (lang,line)\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            result.append((lang,line)) \n",
        "    return result\n",
        "\n",
        "\n",
        "def read_data_all_langs(part):\n",
        "    \"\"\"Reads train, test or dev data for all languages. part can be train, test, or devel\"\"\"\n",
        "    data=[]\n",
        "    for lang in (\"en\",\"es\",\"et\",\"fi\",\"pt\"):\n",
        "        pairs=read_data_one_lang(lang,part)\n",
        "        data.extend(pairs) #just add these lines to the end\n",
        "    #...done\n",
        "    #but now they come in the order of languages\n",
        "    #we really must scramble these!\n",
        "    random.shuffle(data)\n",
        "    \n",
        "    #let's yet separate the labels and lines, we will need that anyway\n",
        "    labels=[label for label,line in data]\n",
        "    lines=[line for label,line in data]\n",
        "    return labels,lines\n",
        "\n",
        "labels_train, lines_train=read_data_all_langs(\"train\") # train and test data splitting already done!\n",
        "labels_dev,lines_dev=read_data_all_langs(\"devel\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP6vO5awPFGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "b272a950-d13f-42b4-a8ec-74f4c4f78d2a"
      },
      "source": [
        "for label,line in zip(labels_train[:5],lines_train[:5]):\n",
        "    print(label,\"   \",line[:30],\"...\")\n",
        "\n",
        "print(labels_train[0], lines_train[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "en     2. In Section 3.1, in the last ...\n",
            "et     \" KUMMELI TEE\". ...\n",
            "pt     « Psicologia, poesia» ...\n",
            "es     La temporada siguiente, sin Ma ...\n",
            "pt     Estavam repletas de lixo, copo ...\n",
            "en 2. In Section 3.1, in the last sentence after the proviso insert\"a\" before the word\"change\" and after the word\"in\".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN1szyCVLlXc",
        "colab_type": "text"
      },
      "source": [
        "# Reminder\n",
        "\n",
        "Feature matrix has row for each document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfVT4fZOl6MN",
        "colab_type": "code",
        "outputId": "9fa28d09-e529-450a-dbaa-aed6db89581e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sklearn.svm\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=100000, binary=True, ngram_range=(1,1))\n",
        "\n",
        "feature_matrix_train = vectorizer.fit_transform(lines_train) \n",
        "# .fit_transform: Learn the vocabulary dictionary and return term-document matrix.\n",
        "feature_matrix_dev = vectorizer.transform(lines_dev)\n",
        "# .transform: Transform documents to document-term matrix.\n",
        "\n",
        "# print(vectorizer.get_feature_names()) # Words (or ngrams) in learned vocabulary, a HUGE list!\n",
        "print(\"Number of rows (documens) and unique ngrams in feature matrix\")\n",
        "print(feature_matrix_train.shape) \n",
        "print()\n",
        "print(\"Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\")\n",
        "print(feature_matrix_train.toarray())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows (documens) and unique ngrams in feature matrix\n",
            "(5000, 28620)\n",
            "\n",
            "Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjEZcvrqLd6T",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I09AyZhsLcfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "7e4b1273-3744-4566-e448-b39405d9d8a7"
      },
      "source": [
        "for C in (0.001,0.01,0.1,1,10,100):\n",
        "    classifier =  sklearn.svm.LinearSVC(C=C)\n",
        "    classifier.fit(feature_matrix_train, labels_train)\n",
        "    print(\"C=\",C,\"     \",classifier.score(feature_matrix_dev, labels_dev))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C= 0.001       0.8758\n",
            "C= 0.01       0.9144\n",
            "C= 0.1       0.933\n",
            "C= 1       0.9302\n",
            "C= 10       0.9102\n",
            "C= 100       0.8728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCwTZxCl6Mc",
        "colab_type": "text"
      },
      "source": [
        "* 93% accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF3JQwVsWBlT",
        "colab_type": "text"
      },
      "source": [
        "# NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIb32lm1mdSt",
        "colab_type": "text"
      },
      "source": [
        "For NN we will combine the data and separate a proportion of it for validation when fitting the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag9L8TEjl4W6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines =  lines_train + lines_dev\n",
        "labels = labels_train + labels_dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgk8GjZceOl",
        "colab_type": "text"
      },
      "source": [
        "For NN we need to encode each class to numeric value.\n",
        " * Remember[ difference between encoding vs. one hot encoding](https://https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZojDVNIUWBGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d9e792a3-7e9f-4ce8-cd4f-a5bb1e74e6d0"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder() #Turns class labels into integers\n",
        "class_numbers = label_encoder.fit_transform(labels)\n",
        "\n",
        "print(class_numbers)\n",
        "print(\"class_numbers shape=\",class_numbers.shape)\n",
        "print(\"class labels\",label_encoder.classes_) #this will let us translate back from indices to labels"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 2 4 ... 2 3 1]\n",
            "class_numbers shape= (10000,)\n",
            "class labels ['en' 'es' 'et' 'fi' 'pt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3v5Wm_hmvRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "bc552947-68f9-4198-ca8b-35cdd5370db2"
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=100000, binary=True, ngram_range=(1,1))\n",
        "\n",
        "feature_matrix = vectorizer.fit_transform(lines) \n",
        "# .fit_transform: Learn the vocabulary dictionary and return term-document matrix.\n",
        "\n",
        "# print(vectorizer.get_feature_names()) # Words (or ngrams) in learned vocabulary, a HUGE list!\n",
        "print(\"Number of rows (documens) and unique ngrams in feature matrix\")\n",
        "print(feature_matrix.shape) \n",
        "print()\n",
        "print(\"Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\")\n",
        "print(feature_matrix.toarray())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows (documens) and unique ngrams in feature matrix\n",
            "(10000, 46875)\n",
            "\n",
            "Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlD3w6R1iBoP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c52c85be-451b-4805-ecbe-62b502263ee1"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "example_count, feature_count = feature_matrix.shape\n",
        "class_count = len(label_encoder.classes_)\n",
        "\n",
        "inp = Input(shape=(feature_count, )) # tuple\n",
        "hidden = Dense(200, activation=\"tanh\")(inp) # taalla kaytetty tanh. Relu suositumpi? \n",
        "# Jos mitaan aktivointifunktiota ei anneta, tulee syotteen ja kertoimien lineaarinen matriisitulo \n",
        "outp = Dense(class_count, activation=\"softmax\")(hidden) # softmax: tuottaa luokkien jakauman\n",
        "model = Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f5e39acce80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCAt8NK8jrTu",
        "colab_type": "text"
      },
      "source": [
        "Once the model is constructed it needs to be compiled, for that we need to know:\n",
        "* which optimizer we want to use (sgd is fine to begin with)\n",
        "* what is the loss (categorial_crossentropy for multiclass of the kind we have is the right choice)\n",
        "* which metrics to measure, accuracy is an okay choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyQlvJT0jqFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nozNLSNrXMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "3aeedad4-2fee-447b-e5f9-c17ebf008422"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# batch_size kuinka monta inputtia kerralla sisaan. jokaisen batchin jalkeen paivitetaan painokertoimet gradientien keskiarvolla\n",
        "# epochs kuinka monta kertaa mennaan lapi koko data\n",
        "# validation_split: kuinka paljon dataa kaytetaan accuracyn laskemiseen\n",
        "\n",
        "# Callback to stop training when no improvement\n",
        "stop_cb=EarlyStopping(monitor='val_acc', patience=2, verbose=1, mode='auto', baseline=None, \n",
        "                      restore_best_weights=True)\n",
        "\n",
        "hist=model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=100,\n",
        "               validation_split=0.1, callbacks=[stop_cb])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "9000/9000 [==============================] - 5s 594us/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1103 - val_acc: 0.9650\n",
            "Epoch 2/100\n",
            "9000/9000 [==============================] - 5s 596us/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1097 - val_acc: 0.9660\n",
            "Epoch 3/100\n",
            "9000/9000 [==============================] - 5s 594us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1104 - val_acc: 0.9670\n",
            "Epoch 4/100\n",
            "9000/9000 [==============================] - 5s 598us/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1139 - val_acc: 0.9660\n",
            "Epoch 5/100\n",
            "9000/9000 [==============================] - 5s 596us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1140 - val_acc: 0.9650\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plicePXSXTGe",
        "colab_type": "text"
      },
      "source": [
        "Accuracy 96.7 %\n",
        "\n",
        "Experiment with the various parameters of learning (learning rate, optimizer, etc). See [Keras API](https://keras.io/optimizers/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxvmEesywa8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=sgd, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=100,\n",
        "               validation_split=0.1, callbacks=[stop_cb])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}