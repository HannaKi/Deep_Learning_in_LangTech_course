{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "bowcls_demo_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/Deep_Learning_in_LangTech_course/blob/master/language_classifier_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwOk0plcmZez",
        "colab_type": "text"
      },
      "source": [
        "# Compare SVM and NN with language classification \n",
        "\n",
        "Course data folder language_identification contains data for 5 languages. Based on this data\n",
        "* Train an SVM classifier for language recognition between these 5 languages. (Previously done!)\n",
        "* Implement this same classifier using a simple NN\n",
        "* Compare the results you get with NN and SVM Focus on experimenting with the various NN parameters of learning (learning rate, optimizer, etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwuacC9wl6L5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the data in makes sense to structure a little bit\n",
        "# ratkaise, miten kansio tuodaan omasta GitHubista!\n",
        "# toimii myös luomalla Colabiin kansion (tässä nimeltä \"texts\"), \n",
        "# jonne tiedostot raahaa (kansio katoaa, kun ajo päättyy)\n",
        "\n",
        "import random\n",
        "\n",
        "def read_data_one_lang(lang,part):\n",
        "    \"\"\"Reads one file for one language. Returns data in the form of pairs of (lang,line)\"\"\"\n",
        "    filename=\"texts/{}_{}.txt\".format(lang,part)\n",
        "    result=[] #this will be the list of pairs (lang,line)\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            result.append((lang,line)) \n",
        "    return result\n",
        "\n",
        "\n",
        "def read_data_all_langs(part):\n",
        "    \"\"\"Reads train, test or dev data for all languages. part can be train, test, or devel\"\"\"\n",
        "    data=[]\n",
        "    for lang in (\"en\",\"es\",\"et\",\"fi\",\"pt\"):\n",
        "        pairs=read_data_one_lang(lang,part)\n",
        "        data.extend(pairs) #just add these lines to the end\n",
        "    #...done\n",
        "    #but now they come in the order of languages\n",
        "    #we really must scramble these!\n",
        "    random.shuffle(data)\n",
        "    \n",
        "    #let's yet separate the labels and lines, we will need that anyway\n",
        "    labels=[label for label,line in data]\n",
        "    lines=[line for label,line in data]\n",
        "    return labels,lines\n",
        "\n",
        "labels_train, lines_train=read_data_all_langs(\"train\") # train and test data splitting already done!\n",
        "labels_dev,lines_dev=read_data_all_langs(\"devel\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP6vO5awPFGr",
        "colab_type": "code",
        "outputId": "adc186b5-1a45-48f7-9c5d-2401423329c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "for label,line in zip(labels_train[:5],lines_train[:5]):\n",
        "    print(label,\"   \",line[:30],\"...\")\n",
        "\n",
        "print(labels_train[0], lines_train[0])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pt     « Quando fui autarca e fazia r ...\n",
            "es     En una de sus cacerías perdier ...\n",
            "pt     Total de mortos em 92: 310 ...\n",
            "pt     O cartão TVA vai permitir a se ...\n",
            "en     Thanks, Jean ...\n",
            "pt « Quando fui autarca e fazia realojamentos era normal que quem não conseguia logo uma habitação ficasse insatisfeito.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN1szyCVLlXc",
        "colab_type": "text"
      },
      "source": [
        "# Reminder\n",
        "\n",
        "Feature matrix has row for each document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfVT4fZOl6MN",
        "colab_type": "code",
        "outputId": "72e2f1ee-a731-43e8-c309-f1ad47e4f4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sklearn.svm\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=100000, binary=True, ngram_range=(1,1))\n",
        "\n",
        "feature_matrix_train = vectorizer.fit_transform(lines_train) \n",
        "# .fit_transform: Learn the vocabulary dictionary and return term-document matrix.\n",
        "feature_matrix_dev = vectorizer.transform(lines_dev)\n",
        "# .transform: Transform documents to document-term matrix.\n",
        "\n",
        "# print(vectorizer.get_feature_names()) # Words (or ngrams) in learned vocabulary, a HUGE list!\n",
        "print(\"Number of rows (documens) and unique ngrams in feature matrix\")\n",
        "print(feature_matrix_train.shape) \n",
        "print()\n",
        "print(\"Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\")\n",
        "print(feature_matrix_train.toarray())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows (documens) and unique ngrams in feature matrix\n",
            "(5000, 28620)\n",
            "\n",
            "Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjEZcvrqLd6T",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I09AyZhsLcfu",
        "colab_type": "code",
        "outputId": "bd30b19a-8315-4fbb-9ed1-7f8f9072a604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "for C in (0.001,0.01,0.1,1,10,100):\n",
        "    classifier =  sklearn.svm.LinearSVC(C=C)\n",
        "    classifier.fit(feature_matrix_train, labels_train)\n",
        "    print(\"C=\",C,\"     \",classifier.score(feature_matrix_dev, labels_dev))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C= 0.001       0.8758\n",
            "C= 0.01       0.9144\n",
            "C= 0.1       0.933\n",
            "C= 1       0.9302\n",
            "C= 10       0.9102\n",
            "C= 100       0.8728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCwTZxCl6Mc",
        "colab_type": "text"
      },
      "source": [
        "* 93% accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF3JQwVsWBlT",
        "colab_type": "text"
      },
      "source": [
        "# NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIb32lm1mdSt",
        "colab_type": "text"
      },
      "source": [
        "For NN we will combine the data and separate a proportion of it for validation when fitting the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag9L8TEjl4W6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines =  lines_train + lines_dev\n",
        "labels = labels_train + labels_dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgk8GjZceOl",
        "colab_type": "text"
      },
      "source": [
        "For NN we need to encode each class to numeric value.\n",
        " * Remember[ difference between encoding vs. one hot encoding](https://https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZojDVNIUWBGM",
        "colab_type": "code",
        "outputId": "afbc1a48-11a0-45fc-b4ea-610737bf53c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder() #Turns class labels into integers\n",
        "class_numbers = label_encoder.fit_transform(labels)\n",
        "\n",
        "print(class_numbers)\n",
        "print(\"class_numbers shape=\",class_numbers.shape)\n",
        "print(\"class labels\",label_encoder.classes_) #this will let us translate back from indices to labels"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 1 4 ... 1 4 0]\n",
            "class_numbers shape= (10000,)\n",
            "class labels ['en' 'es' 'et' 'fi' 'pt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3v5Wm_hmvRL",
        "colab_type": "code",
        "outputId": "5dac380e-2299-4fe9-d22c-cc7db297e70d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=100000, binary=True, ngram_range=(1,1))\n",
        "\n",
        "feature_matrix = vectorizer.fit_transform(lines) \n",
        "# .fit_transform: Learn the vocabulary dictionary and return term-document matrix.\n",
        "\n",
        "# print(vectorizer.get_feature_names()) # Words (or ngrams) in learned vocabulary, a HUGE list!\n",
        "print(\"Number of rows (documens) and unique ngrams in feature matrix\")\n",
        "print(feature_matrix.shape) \n",
        "print()\n",
        "print(\"Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\")\n",
        "print(feature_matrix.toarray())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows (documens) and unique ngrams in feature matrix\n",
            "(10000, 46875)\n",
            "\n",
            "Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlD3w6R1iBoP",
        "colab_type": "code",
        "outputId": "479f449a-6a55-468b-84bc-f30fd3044f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "example_count, feature_count = feature_matrix.shape\n",
        "class_count = len(label_encoder.classes_)\n",
        "\n",
        "inp = Input(shape=(feature_count, )) # tuple\n",
        "hidden = Dense(200, activation=\"tanh\")(inp) # taalla kaytetty tanh. Relu suositumpi? \n",
        "# Jos mitaan aktivointifunktiota ei anneta, tulee syotteen ja kertoimien lineaarinen matriisitulo \n",
        "outp = Dense(class_count, activation=\"softmax\")(hidden) # softmax: tuottaa luokkien jakauman\n",
        "model = Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f6bd1e23860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCAt8NK8jrTu",
        "colab_type": "text"
      },
      "source": [
        "Once the model is constructed it needs to be compiled, for that we need to know:\n",
        "* which optimizer we want to use (sgd is fine to begin with)\n",
        "* what is the loss (categorial_crossentropy for multiclass of the kind we have is the right choice)\n",
        "* which metrics to measure, accuracy is an okay choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyQlvJT0jqFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nozNLSNrXMB",
        "colab_type": "code",
        "outputId": "cc32dbb7-d879-4279-f5f0-22d5ec5bfc9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# batch_size kuinka monta inputtia kerralla sisaan. jokaisen batchin jalkeen paivitetaan painokertoimet gradientien keskiarvolla\n",
        "# epochs kuinka monta kertaa mennaan lapi koko data\n",
        "# validation_split: kuinka paljon dataa kaytetaan accuracyn laskemiseen\n",
        "\n",
        "# Callback to stop training when no improvement\n",
        "stop_cb=EarlyStopping(monitor='val_acc', patience=2, verbose=1, mode='auto', baseline=None, \n",
        "                      restore_best_weights=True)\n",
        "\n",
        "hist=model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=100,\n",
        "               validation_split=0.1, callbacks=[stop_cb])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "9000/9000 [==============================] - 7s 731us/step - loss: 0.5599 - acc: 0.9226 - val_loss: 0.2110 - val_acc: 0.9590\n",
            "Epoch 2/100\n",
            "9000/9000 [==============================] - 5s 589us/step - loss: 0.1062 - acc: 0.9889 - val_loss: 0.1509 - val_acc: 0.9730\n",
            "Epoch 3/100\n",
            "9000/9000 [==============================] - 4s 470us/step - loss: 0.0468 - acc: 0.9973 - val_loss: 0.1315 - val_acc: 0.9680\n",
            "Epoch 4/100\n",
            "9000/9000 [==============================] - 4s 464us/step - loss: 0.0251 - acc: 0.9990 - val_loss: 0.1229 - val_acc: 0.9630\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plicePXSXTGe",
        "colab_type": "text"
      },
      "source": [
        "Experiment with the various parameters of learning (learning rate, optimizer, etc). See [Keras API](https://keras.io/optimizers/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxvmEesywa8D",
        "colab_type": "code",
        "outputId": "030226e0-f072-4015-9cd3-9fb761a60908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=sgd, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=10,\n",
        "               validation_split=0.1, callbacks=[stop_cb])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 5s 585us/step - loss: 0.0559 - acc: 0.9977 - val_loss: 0.1508 - val_acc: 0.9730\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 446us/step - loss: 0.0555 - acc: 0.9977 - val_loss: 0.1506 - val_acc: 0.9730\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 436us/step - loss: 0.0552 - acc: 0.9978 - val_loss: 0.1504 - val_acc: 0.9730\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUcEdD0vG2by",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2c8f05a-0974-4cb9-ccab-2d5e3f9aa78d"
      },
      "source": [
        "# ops_param = np.array([['Adadelta', 50.0, 'b'],\n",
        "#                      ['Adagrad', 0.10, 'g'],\n",
        "#                      ['Adam', 0.05, 'r'],\n",
        "#                      ['Ftrl', 0.5, 'c'],\n",
        "#                      ['SGD', 0.05, 'm'],\n",
        "#                      ['Momentum', 0.01, 'y'],\n",
        "#                      ['RMSProp', 0.02, 'k'],\n",
        "#                       ['Adamax'],\n",
        "#                       'Nadam'])\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "lrs = np.logspace(-10, 10, base = 2, num = 20)\n",
        "\n",
        "# import tensorflow as tf\n",
        "# ops = ('Adadelta', 'Adagrad', 'Adam', 'Ftrl', 'SGD', 'Momentum', 'RMSProp', 'Adamax', 'Nadam')\n",
        "# Why does the code crash with ops abowe? Ftrl and Momentum wont work although they are \n",
        "# documented optimizers\n",
        "# and used in here: https://github.com/Jaewan-Yun/optimizer-visualization/blob/master/optimizer_visualization.py\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl\n",
        "ops = ('Adadelta', 'Adagrad', 'Adam', 'SGD', 'RMSProp', 'Adamax', 'Nadam')\n",
        "\n",
        "for op in ops:\n",
        "  print(op)\n",
        "  print()\n",
        "  model.compile(optimizer = op, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  hist = model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=10,\n",
        "              validation_split=0.1, callbacks=[stop_cb])\n",
        "  print()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adadelta\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 6s 625us/step - loss: 0.0470 - acc: 0.9973 - val_loss: 0.1313 - val_acc: 0.9620\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 456us/step - loss: 0.0349 - acc: 0.9974 - val_loss: 0.1215 - val_acc: 0.9650\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 464us/step - loss: 0.0281 - acc: 0.9979 - val_loss: 0.1120 - val_acc: 0.9690\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 4s 462us/step - loss: 0.0235 - acc: 0.9984 - val_loss: 0.1078 - val_acc: 0.9670\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 4s 456us/step - loss: 0.0200 - acc: 0.9986 - val_loss: 0.1067 - val_acc: 0.9640\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00005: early stopping\n",
            "\n",
            "Adagrad\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 5s 580us/step - loss: 0.0190 - acc: 0.9970 - val_loss: 0.0922 - val_acc: 0.9630\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 432us/step - loss: 0.0063 - acc: 0.9993 - val_loss: 0.0921 - val_acc: 0.9630\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 433us/step - loss: 0.0042 - acc: 0.9996 - val_loss: 0.0945 - val_acc: 0.9620\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n",
            "\n",
            "Adam\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 5s 592us/step - loss: 0.0062 - acc: 0.9991 - val_loss: 0.0940 - val_acc: 0.9670\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 444us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 0.0987 - val_acc: 0.9620\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 442us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1003 - val_acc: 0.9610\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n",
            "\n",
            "SGD\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 5s 559us/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0940 - val_acc: 0.9670\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 425us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0939 - val_acc: 0.9680\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 419us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0939 - val_acc: 0.9690\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 4s 422us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0939 - val_acc: 0.9710\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 4s 435us/step - loss: 0.0036 - acc: 0.9991 - val_loss: 0.0940 - val_acc: 0.9700\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 4s 421us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 0.0940 - val_acc: 0.9670\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "RMSProp\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 724us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 0.0959 - val_acc: 0.9660\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 5s 580us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0952 - val_acc: 0.9680\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 5s 582us/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1067 - val_acc: 0.9590\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 5s 578us/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.1135 - val_acc: 0.9600\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n",
            "Adamax\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 737us/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1021 - val_acc: 0.9640\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 5s 581us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1058 - val_acc: 0.9620\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 5s 578us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1050 - val_acc: 0.9630\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n",
            "\n",
            "Nadam\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 784us/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.1110 - val_acc: 0.9600\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 6s 613us/step - loss: 0.0010 - acc: 0.9996 - val_loss: 0.1221 - val_acc: 0.9580\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 5s 609us/step - loss: 8.6510e-04 - acc: 0.9998 - val_loss: 0.1272 - val_acc: 0.9530\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}