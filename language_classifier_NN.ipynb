{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "bowcls_demo_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/Deep_Learning_in_LangTech_course/blob/master/language_classifier_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwOk0plcmZez",
        "colab_type": "text"
      },
      "source": [
        "# Compare SVM and NN with language classification \n",
        "\n",
        "Course data folder language_identification contains data for 5 languages. Based on this data\n",
        "* Train an SVM classifier for language recognition between these 5 languages. (Previously done!)\n",
        "* Implement this same classifier using a simple NN\n",
        "* Compare the results you get with NN and SVM Focus on experimenting with the various NN parameters of learning (learning rate, optimizer, etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwuacC9wl6L5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the data in makes sense to structure a little bit\n",
        "# ratkaise, miten kansio tuodaan omasta GitHubista!\n",
        "# toimii myös luomalla Colabiin kansion (tässä nimeltä \"texts\"), \n",
        "# jonne tiedostot raahaa (kansio katoaa, kun ajo päättyy)\n",
        "\n",
        "import random\n",
        "\n",
        "def read_data_one_lang(lang,part):\n",
        "    \"\"\"Reads one file for one language. Returns data in the form of pairs of (lang,line)\"\"\"\n",
        "    filename=\"texts/{}_{}.txt\".format(lang,part)\n",
        "    result=[] #this will be the list of pairs (lang,line)\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            result.append((lang,line)) \n",
        "    return result\n",
        "\n",
        "\n",
        "def read_data_all_langs(part):\n",
        "    \"\"\"Reads train, test or dev data for all languages. part can be train, test, or devel\"\"\"\n",
        "    data=[]\n",
        "    for lang in (\"en\",\"es\",\"et\",\"fi\",\"pt\"):\n",
        "        pairs=read_data_one_lang(lang,part)\n",
        "        data.extend(pairs) #just add these lines to the end\n",
        "    #...done\n",
        "    #but now they come in the order of languages\n",
        "    #we really must scramble these!\n",
        "    random.shuffle(data)\n",
        "    \n",
        "    #let's yet separate the labels and lines, we will need that anyway\n",
        "    labels=[label for label,line in data]\n",
        "    lines=[line for label,line in data]\n",
        "    return labels,lines\n",
        "\n",
        "labels_train, lines_train=read_data_all_langs(\"train\") # train and test data splitting already done!\n",
        "labels_dev,lines_dev=read_data_all_langs(\"devel\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP6vO5awPFGr",
        "colab_type": "code",
        "outputId": "adc186b5-1a45-48f7-9c5d-2401423329c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "for label,line in zip(labels_train[:5],lines_train[:5]):\n",
        "    print(label,\"   \",line[:30],\"...\")\n",
        "\n",
        "print(labels_train[0], lines_train[0])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pt     « Quando fui autarca e fazia r ...\n",
            "es     En una de sus cacerías perdier ...\n",
            "pt     Total de mortos em 92: 310 ...\n",
            "pt     O cartão TVA vai permitir a se ...\n",
            "en     Thanks, Jean ...\n",
            "pt « Quando fui autarca e fazia realojamentos era normal que quem não conseguia logo uma habitação ficasse insatisfeito.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN1szyCVLlXc",
        "colab_type": "text"
      },
      "source": [
        "# Reminder\n",
        "\n",
        "Feature matrix has row for each document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfVT4fZOl6MN",
        "colab_type": "code",
        "outputId": "72e2f1ee-a731-43e8-c309-f1ad47e4f4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sklearn.svm\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=100000, binary=True, ngram_range=(1,1))\n",
        "\n",
        "feature_matrix_train = vectorizer.fit_transform(lines_train) \n",
        "# .fit_transform: Learn the vocabulary dictionary and return term-document matrix.\n",
        "feature_matrix_dev = vectorizer.transform(lines_dev)\n",
        "# .transform: Transform documents to document-term matrix.\n",
        "\n",
        "# print(vectorizer.get_feature_names()) # Words (or ngrams) in learned vocabulary, a HUGE list!\n",
        "print(\"Number of rows (documens) and unique ngrams in feature matrix\")\n",
        "print(feature_matrix_train.shape) \n",
        "print()\n",
        "print(\"Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\")\n",
        "print(feature_matrix_train.toarray())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows (documens) and unique ngrams in feature matrix\n",
            "(5000, 28620)\n",
            "\n",
            "Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjEZcvrqLd6T",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I09AyZhsLcfu",
        "colab_type": "code",
        "outputId": "bd30b19a-8315-4fbb-9ed1-7f8f9072a604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "for C in (0.001,0.01,0.1,1,10,100):\n",
        "    classifier =  sklearn.svm.LinearSVC(C=C)\n",
        "    classifier.fit(feature_matrix_train, labels_train)\n",
        "    print(\"C=\",C,\"     \",classifier.score(feature_matrix_dev, labels_dev))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C= 0.001       0.8758\n",
            "C= 0.01       0.9144\n",
            "C= 0.1       0.933\n",
            "C= 1       0.9302\n",
            "C= 10       0.9102\n",
            "C= 100       0.8728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCwTZxCl6Mc",
        "colab_type": "text"
      },
      "source": [
        "* 93% accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF3JQwVsWBlT",
        "colab_type": "text"
      },
      "source": [
        "# NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIb32lm1mdSt",
        "colab_type": "text"
      },
      "source": [
        "For NN we will combine the data and separate a proportion of it for validation when fitting the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag9L8TEjl4W6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines =  lines_train + lines_dev\n",
        "labels = labels_train + labels_dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgk8GjZceOl",
        "colab_type": "text"
      },
      "source": [
        "For NN we need to encode each class to numeric value.\n",
        " * Remember[ difference between encoding vs. one hot encoding](https://https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZojDVNIUWBGM",
        "colab_type": "code",
        "outputId": "afbc1a48-11a0-45fc-b4ea-610737bf53c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder() #Turns class labels into integers\n",
        "class_numbers = label_encoder.fit_transform(labels)\n",
        "\n",
        "print(class_numbers)\n",
        "print(\"class_numbers shape=\",class_numbers.shape)\n",
        "print(\"class labels\",label_encoder.classes_) #this will let us translate back from indices to labels"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 1 4 ... 1 4 0]\n",
            "class_numbers shape= (10000,)\n",
            "class labels ['en' 'es' 'et' 'fi' 'pt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3v5Wm_hmvRL",
        "colab_type": "code",
        "outputId": "5dac380e-2299-4fe9-d22c-cc7db297e70d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=100000, binary=True, ngram_range=(1,1))\n",
        "\n",
        "feature_matrix = vectorizer.fit_transform(lines) \n",
        "# .fit_transform: Learn the vocabulary dictionary and return term-document matrix.\n",
        "\n",
        "# print(vectorizer.get_feature_names()) # Words (or ngrams) in learned vocabulary, a HUGE list!\n",
        "print(\"Number of rows (documens) and unique ngrams in feature matrix\")\n",
        "print(feature_matrix.shape) \n",
        "print()\n",
        "print(\"Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\")\n",
        "print(feature_matrix.toarray())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows (documens) and unique ngrams in feature matrix\n",
            "(10000, 46875)\n",
            "\n",
            "Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlD3w6R1iBoP",
        "colab_type": "code",
        "outputId": "479f449a-6a55-468b-84bc-f30fd3044f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "example_count, feature_count = feature_matrix.shape\n",
        "class_count = len(label_encoder.classes_)\n",
        "\n",
        "inp = Input(shape=(feature_count, )) # tuple\n",
        "hidden = Dense(200, activation=\"tanh\")(inp) # taalla kaytetty tanh. Relu suositumpi? \n",
        "# Jos mitaan aktivointifunktiota ei anneta, tulee syotteen ja kertoimien lineaarinen matriisitulo \n",
        "outp = Dense(class_count, activation=\"softmax\")(hidden) # softmax: tuottaa luokkien jakauman\n",
        "model = Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f6bd1e23860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCAt8NK8jrTu",
        "colab_type": "text"
      },
      "source": [
        "Once the model is constructed it needs to be compiled, for that we need to know:\n",
        "* which optimizer we want to use (sgd is fine to begin with)\n",
        "* what is the loss (categorial_crossentropy for multiclass of the kind we have is the right choice)\n",
        "* which metrics to measure, accuracy is an okay choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyQlvJT0jqFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nozNLSNrXMB",
        "colab_type": "code",
        "outputId": "cc32dbb7-d879-4279-f5f0-22d5ec5bfc9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# batch_size kuinka monta inputtia kerralla sisaan. jokaisen batchin jalkeen paivitetaan painokertoimet gradientien keskiarvolla\n",
        "# epochs kuinka monta kertaa mennaan lapi koko data\n",
        "# validation_split: kuinka paljon dataa kaytetaan accuracyn laskemiseen\n",
        "\n",
        "# Callback to stop training when no improvement\n",
        "stop_cb=EarlyStopping(monitor='val_acc', patience=2, verbose=1, mode='auto', baseline=None, \n",
        "                      restore_best_weights=True)\n",
        "\n",
        "hist=model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=100,\n",
        "               validation_split=0.1, callbacks=[stop_cb])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "9000/9000 [==============================] - 7s 731us/step - loss: 0.5599 - acc: 0.9226 - val_loss: 0.2110 - val_acc: 0.9590\n",
            "Epoch 2/100\n",
            "9000/9000 [==============================] - 5s 589us/step - loss: 0.1062 - acc: 0.9889 - val_loss: 0.1509 - val_acc: 0.9730\n",
            "Epoch 3/100\n",
            "9000/9000 [==============================] - 4s 470us/step - loss: 0.0468 - acc: 0.9973 - val_loss: 0.1315 - val_acc: 0.9680\n",
            "Epoch 4/100\n",
            "9000/9000 [==============================] - 4s 464us/step - loss: 0.0251 - acc: 0.9990 - val_loss: 0.1229 - val_acc: 0.9630\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plicePXSXTGe",
        "colab_type": "text"
      },
      "source": [
        "Experiment with the various parameters of learning (learning rate, optimizer, etc). See [Keras API](https://keras.io/optimizers/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxvmEesywa8D",
        "colab_type": "code",
        "outputId": "030226e0-f072-4015-9cd3-9fb761a60908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=sgd, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=10,\n",
        "               validation_split=0.1, callbacks=[stop_cb])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 5s 585us/step - loss: 0.0559 - acc: 0.9977 - val_loss: 0.1508 - val_acc: 0.9730\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 446us/step - loss: 0.0555 - acc: 0.9977 - val_loss: 0.1506 - val_acc: 0.9730\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 436us/step - loss: 0.0552 - acc: 0.9978 - val_loss: 0.1504 - val_acc: 0.9730\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUcEdD0vG2by",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ca1f7ac-6b71-4ab0-e10a-2f72222a961d"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# ops = ('Adadelta', 'Adagrad', 'Adam', 'Ftrl', 'SGD', 'Momentum', 'RMSProp', 'Adamax', 'Nadam')\n",
        "# Why does the code crash with ops above? Ftrl and Momentum won't work although they are \n",
        "# documented optimizers\n",
        "# and used in here: https://github.com/Jaewan-Yun/optimizer-visualization/blob/master/optimizer_visualization.py\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl\n",
        "ops = ('Adadelta', 'Adagrad', 'Adam', 'SGD', 'RMSProp', 'Adamax', 'Nadam')\n",
        "\n",
        "for op in ops:\n",
        "  print(op)\n",
        "  print()\n",
        "  model.compile(optimizer = op, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  hist = model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=10,\n",
        "              validation_split=0.1, callbacks=[stop_cb])\n",
        "  print()\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adadelta\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 6s 650us/step - loss: 8.8328e-04 - acc: 0.9998 - val_loss: 0.1177 - val_acc: 0.9610\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 463us/step - loss: 8.2707e-04 - acc: 0.9998 - val_loss: 0.1232 - val_acc: 0.9580\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 454us/step - loss: 7.8508e-04 - acc: 0.9998 - val_loss: 0.1263 - val_acc: 0.9560\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n",
            "\n",
            "Adagrad\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 6s 614us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1323 - val_acc: 0.9560\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 422us/step - loss: 7.5296e-04 - acc: 0.9998 - val_loss: 0.1342 - val_acc: 0.9540\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 428us/step - loss: 7.0974e-04 - acc: 0.9998 - val_loss: 0.1384 - val_acc: 0.9520\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n",
            "\n",
            "Adam\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 6s 621us/step - loss: 7.2556e-04 - acc: 0.9998 - val_loss: 0.1365 - val_acc: 0.9530\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 432us/step - loss: 7.2242e-04 - acc: 0.9998 - val_loss: 0.1398 - val_acc: 0.9520\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 421us/step - loss: 6.9445e-04 - acc: 0.9998 - val_loss: 0.1426 - val_acc: 0.9510\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n",
            "\n",
            "SGD\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 5s 581us/step - loss: 6.3001e-04 - acc: 0.9998 - val_loss: 0.1366 - val_acc: 0.9530\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 403us/step - loss: 6.2995e-04 - acc: 0.9998 - val_loss: 0.1366 - val_acc: 0.9530\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 398us/step - loss: 6.2989e-04 - acc: 0.9998 - val_loss: 0.1366 - val_acc: 0.9530\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n",
            "\n",
            "RMSProp\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 4s 446us/step - loss: 7.2294e-04 - acc: 0.9998 - val_loss: 0.1558 - val_acc: 0.9500\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 2s 260us/step - loss: 7.4775e-04 - acc: 0.9998 - val_loss: 0.1669 - val_acc: 0.9480\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 2s 256us/step - loss: 7.7000e-04 - acc: 0.9998 - val_loss: 0.1761 - val_acc: 0.9470\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n",
            "\n",
            "Adamax\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 5s 606us/step - loss: 6.5705e-04 - acc: 0.9998 - val_loss: 0.1424 - val_acc: 0.9520\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 414us/step - loss: 6.3390e-04 - acc: 0.9998 - val_loss: 0.1474 - val_acc: 0.9510\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 411us/step - loss: 6.2997e-04 - acc: 0.9998 - val_loss: 0.1481 - val_acc: 0.9510\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00003: early stopping\n",
            "\n",
            "Nadam\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 6s 627us/step - loss: 7.1344e-04 - acc: 0.9998 - val_loss: 0.1742 - val_acc: 0.9470\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 424us/step - loss: 8.6046e-04 - acc: 0.9998 - val_loss: 0.1648 - val_acc: 0.9500\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 417us/step - loss: 7.9197e-04 - acc: 0.9998 - val_loss: 0.1572 - val_acc: 0.9520\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 4s 417us/step - loss: 6.8496e-04 - acc: 0.9997 - val_loss: 0.1798 - val_acc: 0.9480\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 4s 419us/step - loss: 7.7023e-04 - acc: 0.9998 - val_loss: 0.1553 - val_acc: 0.9520\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00005: early stopping\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9_y6FrXfExG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "d1d3f133-0272-49f1-ea40-d2f0dd5f931e"
      },
      "source": [
        "lrs = np.logspace(-15, 5, base = 2, num = 20)\n",
        "lrs"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.05175781e-05, 6.33029202e-05, 1.31309886e-04, 2.72377421e-04,\n",
              "       5.64995233e-04, 1.17197531e-03, 2.43104021e-03, 5.04273124e-03,\n",
              "       1.04601883e-02, 2.16976741e-02, 4.50077043e-02, 9.33599351e-02,\n",
              "       1.93657455e-01, 4.01705613e-01, 8.33262006e-01, 1.72844379e+00,\n",
              "       3.58532838e+00, 7.43708284e+00, 1.54268160e+01, 3.20000000e+01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "95a723b1-b0e2-4cdc-ad20-2daec059d273",
        "id": "D_cqMxq4ZG64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "lrs = np.logspace(-15, 5, base = 2, num = 20)\n",
        "\n",
        "# keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "for lr in lrs:\n",
        "  print(\"Adam, learning rate \" + str(lr))\n",
        "  print()\n",
        "  optim = optimizers.adam(lr=lr)\n",
        "  model.compile(optimizer=optim, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "  hist = model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=10,\n",
        "               validation_split=0.1, callbacks=[stop_cb])\n",
        "  print()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adam, learning rate 3.0517578125e-05\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 8s 834us/step - loss: 0.3909 - acc: 0.9734 - val_loss: 1.4177 - val_acc: 0.9080\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 4s 439us/step - loss: 0.3865 - acc: 0.9739 - val_loss: 1.4158 - val_acc: 0.9080\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 4s 448us/step - loss: 0.3824 - acc: 0.9740 - val_loss: 1.4143 - val_acc: 0.9090\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 4s 438us/step - loss: 0.3787 - acc: 0.9742 - val_loss: 1.4132 - val_acc: 0.9100\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 4s 441us/step - loss: 0.3755 - acc: 0.9749 - val_loss: 1.4124 - val_acc: 0.9100\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 4s 440us/step - loss: 0.3728 - acc: 0.9753 - val_loss: 1.4118 - val_acc: 0.9110\n",
            "Epoch 7/10\n",
            "9000/9000 [==============================] - 4s 443us/step - loss: 0.3706 - acc: 0.9756 - val_loss: 1.4113 - val_acc: 0.9110\n",
            "Epoch 8/10\n",
            "9000/9000 [==============================] - 4s 443us/step - loss: 0.3687 - acc: 0.9758 - val_loss: 1.4110 - val_acc: 0.9110\n",
            "Epoch 9/10\n",
            "9000/9000 [==============================] - 4s 443us/step - loss: 0.3669 - acc: 0.9759 - val_loss: 1.4106 - val_acc: 0.9110\n",
            "Epoch 10/10\n",
            "9000/9000 [==============================] - 4s 441us/step - loss: 0.3651 - acc: 0.9762 - val_loss: 1.4103 - val_acc: 0.9110\n",
            "\n",
            "Adam, learning rate 6.330292019324902e-05\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 805us/step - loss: 0.3632 - acc: 0.9764 - val_loss: 1.4083 - val_acc: 0.9110\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 315us/step - loss: 0.3608 - acc: 0.9768 - val_loss: 1.4063 - val_acc: 0.9110\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 320us/step - loss: 0.3583 - acc: 0.9768 - val_loss: 1.4047 - val_acc: 0.9110\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 311us/step - loss: 0.3559 - acc: 0.9768 - val_loss: 1.4032 - val_acc: 0.9110\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 310us/step - loss: 0.3539 - acc: 0.9770 - val_loss: 1.4017 - val_acc: 0.9110\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 313us/step - loss: 0.3524 - acc: 0.9773 - val_loss: 1.4008 - val_acc: 0.9110\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "Adam, learning rate 0.0001313098860132058\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 6s 699us/step - loss: 0.3608 - acc: 0.9768 - val_loss: 1.4049 - val_acc: 0.9110\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 321us/step - loss: 0.3566 - acc: 0.9768 - val_loss: 1.4023 - val_acc: 0.9110\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 323us/step - loss: 0.3531 - acc: 0.9773 - val_loss: 1.3993 - val_acc: 0.9110\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 318us/step - loss: 0.3507 - acc: 0.9778 - val_loss: 1.3973 - val_acc: 0.9110\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 318us/step - loss: 0.3480 - acc: 0.9780 - val_loss: 1.3951 - val_acc: 0.9110\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 321us/step - loss: 0.3457 - acc: 0.9781 - val_loss: 1.3935 - val_acc: 0.9110\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "Adam, learning rate 0.00027237742132850206\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 6s 679us/step - loss: 0.3561 - acc: 0.9769 - val_loss: 1.3998 - val_acc: 0.9110\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 319us/step - loss: 0.3506 - acc: 0.9777 - val_loss: 1.3933 - val_acc: 0.9110\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 319us/step - loss: 0.3442 - acc: 0.9782 - val_loss: 1.3854 - val_acc: 0.9110\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 318us/step - loss: 0.3365 - acc: 0.9789 - val_loss: 1.3678 - val_acc: 0.9130\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 319us/step - loss: 0.3306 - acc: 0.9792 - val_loss: 1.3581 - val_acc: 0.9130\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 319us/step - loss: 0.3256 - acc: 0.9796 - val_loss: 1.3523 - val_acc: 0.9130\n",
            "Epoch 7/10\n",
            "9000/9000 [==============================] - 3s 325us/step - loss: 0.3197 - acc: 0.9797 - val_loss: 1.3449 - val_acc: 0.9140\n",
            "Epoch 8/10\n",
            "9000/9000 [==============================] - 3s 318us/step - loss: 0.3160 - acc: 0.9800 - val_loss: 1.3380 - val_acc: 0.9150\n",
            "Epoch 9/10\n",
            "9000/9000 [==============================] - 3s 321us/step - loss: 0.3126 - acc: 0.9803 - val_loss: 1.3333 - val_acc: 0.9150\n",
            "Epoch 10/10\n",
            "9000/9000 [==============================] - 3s 329us/step - loss: 0.3115 - acc: 0.9803 - val_loss: 1.3293 - val_acc: 0.9150\n",
            "\n",
            "Adam, learning rate 0.0005649952330482026\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 6s 720us/step - loss: 0.3108 - acc: 0.9804 - val_loss: 1.2960 - val_acc: 0.9160\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 333us/step - loss: 0.3052 - acc: 0.9807 - val_loss: 1.1531 - val_acc: 0.9190\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 330us/step - loss: 0.2966 - acc: 0.9811 - val_loss: 1.1436 - val_acc: 0.9160\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 325us/step - loss: 0.2929 - acc: 0.9816 - val_loss: 1.1616 - val_acc: 0.9140\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 332us/step - loss: 0.2893 - acc: 0.9819 - val_loss: 1.1682 - val_acc: 0.9130\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 333us/step - loss: 0.2887 - acc: 0.9820 - val_loss: 1.1664 - val_acc: 0.9130\n",
            "Epoch 7/10\n",
            "9000/9000 [==============================] - 3s 328us/step - loss: 0.2887 - acc: 0.9820 - val_loss: 1.1652 - val_acc: 0.9150\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00007: early stopping\n",
            "\n",
            "Adam, learning rate 0.0011719753120879915\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 744us/step - loss: 0.2968 - acc: 0.9811 - val_loss: 1.1384 - val_acc: 0.9150\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 332us/step - loss: 0.2911 - acc: 0.9817 - val_loss: 1.1964 - val_acc: 0.9130\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 334us/step - loss: 0.2888 - acc: 0.9819 - val_loss: 1.2140 - val_acc: 0.9110\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 336us/step - loss: 0.2871 - acc: 0.9821 - val_loss: 1.2147 - val_acc: 0.9130\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 336us/step - loss: 0.2853 - acc: 0.9821 - val_loss: 1.2178 - val_acc: 0.9130\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 339us/step - loss: 0.2852 - acc: 0.9822 - val_loss: 1.2167 - val_acc: 0.9180\n",
            "Epoch 7/10\n",
            "9000/9000 [==============================] - 3s 336us/step - loss: 0.2855 - acc: 0.9821 - val_loss: 1.2139 - val_acc: 0.9130\n",
            "Epoch 8/10\n",
            "9000/9000 [==============================] - 3s 345us/step - loss: 0.2859 - acc: 0.9820 - val_loss: 1.2428 - val_acc: 0.9110\n",
            "Epoch 9/10\n",
            "9000/9000 [==============================] - 3s 334us/step - loss: 0.2836 - acc: 0.9823 - val_loss: 1.2140 - val_acc: 0.9130\n",
            "Epoch 10/10\n",
            "9000/9000 [==============================] - 3s 333us/step - loss: 0.2836 - acc: 0.9822 - val_loss: 1.2184 - val_acc: 0.9130\n",
            "\n",
            "Adam, learning rate 0.002431040213796921\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 755us/step - loss: 0.2843 - acc: 0.9822 - val_loss: 1.2224 - val_acc: 0.9130\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 332us/step - loss: 0.2836 - acc: 0.9823 - val_loss: 1.2536 - val_acc: 0.9170\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 332us/step - loss: 0.2844 - acc: 0.9822 - val_loss: 1.1945 - val_acc: 0.9130\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 339us/step - loss: 0.2847 - acc: 0.9822 - val_loss: 1.2897 - val_acc: 0.9120\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 331us/step - loss: 0.2844 - acc: 0.9823 - val_loss: 1.1980 - val_acc: 0.9110\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 329us/step - loss: 0.2842 - acc: 0.9822 - val_loss: 1.2086 - val_acc: 0.9110\n",
            "Epoch 7/10\n",
            "9000/9000 [==============================] - 3s 331us/step - loss: 0.2840 - acc: 0.9822 - val_loss: 1.2069 - val_acc: 0.9110\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00007: early stopping\n",
            "\n",
            "Adam, learning rate 0.00504273124198205\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 759us/step - loss: 0.2853 - acc: 0.9822 - val_loss: 1.3525 - val_acc: 0.9060\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 332us/step - loss: 0.2848 - acc: 0.9823 - val_loss: 1.3523 - val_acc: 0.9060\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 331us/step - loss: 0.2848 - acc: 0.9823 - val_loss: 1.3522 - val_acc: 0.9060\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 328us/step - loss: 0.2848 - acc: 0.9823 - val_loss: 1.3522 - val_acc: 0.9060\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 343us/step - loss: 0.2848 - acc: 0.9823 - val_loss: 1.3522 - val_acc: 0.9060\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 344us/step - loss: 0.2848 - acc: 0.9823 - val_loss: 1.3522 - val_acc: 0.9060\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "Adam, learning rate 0.010460188290816186\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 788us/step - loss: 0.2848 - acc: 0.9823 - val_loss: 1.3460 - val_acc: 0.9060\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 331us/step - loss: 0.2848 - acc: 0.9823 - val_loss: 1.3397 - val_acc: 0.9060\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 334us/step - loss: 0.2678 - acc: 0.9829 - val_loss: 1.3137 - val_acc: 0.9130\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 328us/step - loss: 0.2414 - acc: 0.9842 - val_loss: 1.1169 - val_acc: 0.9220\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 333us/step - loss: 0.2203 - acc: 0.9862 - val_loss: 1.0021 - val_acc: 0.9270\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 338us/step - loss: 0.1926 - acc: 0.9877 - val_loss: 1.1105 - val_acc: 0.9230\n",
            "Epoch 7/10\n",
            "9000/9000 [==============================] - 3s 330us/step - loss: 0.1875 - acc: 0.9881 - val_loss: 1.1850 - val_acc: 0.9200\n",
            "Epoch 8/10\n",
            "9000/9000 [==============================] - 3s 325us/step - loss: 0.1840 - acc: 0.9884 - val_loss: 1.0413 - val_acc: 0.9330\n",
            "Epoch 9/10\n",
            "9000/9000 [==============================] - 3s 333us/step - loss: 0.1881 - acc: 0.9883 - val_loss: 1.0288 - val_acc: 0.9340\n",
            "Epoch 10/10\n",
            "9000/9000 [==============================] - 3s 326us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0288 - val_acc: 0.9340\n",
            "\n",
            "Adam, learning rate 0.021697674103353992\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 780us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 333us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 330us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 334us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 327us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 331us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "Adam, learning rate 0.04500770429808619\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 766us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 317us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 316us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0289 - val_acc: 0.9340\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 322us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0290 - val_acc: 0.9340\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 323us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0290 - val_acc: 0.9340\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 323us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0283 - val_acc: 0.9340\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "Adam, learning rate 0.09335993510340528\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 770us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 332us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 332us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 325us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 331us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 333us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "Adam, learning rate 0.193657455283332\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 779us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 326us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 322us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 325us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 329us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 330us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "Adam, learning rate 0.4017056132834412\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 737us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 287us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 288us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 289us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 285us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 283us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "Adam, learning rate 0.8332620063985443\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 744us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 282us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 283us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0284 - val_acc: 0.9340\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 281us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0276 - val_acc: 0.9340\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 2s 277us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0263 - val_acc: 0.9340\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 282us/step - loss: 0.1880 - acc: 0.9883 - val_loss: 1.0349 - val_acc: 0.9350\n",
            "Epoch 7/10\n",
            "9000/9000 [==============================] - 3s 282us/step - loss: 2.7023 - acc: 0.8307 - val_loss: 4.7065 - val_acc: 0.7080\n",
            "Epoch 8/10\n",
            "9000/9000 [==============================] - 2s 273us/step - loss: 3.8764 - acc: 0.7581 - val_loss: 4.3271 - val_acc: 0.7310\n",
            "Epoch 9/10\n",
            "9000/9000 [==============================] - 3s 279us/step - loss: 3.7299 - acc: 0.7677 - val_loss: 4.0420 - val_acc: 0.7480\n",
            "Epoch 10/10\n",
            "9000/9000 [==============================] - 2s 276us/step - loss: 3.0157 - acc: 0.8122 - val_loss: 3.4332 - val_acc: 0.7870\n",
            "\n",
            "Adam, learning rate 1.7284437865632103\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 732us/step - loss: 3.6481 - acc: 0.7727 - val_loss: 4.2559 - val_acc: 0.7350\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 285us/step - loss: 3.2132 - acc: 0.7999 - val_loss: 3.3042 - val_acc: 0.7950\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 288us/step - loss: 3.0749 - acc: 0.8086 - val_loss: 5.7845 - val_acc: 0.6410\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 278us/step - loss: 3.9210 - acc: 0.7561 - val_loss: 3.3889 - val_acc: 0.7890\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 2s 276us/step - loss: 3.5397 - acc: 0.7798 - val_loss: 4.5614 - val_acc: 0.7170\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 284us/step - loss: 4.4160 - acc: 0.7253 - val_loss: 5.3351 - val_acc: 0.6690\n",
            "Epoch 7/10\n",
            "9000/9000 [==============================] - 3s 290us/step - loss: 4.6280 - acc: 0.7128 - val_loss: 4.9644 - val_acc: 0.6920\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00007: early stopping\n",
            "\n",
            "Adam, learning rate 3.585328384551421\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 739us/step - loss: 3.9355 - acc: 0.7557 - val_loss: 4.2239 - val_acc: 0.7370\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 290us/step - loss: 5.2946 - acc: 0.6711 - val_loss: 7.6722 - val_acc: 0.5240\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 288us/step - loss: 5.8249 - acc: 0.6382 - val_loss: 4.6028 - val_acc: 0.7140\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 286us/step - loss: 6.0286 - acc: 0.6258 - val_loss: 6.1416 - val_acc: 0.6190\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 283us/step - loss: 5.8197 - acc: 0.6387 - val_loss: 5.8149 - val_acc: 0.6390\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 289us/step - loss: 6.1452 - acc: 0.6182 - val_loss: 6.0604 - val_acc: 0.6240\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "Adam, learning rate 7.437082840067253\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 732us/step - loss: 4.8415 - acc: 0.6991 - val_loss: 9.0745 - val_acc: 0.4370\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 284us/step - loss: 7.7486 - acc: 0.5190 - val_loss: 9.9449 - val_acc: 0.3830\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 286us/step - loss: 7.4085 - acc: 0.5402 - val_loss: 7.8656 - val_acc: 0.5120\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 283us/step - loss: 7.4291 - acc: 0.5389 - val_loss: 7.2602 - val_acc: 0.5490\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 283us/step - loss: 5.7993 - acc: 0.6399 - val_loss: 5.8186 - val_acc: 0.6390\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 284us/step - loss: 5.9940 - acc: 0.6279 - val_loss: 8.1131 - val_acc: 0.4960\n",
            "Epoch 7/10\n",
            "9000/9000 [==============================] - 3s 283us/step - loss: 7.6510 - acc: 0.5250 - val_loss: 6.3344 - val_acc: 0.6070\n",
            "Epoch 8/10\n",
            "9000/9000 [==============================] - 2s 277us/step - loss: 5.2041 - acc: 0.6771 - val_loss: 5.5769 - val_acc: 0.6540\n",
            "Epoch 9/10\n",
            "9000/9000 [==============================] - 3s 281us/step - loss: 5.0736 - acc: 0.6852 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 10/10\n",
            "9000/9000 [==============================] - 3s 284us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "\n",
            "Adam, learning rate 15.426815967079905\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 739us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 284us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 284us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 288us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 279us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 283us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n",
            "Adam, learning rate 32.0\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 7s 751us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 3s 282us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 3s 281us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 3s 288us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 3s 287us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 3s 291us/step - loss: 5.0539 - acc: 0.6864 - val_loss: 5.4479 - val_acc: 0.6620\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}