{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "bowcls_demo_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/Deep_Learning_in_LangTech_course/blob/master/language_classifier_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwOk0plcmZez",
        "colab_type": "text"
      },
      "source": [
        "# Compare SVM and NN with language classification \n",
        "\n",
        "Course data folder language_identification contains data for 5 languages. Based on this data\n",
        "* Train an SVM classifier for language recognition between these 5 languages. (Previously done!)\n",
        "* Implement this same classifier using a simple NN\n",
        "* Compare the results you get with NN and SVM Focus on experimenting with the various NN parameters of learning (learning rate, optimizer, etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwuacC9wl6L5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the data in makes sense to structure a little bit\n",
        "# ratkaise, miten kansio tuodaan omasta GitHubista!\n",
        "# toimii myös luomalla Colabiin kansion (tässä nimeltä \"texts\"), \n",
        "# jonne tiedostot raahaa (kansio katoaa, kun ajo päättyy)\n",
        "\n",
        "import random\n",
        "\n",
        "def read_data_one_lang(lang,part):\n",
        "    \"\"\"Reads one file for one language. Returns data in the form of pairs of (lang,line)\"\"\"\n",
        "    filename=\"texts/{}_{}.txt\".format(lang,part)\n",
        "    result=[] #this will be the list of pairs (lang,line)\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            result.append((lang,line)) \n",
        "    return result\n",
        "\n",
        "\n",
        "def read_data_all_langs(part):\n",
        "    \"\"\"Reads train, test or dev data for all languages. part can be train, test, or devel\"\"\"\n",
        "    data=[]\n",
        "    for lang in (\"en\",\"es\",\"et\",\"fi\",\"pt\"):\n",
        "        pairs=read_data_one_lang(lang,part)\n",
        "        data.extend(pairs) #just add these lines to the end\n",
        "    #...done\n",
        "    #but now they come in the order of languages\n",
        "    #we really must scramble these!\n",
        "    random.shuffle(data)\n",
        "    \n",
        "    #let's yet separate the labels and lines, we will need that anyway\n",
        "    labels=[label for label,line in data]\n",
        "    lines=[line for label,line in data]\n",
        "    return labels,lines\n",
        "\n",
        "labels_train, lines_train=read_data_all_langs(\"train\") # train and test data splitting already done!\n",
        "labels_dev,lines_dev=read_data_all_langs(\"devel\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP6vO5awPFGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "859e46b4-d0c4-4ab8-d20e-1324b5ed18b6"
      },
      "source": [
        "for label,line in zip(labels_train[:5],lines_train[:5]):\n",
        "    print(label,\"   \",line[:30],\"...\")\n",
        "\n",
        "print(labels_train[0], lines_train[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pt     « Um dia, o Humberto Madeira,  ...\n",
            "pt     Parágrafo 3º.-- Em os contrato ...\n",
            "et     Kui“ saadud arv” on võrdne puh ...\n",
            "pt     « As denúncias que vêm sendo f ...\n",
            "es     La Asamblea General de las Nac ...\n",
            "pt « Um dia, o Humberto Madeira, o Raul Solnado e o meu pai iam em viagem e passaram por o pinhal de Leiria.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN1szyCVLlXc",
        "colab_type": "text"
      },
      "source": [
        "# Reminder\n",
        "\n",
        "Feature matrix has row for each document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfVT4fZOl6MN",
        "colab_type": "code",
        "outputId": "09a32f21-5b24-4d87-e7b2-658f90a8685f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sklearn.svm\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=100000, binary=True, ngram_range=(1,1))\n",
        "\n",
        "feature_matrix_train = vectorizer.fit_transform(lines_train) \n",
        "# .fit_transform: Learn the vocabulary dictionary and return term-document matrix.\n",
        "feature_matrix_dev = vectorizer.transform(lines_dev)\n",
        "# .transform: Transform documents to document-term matrix.\n",
        "\n",
        "# print(vectorizer.get_feature_names()) # Words (or ngrams) in learned vocabulary, a HUGE list!\n",
        "print(\"Number of rows (documens) and unique ngrams in feature matrix\")\n",
        "print(feature_matrix_train.shape) \n",
        "print()\n",
        "print(\"Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\")\n",
        "print(feature_matrix_train.toarray())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows (documens) and unique ngrams in feature matrix\n",
            "(5000, 28620)\n",
            "\n",
            "Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjEZcvrqLd6T",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I09AyZhsLcfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "72439598-44a1-4a55-f638-05ea01911495"
      },
      "source": [
        "for C in (0.001,0.01,0.1,1,10,100):\n",
        "    classifier =  sklearn.svm.LinearSVC(C=C)\n",
        "    classifier.fit(feature_matrix_train, labels_train)\n",
        "    print(\"C=\",C,\"     \",classifier.score(feature_matrix_dev, labels_dev))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C= 0.001       0.8758\n",
            "C= 0.01       0.9144\n",
            "C= 0.1       0.933\n",
            "C= 1       0.9302\n",
            "C= 10       0.9102\n",
            "C= 100       0.873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCwTZxCl6Mc",
        "colab_type": "text"
      },
      "source": [
        "* 93% accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF3JQwVsWBlT",
        "colab_type": "text"
      },
      "source": [
        "# NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIb32lm1mdSt",
        "colab_type": "text"
      },
      "source": [
        "For NN we will combine the data and separate a proportion of it for validation when fitting the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag9L8TEjl4W6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines =  lines_train + lines_dev\n",
        "labels = labels_train + labels_dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgk8GjZceOl",
        "colab_type": "text"
      },
      "source": [
        "For NN we need to encode each class to numeric value.\n",
        " * Remember[ difference between encoding vs. one hot encoding](https://https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZojDVNIUWBGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8eed3131-8e4a-4be4-a2d4-5069e5b19b67"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder() #Turns class labels into integers\n",
        "class_numbers = label_encoder.fit_transform(labels)\n",
        "\n",
        "print(class_numbers)\n",
        "print(\"class_numbers shape=\",class_numbers.shape)\n",
        "print(\"class labels\",label_encoder.classes_) #this will let us translate back from indices to labels"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 4 2 ... 0 3 0]\n",
            "class_numbers shape= (10000,)\n",
            "class labels ['en' 'es' 'et' 'fi' 'pt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3v5Wm_hmvRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "7dbe8a5d-bed1-4296-cd41-89b1d7f11159"
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=100000, binary=True, ngram_range=(1,1))\n",
        "\n",
        "feature_matrix = vectorizer.fit_transform(lines) \n",
        "# .fit_transform: Learn the vocabulary dictionary and return term-document matrix.\n",
        "\n",
        "# print(vectorizer.get_feature_names()) # Words (or ngrams) in learned vocabulary, a HUGE list!\n",
        "print(\"Number of rows (documens) and unique ngrams in feature matrix\")\n",
        "print(feature_matrix.shape) \n",
        "print()\n",
        "print(\"Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\")\n",
        "print(feature_matrix.toarray())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows (documens) and unique ngrams in feature matrix\n",
            "(10000, 46875)\n",
            "\n",
            "Since most of the texts only use a limited amonut of words (ngrams) in the vocabulary, feature matrix is sparse!\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlD3w6R1iBoP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "afa35396-a24c-4b75-978d-e30cc7881437"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "example_count, feature_count = feature_matrix.shape\n",
        "class_count = len(label_encoder.classes_)\n",
        "\n",
        "inp = Input(shape=(feature_count, )) # tuple\n",
        "hidden = Dense(200, activation=\"tanh\")(inp) # taalla kaytetty tanh. Relu suositumpi? \n",
        "# Jos mitaan aktivointifunktiota ei anneta, tulee syotteen ja kertoimien lineaarinen matriisitulo \n",
        "outp = Dense(class_count, activation=\"softmax\")(hidden) # softmax: tuottaa luokkien jakauman\n",
        "model = Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f5df05d5390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCAt8NK8jrTu",
        "colab_type": "text"
      },
      "source": [
        "Once the model is constructed it needs to be compiled, for that we need to know:\n",
        "* which optimizer we want to use (sgd is fine to begin with)\n",
        "* what is the loss (categorial_crossentropy for multiclass of the kind we have is the right choice)\n",
        "* which metrics to measure, accuracy is an okay choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyQlvJT0jqFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nozNLSNrXMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "98b8bc8e-d9a5-45f3-bb2a-4a4d60fcfd21"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# batch_size kuinka monta inputtia kerralla sisaan. jokaisen batchin jalkeen paivitetaan painokertoimet gradientien keskiarvolla\n",
        "# epochs kuinka monta kertaa mennaan lapi koko data\n",
        "# validation_split: kuinka paljon dataa kaytetaan accuracyn laskemiseen\n",
        "\n",
        "# Callback to stop training when no improvement\n",
        "stop_cb=EarlyStopping(monitor='val_acc', patience=2, verbose=1, mode='auto', baseline=None, \n",
        "                      restore_best_weights=True)\n",
        "\n",
        "hist=model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=100,\n",
        "               validation_split=0.1, callbacks=[stop_cb])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "9000/9000 [==============================] - 6s 655us/step - loss: 0.5524 - acc: 0.9204 - val_loss: 0.2498 - val_acc: 0.9470\n",
            "Epoch 2/100\n",
            "9000/9000 [==============================] - 5s 598us/step - loss: 0.1027 - acc: 0.9893 - val_loss: 0.1883 - val_acc: 0.9570\n",
            "Epoch 3/100\n",
            "9000/9000 [==============================] - 5s 597us/step - loss: 0.0460 - acc: 0.9978 - val_loss: 0.1689 - val_acc: 0.9610\n",
            "Epoch 4/100\n",
            "9000/9000 [==============================] - 5s 596us/step - loss: 0.0251 - acc: 0.9989 - val_loss: 0.1576 - val_acc: 0.9580\n",
            "Epoch 5/100\n",
            "9000/9000 [==============================] - 5s 595us/step - loss: 0.0152 - acc: 0.9994 - val_loss: 0.1531 - val_acc: 0.9560\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plicePXSXTGe",
        "colab_type": "text"
      },
      "source": [
        "Accuracy 96 %\n",
        "\n",
        "Experiment with the various parameters of learning (learning rate, optimizer, etc). See [Keras API](https://keras.io/optimizers/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxvmEesywa8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "041c92df-71fc-40bc-f86c-6f30f50f0f6d"
      },
      "source": [
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=sgd, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=100,\n",
        "               validation_split=0.1, callbacks=[stop_cb])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-a522df93bf18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m hist = model.fit(feature_matrix, class_numbers, batch_size=100, verbose=1, epochs=100,\n\u001b[1;32m      5\u001b[0m                validation_split=0.1, callbacks=[stop_cb])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimizers' is not defined"
          ]
        }
      ]
    }
  ]
}