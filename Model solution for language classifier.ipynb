{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fi     EU:n talousjärjestelmä joutui  ...\n",
      "es     La misma fue latinizada a Reti ...\n",
      "en     bd ...\n",
      "pt     Os« contratos de gaveta», vist ...\n",
      "en     But one should not go here exp ...\n"
     ]
    }
   ],
   "source": [
    "# Reading the data in makes sense to structure a little bit\n",
    "import random\n",
    "\n",
    "def read_data_one_lang(lang,part):\n",
    "    \"\"\"Reads one file for one language. Returns data in the form of pairs of (lang,line)\"\"\"\n",
    "    filename=\"language-identification/{}_{}.txt\".format(lang,part)\n",
    "    result=[] #this will be the list of pairs (lang,line)\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            result.append((lang,line)) \n",
    "    return result\n",
    "\n",
    "\n",
    "def read_data_all_langs(part):\n",
    "    \"\"\"Reads train, test or dev data for all languages. part can be train, test, or devel\"\"\"\n",
    "    #glob\n",
    "    data=[]\n",
    "    for lang in (\"en\",\"es\",\"et\",\"fi\",\"pt\"):\n",
    "        pairs=read_data_one_lang(lang,part)\n",
    "        data.extend(pairs) #just add these lines to the end\n",
    "    #...done\n",
    "    #but now they come in the order of languages\n",
    "    #we really must scramble these!\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    #let's yet separate the labels and lines, we will need that anyway\n",
    "    labels=[label for label,line in data]\n",
    "    lines=[line for label,line in data]\n",
    "    return labels,lines\n",
    "\n",
    "labels_train,lines_train=read_data_all_langs(\"train\")\n",
    "labels_dev,lines_dev=read_data_all_langs(\"devel\")\n",
    "for label,line in zip(labels_train[:5],lines_train[:5]):\n",
    "    print(label,\"   \",line[:30],\"...\")\n",
    "#and beyond this point, exactly same code is applicable as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_numbers shape= (5000,)\n",
      "class labels ['en' 'es' 'et' 'fi' 'pt']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#1-3 character grams\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(3,3),analyzer=\"char_wb\")\n",
    "feature_matrix_train=vectorizer.fit_transform(lines_train)\n",
    "feature_matrix_dev=vectorizer.transform(lines_dev)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "class_numbers_train=label_encoder.fit_transform(labels_train)\n",
    "class_numbers_dev=label_encoder.fit_transform(labels_dev)\n",
    "\n",
    "print(\"class_numbers shape=\",class_numbers_train.shape)\n",
    "print(\"class labels\",label_encoder.classes_) #this will let us translate back from indices to labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "5000/5000 [==============================] - 4s 873us/step - loss: 0.6973 - accuracy: 0.8974 - val_loss: 0.3471 - val_accuracy: 0.9800\n",
      "Epoch 2/25\n",
      "5000/5000 [==============================] - 4s 820us/step - loss: 0.2266 - accuracy: 0.9892 - val_loss: 0.2114 - val_accuracy: 0.9822\n",
      "Epoch 3/25\n",
      "5000/5000 [==============================] - 4s 744us/step - loss: 0.1350 - accuracy: 0.9942 - val_loss: 0.1576 - val_accuracy: 0.9834\n",
      "Epoch 4/25\n",
      "5000/5000 [==============================] - 4s 722us/step - loss: 0.0912 - accuracy: 0.9966 - val_loss: 0.1288 - val_accuracy: 0.9836\n",
      "Epoch 5/25\n",
      "5000/5000 [==============================] - 3s 621us/step - loss: 0.0658 - accuracy: 0.9982 - val_loss: 0.1109 - val_accuracy: 0.9842\n",
      "Epoch 6/25\n",
      "5000/5000 [==============================] - 3s 631us/step - loss: 0.0495 - accuracy: 0.9988 - val_loss: 0.0992 - val_accuracy: 0.9838\n",
      "Epoch 7/25\n",
      "5000/5000 [==============================] - 3s 647us/step - loss: 0.0383 - accuracy: 0.9994 - val_loss: 0.0908 - val_accuracy: 0.9836\n",
      "Epoch 8/25\n",
      "5000/5000 [==============================] - 4s 703us/step - loss: 0.0303 - accuracy: 0.9996 - val_loss: 0.0848 - val_accuracy: 0.9840\n",
      "Epoch 9/25\n",
      "5000/5000 [==============================] - 3s 681us/step - loss: 0.0244 - accuracy: 0.9998 - val_loss: 0.0801 - val_accuracy: 0.9830\n",
      "Epoch 10/25\n",
      "5000/5000 [==============================] - 3s 664us/step - loss: 0.0201 - accuracy: 0.9998 - val_loss: 0.0765 - val_accuracy: 0.9834\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "example_count,feature_count=feature_matrix_train.shape\n",
    "class_count=len(label_encoder.classes_)\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(20,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "stop_cb=EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
    "hist=model.fit(feature_matrix_train,class_numbers_train,batch_size=100,verbose=1,epochs=25,validation_data=(feature_matrix_dev,class_numbers_dev),callbacks=[stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's try to identify misclassified documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: en Correct: et Text: Loe ka:, ML, 14. november\n",
      "Prediction: et Correct: en Text: junkie lube?!\n",
      "Prediction: en Correct: pt Text: Nervosas!\n",
      "Prediction: es Correct: pt Text: Lambari d' Oeste( 5.000 habitantes) virou município há dois anos.\n",
      "Prediction: en Correct: fi Text: I know someone you don’t know... zzzztsts...\n",
      "Prediction: en Correct: pt Text: Em um único ano, 1937, eles compuseram« They Can't Take That Away From Me»,« Let's Call the Whole Thing Off»,« A Foggy Day»,« Nice Work if You Can Get it»,« They All Laughed»,« Love Walked In»' e« Love Is Here to Stay»', e essas são apenas as que ficaram universalmente conhecidas.\n",
      "Prediction: et Correct: fi Text: Se oli kamalaa.\n",
      "Prediction: et Correct: fi Text: Valinta miljoonien lauseiden joukosta on minun tekoni.\n",
      "Prediction: et Correct: en Text: 2 Peels:\n",
      "Prediction: et Correct: fi Text: Tulet sä ja?\n",
      "Prediction: en Correct: et Text: Üle tuhande meedialogo\n",
      "Prediction: fi Correct: et Text: Vala viina!\n",
      "Prediction: pt Correct: es Text: Plantas acaulescentes.\n",
      "Prediction: en Correct: fi Text: I\n",
      "Prediction: pt Correct: es Text: Seguro que no va a ser el último.\n",
      "Prediction: en Correct: et Text: Sott gramm.\n",
      "Prediction: fi Correct: et Text: Kuulan.\n",
      "Prediction: es Correct: pt Text: China condiciona\n",
      "Prediction: en Correct: et Text: Filter-disco, anyone?\n",
      "Prediction: es Correct: pt Text: Montoya recupera\n",
      "Prediction: en Correct: fi Text: Yritysesittely: Accenture oy\n",
      "Prediction: es Correct: pt Text: General Nilton Cerqueira, presidente de o Clube Militar:\n",
      "Prediction: pt Correct: es Text: Florece de marzo a mayo.\n",
      "Prediction: en Correct: pt Text: Faber and Faber\n",
      "Prediction: pt Correct: en Text: Ramtanu Maitra\n",
      "Prediction: en Correct: pt Text: Kim[ rindo]-- Ele é Susan Hayward:\n",
      "Prediction: et Correct: fi Text: LIITE 1\n",
      "Prediction: et Correct: fi Text: Kristus 2011\n",
      "Prediction: en Correct: pt Text: WILSON BALDINI JR.\n",
      "Prediction: en Correct: pt Text: Iori Hamer\n",
      "Prediction: pt Correct: es Text: Mandan a sicarios a apalear a los morosos.\n",
      "Prediction: fi Correct: et Text: Katsun.\n",
      "Prediction: en Correct: fi Text: ANNEX 16\n",
      "Prediction: pt Correct: en Text: Barbara Quimba 1/30/10\n",
      "Prediction: pt Correct: es Text: Concejal de Soria( 1991-1996);\n",
      "Prediction: en Correct: pt Text: Com Chevy Chase, Dan Akroyd, John Candy, Demi Moore\n",
      "Prediction: en Correct: pt Text: Ouro nazi\n",
      "Prediction: en Correct: et Text: BRISTOL MYERS 36,9\n",
      "Prediction: en Correct: et Text: AT.\n",
      "Prediction: et Correct: fi Text: - Tiiätkö, mää oon kelannu.\n",
      "Prediction: es Correct: pt Text: Rostos eloquentes\n",
      "Prediction: et Correct: fi Text: 4.1. Asema ja rakenne\n",
      "Prediction: en Correct: pt Text: « On tourne!»\n",
      "Prediction: et Correct: en Text: [Tholt, Jane M.]\n",
      "Prediction: pt Correct: fi Text: Oranje\n",
      "Prediction: pt Correct: es Text: Se denomina así por emplear teja.\n",
      "Prediction: pt Correct: es Text: Por lo tanto no existe prórroga.\n",
      "Prediction: en Correct: es Text: ¿ Mamá?\n",
      "Prediction: en Correct: et Text: Vt\n",
      "Prediction: es Correct: pt Text: Só depois surge Tonya Harding, campeã de os Estados Unidos.\n",
      "Prediction: pt Correct: en Text: For decades.\n",
      "Prediction: en Correct: et Text: Link.\n",
      "Prediction: et Correct: fi Text: Järven valuma-alue on 524 km².\n",
      "Prediction: es Correct: pt Text: Suficientemente refrescante e estival.\n",
      "Prediction: en Correct: fi Text: PHP-skripti\n",
      "Prediction: en Correct: pt Text: FM Radical\n",
      "Prediction: pt Correct: es Text: Se opone a un sistema económico, social o político estratificado.\n",
      "Prediction: en Correct: pt Text: ?:? Acreditam?\n",
      "Prediction: es Correct: pt Text: Visconde da Luz muda de sentido\n",
      "Prediction: pt Correct: en Text: Braque\n",
      "Prediction: en Correct: pt Text: G. Love canta blues\n",
      "Prediction: en Correct: et Text: Mind.\n",
      "Prediction: en Correct: et Text: -?!\n",
      "Prediction: en Correct: et Text: Foto: Reuters\n",
      "Prediction: et Correct: en Text: Tori Kuykendall\n",
      "Prediction: en Correct: es Text: En teatro incluye Othello, A.M.L., Hamlet, The Hunchback of Notre Dame y Looking for the Pony en Manhattan Theater Source con su hermana Adrienne Hurd.\n",
      "Prediction: es Correct: et Text: Tuntuimad DO piirkonnad: Ribera del Duero, Rueda, Toro.\n",
      "Prediction: en Correct: et Text: Hiireke Stuart Little 79 398\n",
      "Prediction: en Correct: pt Text: « Peppermint Tea House-- The Best of Shoukichi Kina».\n",
      "Prediction: en Correct: et Text: 1.1.2..\n",
      "Prediction: et Correct: pt Text: Jamais ninguém me pediu nada.\n",
      "Prediction: fi Correct: et Text: Siin!\n",
      "Prediction: et Correct: fi Text: Pitsakastike;\n",
      "Prediction: en Correct: pt Text: Com fé\n",
      "Prediction: et Correct: fi Text: Se haluaa sinun rahasi.\n",
      "Prediction: pt Correct: et Text: Ljova Bi & Šura Bi\n",
      "Prediction: en Correct: es Text: Apareció en un artículo de The Alternate View:\" Boomerang and the Sound of the Big Bang\"( January 2001).\n",
      "Prediction: es Correct: pt Text: BC espera queda para comprar dólar\n",
      "Prediction: et Correct: en Text: Brenda\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "predictions=model.predict(feature_matrix_dev)\n",
    "pred_classes=numpy.argmax(predictions,axis=-1)\n",
    "for pred,correct,txt_line in zip(pred_classes,labels_dev,lines_dev):\n",
    "    pred_label=label_encoder.classes_[pred]\n",
    "    if pred_label!=correct:\n",
    "        print(\"Prediction:\",pred_label,\"Correct:\",correct,\"Text:\",txt_line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en' 'es' 'et' 'fi' 'pt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.9287121e-01, 9.9074520e-02, 2.2043586e-01, 2.0575805e-01,\n",
       "        1.8186036e-01],\n",
       "       [9.8750848e-01, 5.1748008e-04, 2.0715129e-03, 2.0498259e-03,\n",
       "        7.8526288e-03]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in=vectorizer.transform([\"sdfjfj fsdjfoj fsjofs fjskf fjsklf\",\"I really think this should be classified as English\"])\n",
    "print(label_encoder.classes_)\n",
    "model.predict(data_in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
