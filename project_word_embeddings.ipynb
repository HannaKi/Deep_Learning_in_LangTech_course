{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project word embeddings\n",
    "\n",
    "The project will have you train and thoroughly test word embeddings.\n",
    "\n",
    "## Milestone 1 -- word2vec\n",
    "\n",
    "Train `word2vec` and vary the training parameters, especially the corpus, the window size, and the word2vec architecture (skip-gram vs. BoW). Inspect the embeddings manually using the nearest neighbor, and write up your observations. How do these models differ? For models trained on two different datasets, or with two different architectures, what is the proportion of nearest neighbors which remain unchanged? Can you see any more general trends here? Write a report on your findings. Choose any language you want for your tests.\n",
    "\n",
    "## Milestone 2 -- formal evaluation\n",
    "\n",
    "There are many formal ways to evaluate the embeddings. Test all your models on several of these, and rank them. Do you see any general trends here? Add these findings to your report. Evaluation datasets:\n",
    "\n",
    "* http://wordvectors.org -- web based evaluation of English similarity\n",
    "* https://github.com/spyysalo/wvlib -- several English datasets and evaluation scripts\n",
    "* analogy_fin and analogy_en in the data directory of the course\n",
    "\n",
    "## Milestone 3 -- analogy as a mapping\n",
    "\n",
    "The analogy task uses a simple vector arithmetics to solve problems of the type \"king - man + woman = queen\". You have seen some of my skepticism towards this in the class, and you tested the validity of this arithmetics for yourself in the classes and in milestone 2. You were also shown how to map the embeddings - on the example of mapping between two languages. The king-man-woman-queen style analogy is naturally also a mapping of a kind and therefore it should be learnable like any other mapping. In this milestone, you should therefore solve the word analogy as a learnable task, applying to it your knowledge from the cross-lingual mapping example. Are you able to get any better results, especially for Finnish where the basic analogy does not work that great? Do the learned weights which combine the three vectors to obtain the fourth correspond to the +1 -1 and +1 as in the simple arithmetics? Or are they entirely different? Are there any differeces in performance on the various sub-tasks of the analogy dataset? Summarize your findings.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "venv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
