{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Copy of rnn_named_entity_recognition.ipynb","provenance":[{"file_id":"https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/rnn_named_entity_recognition.ipynb","timestamp":1585823644099}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Y-9s5_atdFTr","colab_type":"text"},"source":["<h2>Named entity recognition</h2>\n","<br>\n","Sequence labelling: input word sequence, output tag sequence; inputs and outputs paired (one output for one input)."]},{"cell_type":"markdown","metadata":{"id":"LPWDkMundFTw","colab_type":"text"},"source":["<img src=\"https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/ner.png?raw=1\" width=\"800\" />"]},{"cell_type":"markdown","metadata":{"id":"5aCrVJ42dFT2","colab_type":"text"},"source":["<h3>Load CoNLL'03 data</h3>"]},{"cell_type":"markdown","metadata":{"id":"opPg8X8LdFT5","colab_type":"text"},"source":["We use the CoNLL'03 dataset. The data in their original format can be found here https://github.com/glample/tagger .\n","\n","In this demo, the dataset has already been converted to json files. The converted data are found here under the file names `ner_train.json` and `ner_test.json`: https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/tree/master/data The code used for convertion can be found here: https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/read_ner.ipynb \n","\n","First, download the data:"]},{"cell_type":"code","metadata":{"id":"kN0n7zFCdFT9","colab_type":"code","outputId":"3ffbd91e-e973-4e0a-b9e2-e3c936978a46","colab":{"base_uri":"https://localhost:8080/","height":643},"executionInfo":{"status":"ok","timestamp":1585824148623,"user_tz":-180,"elapsed":5986,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n","!wget -nc https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/ner-conll03-en-train.json\n","!wget -nc https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/ner-conll03-en-dev.json\n","!wget -nc https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/ner-conll03-en-test.json"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-04-02 10:42:24--  https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/ner-conll03-en-train.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5960031 (5.7M) [text/plain]\n","Saving to: ‘ner-conll03-en-train.json’\n","\n","\r          ner-conll   0%[                    ]       0  --.-KB/s               \rner-conll03-en-trai 100%[===================>]   5.68M  --.-KB/s    in 0.07s   \n","\n","2020-04-02 10:42:24 (84.2 MB/s) - ‘ner-conll03-en-train.json’ saved [5960031/5960031]\n","\n","--2020-04-02 10:42:26--  https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/ner-conll03-en-dev.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1488781 (1.4M) [text/plain]\n","Saving to: ‘ner-conll03-en-dev.json’\n","\n","ner-conll03-en-dev. 100%[===================>]   1.42M  --.-KB/s    in 0.07s   \n","\n","2020-04-02 10:42:26 (19.9 MB/s) - ‘ner-conll03-en-dev.json’ saved [1488781/1488781]\n","\n","--2020-04-02 10:42:27--  https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/ner-conll03-en-test.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1370190 (1.3M) [text/plain]\n","Saving to: ‘ner-conll03-en-test.json’\n","\n","ner-conll03-en-test 100%[===================>]   1.31M  --.-KB/s    in 0.07s   \n","\n","2020-04-02 10:42:28 (20.1 MB/s) - ‘ner-conll03-en-test.json’ saved [1370190/1370190]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-IYP-hz3dFUf","colab_type":"text"},"source":["The data is divided into separate sentences and has also been tokenized already. You don't have to do any preprocessing. The produced json files contain a single dictionary for each sentence. The dictionary has a list of tokens and the corresponding list of labels, e.g.: { \"text\": [ \"EU\", \"rejects\", \"German\", \"call\", \"to\", \"boycott\", \"British\", \"lamb\", \".\" ], \"tags\": [ \"I-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\" ] },"]},{"cell_type":"code","metadata":{"id":"F-ed8o8PdFUi","colab_type":"code","outputId":"39fc62b4-66bf-4364-9314-89bee5280cce","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"ok","timestamp":1585824171949,"user_tz":-180,"elapsed":836,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["import json\n","\n","\n","with open(\"ner-conll03-en-train.json\") as f:\n","    data = json.load(f)\n","\n","    \n","# Look at the data\n","print(type(data))\n","print(type(data[0]))\n","print(data[0].keys())\n","print(data[0]) # tokenaized data"],"execution_count":2,"outputs":[{"output_type":"stream","text":["<class 'list'>\n","<class 'dict'>\n","dict_keys(['text', 'tags'])\n","{'text': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'tags': ['I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yihP2J6XdFU3","colab_type":"text"},"source":["We won't be making use of cross-sentence information, so we can shuffle the data on the sentence level (**not** the token level). Let's also separate the texts and tags from the data."]},{"cell_type":"code","metadata":{"id":"3O46ih1wdFU6","colab_type":"code","outputId":"f8148361-0ecf-4e7a-b6ba-17f69e667163","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1585824254262,"user_tz":-180,"elapsed":1468,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["import random\n","import numpy\n","\n","\n","random.seed(123)    # This makes the shuffle produce the same order every time\n","random.shuffle(data)\n","\n","train_texts = [example[\"text\"] for example in data]\n","train_labels = [example[\"tags\"] for example in data]\n","\n","# Example text and labels\n","print('Text:', train_texts[0])\n","print('Label:', train_labels[0])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Text: ['9/16', '-', 'Luo', 'Yigang', '(', 'China', ')', 'beat', 'Jason', 'Wong', '(', 'Malaysia', ')', '15-5', '15-6']\n","Label: ['O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eqGKyd3pdFVR","colab_type":"text"},"source":["Load the development (validation) data. A separate validation set involving different documents than the train data is used as splitting one set of documents into training and validation data would result in overly optimistic evaluation results due to e.g. repeated names."]},{"cell_type":"code","metadata":{"id":"3U61LUETdFVW","colab_type":"code","colab":{}},"source":["with open(\"ner-conll03-en-dev.json\") as f:\n","    validation_data = json.load(f)\n","\n","validation_texts = [example[\"text\"] for example in validation_data]\n","validation_labels = [example[\"tags\"] for example in validation_data]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xZElQMpydFVk","colab_type":"text"},"source":["### Load pretrained embeddings\n","\n","We'll initialize our embeddings with word vectors pretrained on the English Wikipedia.\n","\n","(Don't worry if the download, unpacking and loading take some time, this is a fair amount of data.)\n","\n","Download embeddings from https://fasttext.cc/docs/en/english-vectors.html"]},{"cell_type":"code","metadata":{"id":"hBACdqD2dFVo","colab_type":"code","outputId":"650a8fa7-a03d-406d-9ad8-393fa81ff98f","colab":{"base_uri":"https://localhost:8080/","height":239},"executionInfo":{"status":"ok","timestamp":1585824352303,"user_tz":-180,"elapsed":16963,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"],"execution_count":5,"outputs":[{"output_type":"stream","text":["--2020-04-02 10:45:39--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4a8e, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 681808098 (650M) [application/zip]\n","Saving to: ‘wiki-news-300d-1M.vec.zip’\n","\n","wiki-news-300d-1M.v 100%[===================>] 650.22M  55.4MB/s    in 13s     \n","\n","2020-04-02 10:45:51 (51.9 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NAkVu7YldFV0","colab_type":"code","outputId":"770f168b-8bd1-44a2-f07f-2d1b98df465b","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1585824383139,"user_tz":-180,"elapsed":36510,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["# Give -n argument so that a possible existing file isn't overwritten \n","!unzip -n wiki-news-300d-1M.vec.zip"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Archive:  wiki-news-300d-1M.vec.zip\n","  inflating: wiki-news-300d-1M.vec   \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mv8I9ssidFV-","colab_type":"text"},"source":["We use gensim to read the embedding model.\n","\n","Note that we are using a vocabulary size of only 50,000 so that the model trains faster."]},{"cell_type":"code","metadata":{"id":"J4dNVCTZdFV_","colab_type":"code","outputId":"c3353389-71cc-404d-96ad-234573be532f","colab":{"base_uri":"https://localhost:8080/","height":184},"executionInfo":{"status":"ok","timestamp":1585824445174,"user_tz":-180,"elapsed":16236,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["from gensim.models import KeyedVectors\n","\n","vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n","\n","\n","# sort based on the index to make sure they are in the correct order\n","words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n","print(\"Words from embedding model:\", len(words))\n","print(\"First 50 words:\", words[:50])\n","\n","# Normalize the vectors to unit length\n","print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n","vector_model.init_sims(replace=True)\n","print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Words from embedding model: 50000\n","First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n","Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n"," -0.0063]\n","After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n","  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cVUImQEYdFWI","colab_type":"code","outputId":"d0585678-6601-4f02-eb9c-2392837521fe","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1585824681575,"user_tz":-180,"elapsed":1043,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["# Build vocabulary mappings\n","\n","# Zero is used for padding in Keras, prevent using it for a normal word.\n","# Also reserve an index for out-of-vocabulary items. No word can be cast out in NER!\n","vocabulary={\n","    \"<PAD>\": 0,\n","    \"<OOV>\": 1\n","}\n","\n","for word in words: # These are words from the word2vec model\n","    vocabulary.setdefault(word, len(vocabulary))\n","\n","print(\"Words in vocabulary:\",len(vocabulary))\n","inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n","\n","\n","# Embedding matrix\n","def load_pretrained_embeddings(vocab, embedding_model):\n","    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n","    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n","    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n","    found=0\n","    for word,idx in vocab.items():\n","        if word in embedding_model.vocab:\n","            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n","            found+=1\n","            \n","    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n","    return pretrained_embeddings\n","\n","pretrained=load_pretrained_embeddings(vocabulary, vector_model)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Words in vocabulary: 50002\n","Found pretrained vectors for 50000 words.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G6es8xF7dFWT","colab_type":"text"},"source":["<h3>Preprocess data</h3>\n","<br>\n","Both the text and the labels are strings; we'll need to convert the labels into integers and the text into an appropriate format for RNN training.<br>\n","<h4>Labels</h4>"]},{"cell_type":"code","metadata":{"id":"T3SQ33imdFWU","colab_type":"code","outputId":"215555d3-8ad6-4b0c-e8ff-1d71c2b51783","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1585824611862,"user_tz":-180,"elapsed":710,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["from pprint import pprint\n","\n","\n","# Label mappings\n","# 1) gather a set of unique labels\n","label_set = set()\n","for sentence_labels in train_labels: #loops over sentences \n","    for label in sentence_labels: #loops over labels in one sentence\n","        label_set.add(label)\n","\n","# 2) index these\n","label_map = {}\n","for index, label in enumerate(label_set):\n","    label_map[label]=index\n","    \n","pprint(label_map)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["{'I-LOC': 3, 'I-ORG': 1, 'I-PER': 2, 'O': 0}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mtYU3QtxdFWc","colab_type":"code","outputId":"e3c00200-9b52-4a72-ede2-5d0e99e8a5e9","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1585824634557,"user_tz":-180,"elapsed":834,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["# vectorize the labels\n","def label_vectorizer(labels,label_map):\n","    vectorized_labels = []\n","    for label in labels:\n","        vectorized_example_label = []\n","        for token in label:\n","            vectorized_example_label.append(label_map[token])\n","        vectorized_labels.append(vectorized_example_label)\n","    vectorized_labels = numpy.array(vectorized_labels)\n","    return vectorized_labels\n","        \n","\n","vectorized_labels = label_vectorizer(train_labels, label_map)\n","validation_vectorized_labels = label_vectorizer(validation_labels, label_map)\n","\n","pprint(vectorized_labels[0])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[0, 0, 2, 2, 0, 3, 0, 0, 2, 2, 0, 3, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jAjcor7JdFWl","colab_type":"text"},"source":["<h4>Texts</h4>"]},{"cell_type":"markdown","metadata":{"id":"XEx4-uKVdFWn","colab_type":"text"},"source":["Next comes the vectorization of the texts."]},{"cell_type":"code","metadata":{"id":"oi3-Vs2jdFWp","colab_type":"code","outputId":"cb408496-de14-4622-d8ea-1b4be93f8934","colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"status":"ok","timestamp":1585824688973,"user_tz":-180,"elapsed":730,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["def text_vectorizer(vocab, texts):\n","    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n","    sentence_lengths = [] # Number of tokens in each sentence\n","    \n","    for i, one_example in enumerate(texts):\n","        vectorized_example = []\n","        for word in one_example:\n","            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n","\n","        vectorized_data.append(vectorized_example)     \n","        sentence_lengths.append(len(one_example))\n","        \n","    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n","    \n","    return vectorized_data, sentence_lengths\n","\n","vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n","validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, validation_texts)\n","\n","print(train_texts[0])\n","print(vectorized_data[0])\n","print([inv_vocabulary[i] for i in vectorized_data[0]])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["['9/16', '-', 'Luo', 'Yigang', '(', 'China', ')', 'beat', 'Jason', 'Wong', '(', 'Malaysia', ')', '15-5', '15-6']\n","[1, 37, 32297, 1, 14, 398, 12, 2270, 4292, 12601, 14, 3826, 12, 1, 1]\n","['<OOV>', '-', 'Luo', '<OOV>', '(', 'China', ')', 'beat', 'Jason', 'Wong', '(', 'Malaysia', ')', '<OOV>', '<OOV>']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dcxkUhAIdFW0","colab_type":"text"},"source":["Note that there are a lot of out-of-vocabulary tokens because of the small vocabulary we use. The number of `<OOV>` tokens can be decreased by using a larger vocabulary.<br><br>\n","We have to pad the sequences so that all of them are of the same length."]},{"cell_type":"code","metadata":{"id":"aQlF3g6adFW2","colab_type":"code","outputId":"b207fda1-afdd-40a1-9995-568116e0d00e","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1585824697171,"user_tz":-180,"elapsed":2782,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["import tensorflow as tf\n","from keras.preprocessing.sequence import pad_sequences\n","\n","\n","print(\"Old shape:\", vectorized_data.shape)\n","vectorized_data_padded=pad_sequences(vectorized_data, padding='post', maxlen=max(lengths))\n","print(\"New shape:\", vectorized_data_padded.shape)\n","print(\"First example:\", vectorized_data_padded[0])\n","\n","# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n","vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='post', maxlen=max(lengths)), -1)\n","print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n","pprint(label_map)\n","print(\"First example labels:\")\n","pprint(vectorized_labels_padded[0])\n","\n","weights = numpy.copy(vectorized_data_padded)\n","weights[weights > 0] = 1\n","print(\"First weight vector:\", weights[0])\n","\n","# Same stuff for the validation data\n","validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='post', maxlen=max(lengths))\n","validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='post',maxlen=max(lengths)), -1)\n","validation_weights = numpy.copy(validation_vectorized_data_padded)\n","validation_weights[validation_weights > 0] = 1"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Old shape: (14041,)\n","New shape: (14041, 113)\n","First example: [    1    37 32297     1    14   398    12  2270  4292 12601    14  3826\n","    12     1     1     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0]\n","Padded labels shape: (14041, 113, 1)\n","{'I-LOC': 3, 'I-ORG': 1, 'I-PER': 2, 'O': 0}\n","First example labels:\n","array([[0],\n","       [0],\n","       [2],\n","       [2],\n","       [0],\n","       [3],\n","       [0],\n","       [0],\n","       [2],\n","       [2],\n","       [0],\n","       [3],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0],\n","       [0]], dtype=int32)\n","First weight vector: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jsG2xjiNdFXB","colab_type":"text"},"source":["<h4>Evaluation function</h4>\n","<br>\n","The entities can continue beyond one token so the model has to evaluate predictions based on that. We write a function that converts the sequence of tokens into entities. We then write another function that calculates precision, recall, and f-score based on the predictions. We put these functions into a custom callback which we use during training."]},{"cell_type":"code","metadata":{"id":"UVeoqOHWdFXC","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","\n","\n","def _convert_to_entities(input_sequence):\n","    \"\"\"\n","    Reads a sequence of tags and converts them into a set of entities.\n","    \"\"\"\n","    entities = []\n","    current_entity = []\n","    previous_tag = label_map['O']\n","    for i, tag in enumerate(input_sequence):\n","        if tag != previous_tag and tag != label_map['O']: # New entity starts\n","            if len(current_entity) > 0:\n","                entities.append(current_entity)\n","                current_entity = []\n","            current_entity.append((tag, i))\n","        elif tag == label_map['O']: # Entity has ended\n","            if len(current_entity) > 0:\n","                entities.append(current_entity)\n","                current_entity = []\n","        elif tag == previous_tag: # Current entity continues\n","            current_entity.append((tag, i))\n","        previous_tag = tag\n","    \n","    # Add the last entity to our entity list if the sentences ends with an entity\n","    if len(current_entity) > 0:\n","        entities.append(current_entity)\n","    \n","    entity_offsets = set()\n","    \n","    for e in entities:\n","        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n","    return entity_offsets\n","\n","\n","def _entity_level_PRF(predictions, gold, lengths):\n","    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n","    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n","    \n","    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n","    pred_count = sum([len(e) for e in pred_entities])\n","    \n","    try:\n","        precision = tp / pred_count # tp / (tp+np)\n","        recall = tp / sum([len(e) for e in gold_entities])\n","        fscore = 2 * precision * recall / (precision + recall)\n","    except Exception as e:\n","        precision, recall, fscore = 0.0, 0.0, 0.0\n","    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n","    return precision, recall, fscore             \n","\n","\n","def evaluate(predictions, gold, lengths):\n","    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n","    return precision, recall, fscore\n","\n","\n","class EvaluateEntities(keras.callbacks.Callback):\n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n","        self.precision = []\n","        self.recall = []\n","        self.fscore = []\n","    def on_epoch_end(self, epoch, logs={}):\n","        self.losses.append(logs.get('loss'))\n","        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n","        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n","        self.precision.append(evaluation_parameters[0])\n","        self.recall.append(evaluation_parameters[1])\n","        self.fscore.append(evaluation_parameters[2])\n","        return"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXEKq3-bdFXJ","colab_type":"text"},"source":["<h3>Predicting word labels with word embeddings</h3>"]},{"cell_type":"markdown","metadata":{"id":"z77BTLscdFXK","colab_type":"text"},"source":["Before we get into RNNs, we first build a simpler model without any recurrent layers:\n","<br>\n","- input: sequence of `sequence_length` integers corresponding to words<br>\n","- embedding: pretrained mapping from integers to `vector_size`-dimensional vectors<br>\n","- hidden: `hidden_size`-dimensional fully connected layer with relu activation<br>\n","- output: `class_count`-dimensional fully connected layer with softmax activation<br>"]},{"cell_type":"code","metadata":{"id":"xf9-n--IdFXL","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n","from tensorflow.keras.optimizers import SGD, Adam\n","\n","\n","example_count, sequence_len = vectorized_data_padded.shape\n","class_count = len(label_set)\n","hidden_size = 100\n","\n","vector_size= pretrained.shape[1]\n","\n","\n","def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n","    inp=Input(shape=(sequence_len,))\n","    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n","    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings) # We change this activation function\n","    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n","    return Model(inputs=[inp], outputs=[outp])\n","\n","\n","model = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v4l8q4zOdFXU","colab_type":"code","outputId":"f061fd08-4ad5-4aac-8f2d-d3e7f1a99c91","colab":{}},"source":["print(model.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 113)               0         \n","_________________________________________________________________\n","embedding_2 (Embedding)      (None, 113, 300)          15000600  \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 113, 100)          160400    \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 113, 4)            404       \n","=================================================================\n","Total params: 15,161,404\n","Trainable params: 160,804\n","Non-trainable params: 15,000,600\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PN_k9I1-dFXb","colab_type":"text"},"source":["<h3>Train the model</h3>\n","<br>\n","We'll then compile and train our model."]},{"cell_type":"code","metadata":{"id":"dy6SK2vNdFXc","colab_type":"code","outputId":"9ca0bb2c-8c32-4d25-ba10-c736393a7344","colab":{"base_uri":"https://localhost:8080/","height":772},"executionInfo":{"status":"ok","timestamp":1585824845791,"user_tz":-180,"elapsed":16260,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["optimizer = Adam(lr=0.001) # define the learning rate\n","model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n","evaluation_function=EvaluateEntities()\n","\n","# train\n","vanilla_hist=model.fit(\n","    vectorized_data_padded,\n","    vectorized_labels_padded,\n","    sample_weight=weights,\n","    batch_size=100,\n","    verbose=2,\n","    epochs=10,\n","    callbacks=[evaluation_function]\n",")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","\n","Precision/Recall/F-score: 0.606687898089172 / 0.3794820717131474 / 0.46691176470588236\n","141/141 - 1s - loss: 0.0691\n","Epoch 2/10\n","\n","Precision/Recall/F-score: 0.5672842515845928 / 0.4635458167330677 / 0.5101951326463494\n","141/141 - 1s - loss: 0.0297\n","Epoch 3/10\n","\n","Precision/Recall/F-score: 0.5682622023107757 / 0.4800796812749004 / 0.5204621531152143\n","141/141 - 1s - loss: 0.0259\n","Epoch 4/10\n","\n","Precision/Recall/F-score: 0.5796449704142012 / 0.4878486055776892 / 0.5297998918334235\n","141/141 - 1s - loss: 0.0248\n","Epoch 5/10\n","\n","Precision/Recall/F-score: 0.5827489481065918 / 0.49661354581673306 / 0.5362443536244353\n","141/141 - 1s - loss: 0.0242\n","Epoch 6/10\n","\n","Precision/Recall/F-score: 0.5830801589156345 / 0.49701195219123506 / 0.5366168405204861\n","141/141 - 1s - loss: 0.0238\n","Epoch 7/10\n","\n","Precision/Recall/F-score: 0.5813036418464393 / 0.499203187250996 / 0.5371342835708927\n","141/141 - 1s - loss: 0.0235\n","Epoch 8/10\n","\n","Precision/Recall/F-score: 0.5901754385964912 / 0.502589641434263 / 0.5428725121032814\n","141/141 - 1s - loss: 0.0233\n","Epoch 9/10\n","\n","Precision/Recall/F-score: 0.598066949552098 / 0.5053784860557768 / 0.5478298423666594\n","141/141 - 1s - loss: 0.0231\n","Epoch 10/10\n","\n","Precision/Recall/F-score: 0.6004705882352941 / 0.5083665338645418 / 0.5505933117583603\n","141/141 - 1s - loss: 0.0230\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8XSugEEvdFXi","colab_type":"text"},"source":["Let's plot the f-scores"]},{"cell_type":"code","metadata":{"id":"u-5EpgirdFXj","colab_type":"code","outputId":"35f7ae58-54cd-46a6-9f67-75fc98d72991","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1585824881788,"user_tz":-180,"elapsed":832,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","\n","def plot_history(fscores):\n","    print(\"History:\", fscores)\n","    print(\"Highest f-score:\", max(fscores))\n","    plt.plot(fscores)\n","    plt.legend(loc='lower center', borderaxespad=0.)\n","    plt.show()\n","\n","\n","plot_history(evaluation_function.fscore)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["No handles with labels found to put in legend.\n"],"name":"stderr"},{"output_type":"stream","text":["History: [0.46691176470588236, 0.5101951326463494, 0.5204621531152143, 0.5297998918334235, 0.5362443536244353, 0.5366168405204861, 0.5371342835708927, 0.5428725121032814, 0.5478298423666594, 0.5505933117583603]\n","Highest f-score: 0.5505933117583603\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ffaKkLN9dFXr","colab_type":"text"},"source":["### Ideas to try yourself\n","\n","Try playing around with the neural network. You could, for example, try\n","\n","- Different activation functions\n","- Altering the learning rate\n","- Use different optimizers\n","- Adjusting the vocabulary size of the embeddings\n","\n","Activation functions and optimizers supported by Keras can be found here: https://keras.io/"]},{"cell_type":"markdown","metadata":{"id":"PgkO29J9dFXs","colab_type":"text"},"source":["<h3>Including the context</h3>"]},{"cell_type":"markdown","metadata":{"id":"dnBGyw4bdFXu","colab_type":"text"},"source":["We then include the context by defining and building an RNN model which has the following layers:\n","<br>\n","- input: sequence of `sequence_length` integers corresponding to words\n","- embedding: pretrained mapping from integers to `vector_size`-dimensional vectors\n","- rnn: recurrent neural network, specifically LSTM, with `rnn_size`-dimensional state\n","- output: `class_count`-dimensional fully connected layer with softmax activation"]},{"cell_type":"code","metadata":{"id":"khrLif87dFXz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"cfcfe92f-544e-4001-fe51-d105e1dfb1c5","executionInfo":{"status":"ok","timestamp":1585824914386,"user_tz":-180,"elapsed":1228,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["from tensorflow.keras.layers import LSTM\n","\n","\n","example_count, sequence_len = vectorized_data_padded.shape\n","class_count = len(label_set)\n","rnn_size = 100\n","\n","vector_size= pretrained.shape[1]\n","\n","\n","def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n","    inp=Input(shape=(sequence_len,))\n","    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n","    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n","    outp=Dense(class_count, activation=\"softmax\")(rnn)\n","    return Model(inputs=[inp], outputs=[outp])\n","\n","\n","rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9TSZFThjdFX8","colab_type":"code","outputId":"1e5dbece-a84b-4dee-9c37-6f1d58c7b220","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"ok","timestamp":1585825041858,"user_tz":-180,"elapsed":760,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["print(rnn_model.summary())"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, 113)]             0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 113, 300)          15000600  \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 113, 100)          160400    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 113, 4)            404       \n","=================================================================\n","Total params: 15,161,404\n","Trainable params: 160,804\n","Non-trainable params: 15,000,600\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J2oa8NcodFYC","colab_type":"text"},"source":["<h3>Train an RNN model</h3>\n","<br>\n","We compile and train our model"]},{"cell_type":"code","metadata":{"id":"QNeFIwzRdFYD","colab_type":"code","outputId":"992b0c87-6200-4aee-ed92-268090381044","colab":{"base_uri":"https://localhost:8080/","height":772},"executionInfo":{"status":"ok","timestamp":1585825660338,"user_tz":-180,"elapsed":532131,"user":{"displayName":"Hanna Kitti","photoUrl":"","userId":"01511785927733709431"}}},"source":["optimizer=Adam(lr=0.001) # define the learning rate\n","rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n","\n","evaluation_function=EvaluateEntities()\n","\n","# train\n","rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","\n","Precision/Recall/F-score: 0.2059238363892807 / 0.02908366533864542 / 0.05096875545470413\n","141/141 - 51s - loss: 0.0774\n","Epoch 2/10\n","\n","Precision/Recall/F-score: 0.6729126481124277 / 0.4864541832669323 / 0.5646895594866459\n","141/141 - 53s - loss: 0.0361\n","Epoch 3/10\n","\n","Precision/Recall/F-score: 0.7007801743919229 / 0.6083665338645419 / 0.6513115802943058\n","141/141 - 51s - loss: 0.0237\n","Epoch 4/10\n","\n","Precision/Recall/F-score: 0.7161389621811786 / 0.6488047808764941 / 0.6808110367892978\n","141/141 - 52s - loss: 0.0199\n","Epoch 5/10\n","\n","Precision/Recall/F-score: 0.729676140118969 / 0.6597609561752988 / 0.6929595145935769\n","141/141 - 53s - loss: 0.0178\n","Epoch 6/10\n","\n","Precision/Recall/F-score: 0.7482158786797503 / 0.6683266932270916 / 0.7060185185185186\n","141/141 - 53s - loss: 0.0166\n","Epoch 7/10\n","\n","Precision/Recall/F-score: 0.7420388972002565 / 0.6916334661354582 / 0.715950097948242\n","141/141 - 53s - loss: 0.0158\n","Epoch 8/10\n","\n","Precision/Recall/F-score: 0.7483007646559049 / 0.7017928286852589 / 0.7243009868421052\n","141/141 - 53s - loss: 0.0151\n","Epoch 9/10\n","\n","Precision/Recall/F-score: 0.749893025246042 / 0.6982071713147411 / 0.7231277078605324\n","141/141 - 53s - loss: 0.0146\n","Epoch 10/10\n","\n","Precision/Recall/F-score: 0.7496812579685508 / 0.7027888446215139 / 0.7254780999383096\n","141/141 - 53s - loss: 0.0141\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_sppOYeBdFYa","colab_type":"text"},"source":["Let's plot the f-scores"]},{"cell_type":"code","metadata":{"id":"ArHaHpsPdFYd","colab_type":"code","outputId":"c50228cd-15a7-436e-917f-27a385b22c24","colab":{}},"source":["%matplotlib inline\n","\n","plot_history(evaluation_function.fscore)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["No handles with labels found to put in legend.\n"],"name":"stderr"},{"output_type":"stream","text":["History: [0.0, 0.5047705588368925, 0.6336435504853397, 0.6651690201112538, 0.7012252591894438, 0.7080045095828635, 0.7019754404698345, 0.7156576200417536, 0.7230527966544694, 0.7202739157501556]\n","Highest f-score: 0.7230527966544694\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"5ZSYjAYOdFYj","colab_type":"text"},"source":["By including the context, our f-score increase by over 15 points! This isn't good performance, but should demonstrate the value of recurrent networks for the task."]},{"cell_type":"markdown","metadata":{"id":"_UJ6sJSydFYm","colab_type":"text"},"source":["### Ideas to try yourself\n","\n","Try altering the structure of the network. You could, for example\n","\n","- Try different RNNs (LSTM, biLSTM)\n","- Try using CNNs (hint: use the Conv1D layer, and the Dense layer afterwards needs a TimeDistributed wrapper layer)\n","\n","Again, the layers supported by Keras can be found from its documentation: https://keras.io/"]}]}