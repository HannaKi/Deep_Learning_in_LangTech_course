{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data, vocabulary and pretrained embeddings\n",
    "These parts are similar to the previous examples. Things to note though:\n",
    "* Our data has already been tokenized and divided into sentences\n",
    "* We _cannot_ skip tokens\n",
    "* We are using a specific OOV (out-of-vocabulary) embedding for all words which are not present in our vocab\n",
    "* We now have one label for each token, not for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], 'text': ['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']}\n",
      "[['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.'], ['[', 'This', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']]\n",
      "[['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], ['PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'AUX', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADP', 'NOUN', 'PART', 'VERB', 'PUNCT', 'PUNCT']]\n",
      "Words from embedding model: 50000\n",
      "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
      "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
      " -0.0063]\n",
      "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n",
      "Words in vocabulary: 50002\n",
      "Found pretrained vectors for 50000 words.\n"
     ]
    }
   ],
   "source": [
    "# Load our training data\n",
    "import json\n",
    "import random\n",
    "import numpy\n",
    "with open(\"data/pos_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "print(data[0])\n",
    "\n",
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"tags\"] for one_example in data] # This is now a list of lists just like the texts variable\n",
    "print(texts[:2])\n",
    "print(labels[:2])\n",
    "\n",
    "# Lets do the same thing for the validation data\n",
    "# We use a separate validation set, since generally using sentences from the same documents as train/validation results in overly optimistic scores\n",
    "with open(\"data/pos_devel.json\") as f:\n",
    "    validation_data=json.load(f)\n",
    "validation_texts=[one_example[\"text\"] for one_example in data]\n",
    "validation_labels=[one_example[\"tags\"] for one_example in data]\n",
    "\n",
    "# Use gensim to read the embedding model\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model=KeyedVectors.load_word2vec_format(\"data/wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words=[k for k,v in sorted(vector_model.vocab.items(), key=lambda x:x[1].index)]\n",
    "print(\"Words from embedding model:\",len(words))\n",
    "print(\"First 50 words:\",words[:50])\n",
    "\n",
    "# Normalize the vectors\n",
    "\n",
    "print(\"Before normalization:\",vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\",vector_model.get_vector(\"in\")[:10])\n",
    "\n",
    "# Build vocabulary mappings\n",
    "\n",
    "vocabulary={\"<SPECIAL>\": 0, \"<OOV>\": 1} # zero has a special meaning in sequence models, prevent using it for a normal word\n",
    "for word in words:\n",
    "    vocabulary.setdefault(word, len(vocabulary))\n",
    "\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "inversed_vocabulary={value:key for key, value in vocabulary.items()} # inverse the dictionary\n",
    "\n",
    "# Label mappings\n",
    "label_set = set([label for sentence_labels in labels for label in sentence_labels])\n",
    "label_map = {label: index for index, label in enumerate(label_set)}\n",
    "                \n",
    "# Embedding matrix\n",
    "\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings=numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
    "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing data\n",
    "If we want to consider the task as sequence labeling, we should feed the input data as word sequences and outputs as label sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def vectorizer(vocab, texts, label_map, labels):\n",
    "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
    "    vectorized_labels = [] # same thing for the labels\n",
    "    sentence_lengths = [] # Number of tokens in each sentence\n",
    "    \n",
    "    for i, one_example in enumerate(texts):\n",
    "        vectorized_example = []\n",
    "        vectorized_example_labels = []\n",
    "        for word in one_example:\n",
    "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
    "        \n",
    "        for label in labels[i]:\n",
    "            vectorized_example_labels.append(label_map[label])\n",
    "\n",
    "        vectorized_data.append(vectorized_example)\n",
    "        vectorized_labels.append(vectorized_example_labels)\n",
    "        \n",
    "        sentence_lengths.append(len(one_example))\n",
    "        \n",
    "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy matrix\n",
    "    vectorized_labels = numpy.array(vectorized_labels)\n",
    "    \n",
    "    return vectorized_data, vectorized_labels, sentence_lengths\n",
    "\n",
    "vectorized_data, vectorized_labels, lengths=vectorizer(vocabulary, texts, label_map, labels)\n",
    "validation_vectorized_data, validation_vectorized_labels, validation_lengths=vectorizer(vocabulary, validation_texts, label_map, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "We add padding to the label sequences as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kahaka/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (12543,)\n",
      "New shape: (12543, 159)\n",
      "First example: [ 3424    37     1    11   285  1084   974 34462 10554  4733    37 43264\n",
      "     2     3 16500    29     3  8683     8     3   754     6     1     2\n",
      "   504     3  4761  1757     4     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "Padded labels shape: (12543, 159, 1)\n",
      "{'SYM': 0, 'AUX': 1, 'PUNCT': 2, 'SCONJ': 3, 'VERB': 6, 'PRON': 5, 'CCONJ': 7, 'ADV': 8, 'PART': 9, 'INTJ': 10, 'PROPN': 11, 'NOUN': 12, 'DET': 15, 'ADJ': 13, 'ADP': 14, 'X': 4, 'NUM': 16}\n",
      "First example labels: [[11]\n",
      " [ 2]\n",
      " [11]\n",
      " [ 2]\n",
      " [13]\n",
      " [12]\n",
      " [ 6]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [ 2]\n",
      " [11]\n",
      " [ 2]\n",
      " [15]\n",
      " [12]\n",
      " [14]\n",
      " [15]\n",
      " [12]\n",
      " [14]\n",
      " [15]\n",
      " [12]\n",
      " [14]\n",
      " [11]\n",
      " [ 2]\n",
      " [14]\n",
      " [15]\n",
      " [13]\n",
      " [12]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]]\n",
      "First weight vector: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='post')\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\", vectorized_data_padded[0])\n",
    "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
    "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='post'), -1)\n",
    "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
    "print(label_map)\n",
    "print(\"First example labels:\", vectorized_labels_padded[0])\n",
    "\n",
    "weights = numpy.copy(vectorized_data_padded)\n",
    "weights[weights > 0] = 1\n",
    "print(\"First weight vector:\", weights[0])\n",
    "\n",
    "# Same stuff for the validation data\n",
    "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='post')\n",
    "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='post'), -1)\n",
    "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
    "validation_weights[weights > 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating POS tags\n",
    "Keras does not use sample weighting in metrics (only for losses) (correct me if I'm wrong), so we have to create our own evaluation if we want to ignore padding in models which do not support masking (e.g. convolution).\n",
    "Thus, to have evaluation that is identical for all models, we have to create our own script, which will ignore padded parts of the sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(predictions, gold, lengths):\n",
    "    pred_tags = numpy.concatenate([labels[:lengths[i]] for i, labels in enumerate(predictions)]).ravel()\n",
    "    \n",
    "    gold_tags = numpy.concatenate([labels[:lengths[i], 0] for i, labels in enumerate(gold)]).ravel()\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(gold_tags, pred_tags))\n",
    "\n",
    "class EvaluateTags(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
    "        accuracy(pred, validation_vectorized_labels_padded, validation_lengths) # FIXME: Using global variables here, not good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent classification\n",
    "Time-distributed means that the same dense layer is applied to each time step. This means that we are now simply using a normal feedforward network to classify each word/token separately.\n",
    "\n",
    "Why didn't we one-hot encode our labels? :S\n",
    "\n",
    "It's because the sparse loss is doing it for us implicitly! Neat, right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, Conv1D, TimeDistributed, LSTM, Bidirectional\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "\n",
    "vector_size= pretrained.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 159)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 159, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 159, 100)          30100     \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 159, 17)           1717      \n",
      "=================================================================\n",
      "Total params: 15,032,417\n",
      "Trainable params: 31,817\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "12543/12543 [==============================] - 8s 647us/step - loss: 27.0499\n",
      "Accuracy: 0.30482828055736116\n",
      "Epoch 2/100\n",
      "12543/12543 [==============================] - 8s 641us/step - loss: 25.7625\n",
      "Accuracy: 0.3310297301656346\n",
      "Epoch 3/100\n",
      "12543/12543 [==============================] - 9s 698us/step - loss: 23.9909\n",
      "Accuracy: 0.42641258607965515\n",
      "Epoch 4/100\n",
      "12543/12543 [==============================] - 9s 726us/step - loss: 21.6282\n",
      "Accuracy: 0.4943623629690089\n",
      "Epoch 5/100\n",
      "12543/12543 [==============================] - 9s 724us/step - loss: 19.1440\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "hidden = TimeDistributed(Dense(100, activation=\"softmax\"))(embeddings)\n",
    "outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=1,epochs=100, callbacks=[EvaluateTags()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding context with convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 113, 100)          90100     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 113, 4)            404       \n",
      "=================================================================\n",
      "Total params: 15,091,104\n",
      "Trainable params: 90,504\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.4250\n",
      "Precision/Recall/F-score: 0.6337763075343638 / 0.5548786577449316 / 0.591709044436753\n",
      "14041/14041 [==============================] - 8s 590us/step - loss: 0.4213\n",
      "Epoch 2/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.1614\n",
      "Precision/Recall/F-score: 0.7273012552301256 / 0.6943972835314092 / 0.7104685025289941\n",
      "14041/14041 [==============================] - 4s 313us/step - loss: 0.1605\n",
      "Epoch 3/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.1283\n",
      "Precision/Recall/F-score: 0.7467646754322393 / 0.7203635274143614 / 0.7333265555103702\n",
      "14041/14041 [==============================] - 4s 293us/step - loss: 0.1280\n",
      "Epoch 4/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.1161\n",
      "Precision/Recall/F-score: 0.7585977822065512 / 0.7412863277738939 / 0.7498421517868418\n",
      "14041/14041 [==============================] - 4s 289us/step - loss: 0.1160\n",
      "Epoch 5/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.1099\n",
      "Precision/Recall/F-score: 0.7705428731934018 / 0.7534205532807351 / 0.7618855252859343\n",
      "14041/14041 [==============================] - 4s 294us/step - loss: 0.1098\n",
      "Epoch 6/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.1046\n",
      "Precision/Recall/F-score: 0.7863476572747783 / 0.7483771097573155 / 0.7668926698221824\n",
      "14041/14041 [==============================] - 4s 269us/step - loss: 0.1046\n",
      "Epoch 7/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.1007\n",
      "Precision/Recall/F-score: 0.7557491786887588 / 0.7926195945271147 / 0.7737453995954081\n",
      "14041/14041 [==============================] - 4s 289us/step - loss: 0.1002\n",
      "Epoch 8/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0969\n",
      "Precision/Recall/F-score: 0.786902495240004 / 0.7842305003495456 / 0.7855642256902762\n",
      "14041/14041 [==============================] - 4s 274us/step - loss: 0.0972\n",
      "Epoch 9/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0941\n",
      "Precision/Recall/F-score: 0.8076535215752879 / 0.7598621791670828 / 0.7830293050659943\n",
      "14041/14041 [==============================] - 4s 271us/step - loss: 0.0942\n",
      "Epoch 10/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0914\n",
      "Precision/Recall/F-score: 0.8045163290615956 / 0.7774393288724658 / 0.790746101884301\n",
      "14041/14041 [==============================] - 4s 250us/step - loss: 0.0914\n",
      "Epoch 11/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0894\n",
      "Precision/Recall/F-score: 0.8064713687863632 / 0.789074203535404 / 0.7976779404341241\n",
      "14041/14041 [==============================] - 4s 278us/step - loss: 0.0892\n",
      "Epoch 12/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0867\n",
      "Precision/Recall/F-score: 0.7801290567470108 / 0.8210326575451913 / 0.8000583913191572\n",
      "14041/14041 [==============================] - 4s 255us/step - loss: 0.0867\n",
      "Epoch 13/100\n",
      "13200/14041 [===========================>..] - ETA: 0s - loss: 0.0845\n",
      "Precision/Recall/F-score: 0.7828568710459972 / 0.8218316188954359 / 0.8018709347365344\n",
      "14041/14041 [==============================] - 4s 268us/step - loss: 0.0851\n",
      "Epoch 14/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0821\n",
      "Precision/Recall/F-score: 0.7980834711547816 / 0.819284929591531 / 0.8085452395032525\n",
      "14041/14041 [==============================] - 4s 309us/step - loss: 0.0824\n",
      "Epoch 15/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 0.0809\n",
      "Precision/Recall/F-score: 0.826640447112399 / 0.7976630380505343 / 0.8118932655654384\n",
      "14041/14041 [==============================] - 4s 303us/step - loss: 0.0809\n",
      "Epoch 16/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0795\n",
      "Precision/Recall/F-score: 0.8025378989683731 / 0.8274243483471487 / 0.8147911390848966\n",
      "14041/14041 [==============================] - 4s 276us/step - loss: 0.0795\n",
      "Epoch 17/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0776\n",
      "Precision/Recall/F-score: 0.837376879212027 / 0.8066014181563966 / 0.821701088615322\n",
      "14041/14041 [==============================] - 4s 289us/step - loss: 0.0776\n",
      "Epoch 18/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0760\n",
      "Precision/Recall/F-score: 0.8032786885245902 / 0.8368121442125237 / 0.8197026022304834\n",
      "14041/14041 [==============================] - 4s 292us/step - loss: 0.0762\n",
      "Epoch 19/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 0.0745\n",
      "Precision/Recall/F-score: 0.8048827098561941 / 0.8412563667232598 / 0.8226676758551652\n",
      "14041/14041 [==============================] - 4s 303us/step - loss: 0.0748\n",
      "Epoch 20/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 0.0735\n",
      "Precision/Recall/F-score: 0.8012252591894439 / 0.8489963048037551 / 0.8244193376327402\n",
      "14041/14041 [==============================] - 4s 291us/step - loss: 0.0734\n",
      "Epoch 21/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0716\n",
      "Precision/Recall/F-score: 0.8084883166428231 / 0.8465994207530211 / 0.827105083422773\n",
      "14041/14041 [==============================] - 5s 347us/step - loss: 0.0718\n",
      "Epoch 22/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0708\n",
      "Precision/Recall/F-score: 0.813953488372093 / 0.845900329571557 / 0.829619472060336\n",
      "14041/14041 [==============================] - 4s 317us/step - loss: 0.0709\n",
      "Epoch 23/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0695\n",
      "Precision/Recall/F-score: 0.8213090097520741 / 0.8453011085588734 / 0.8331323670546545\n",
      "14041/14041 [==============================] - 4s 288us/step - loss: 0.0697\n",
      "Epoch 24/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0682\n",
      "Precision/Recall/F-score: 0.8245333333333333 / 0.8491960451413163 / 0.8366829844284274\n",
      "14041/14041 [==============================] - 5s 335us/step - loss: 0.0682\n",
      "Epoch 25/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0675\n",
      "Precision/Recall/F-score: 0.8252230528092597 / 0.854439229002297 / 0.8395770466867839\n",
      "14041/14041 [==============================] - 4s 307us/step - loss: 0.0674\n",
      "Epoch 26/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0664\n",
      "Precision/Recall/F-score: 0.8280088794517904 / 0.8567861779686408 / 0.8421517620496712\n",
      "14041/14041 [==============================] - 4s 284us/step - loss: 0.0663\n",
      "Epoch 27/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0654\n",
      "Precision/Recall/F-score: 0.8290895061728395 / 0.8584839708379107 / 0.8435307394141602\n",
      "14041/14041 [==============================] - 5s 348us/step - loss: 0.0655\n",
      "Epoch 28/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0643\n",
      "Precision/Recall/F-score: 0.8235992402659069 / 0.8661240387496255 / 0.8443265345859904\n",
      "14041/14041 [==============================] - 4s 317us/step - loss: 0.0644\n",
      "Epoch 29/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0632\n",
      "Precision/Recall/F-score: 0.8285042694042023 / 0.8624288425047438 / 0.8451262477980035\n",
      "14041/14041 [==============================] - 4s 310us/step - loss: 0.0634\n",
      "Epoch 30/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0628\n",
      "Precision/Recall/F-score: 0.8351195383347073 / 0.8599320882852292 / 0.8473442074445838\n",
      "14041/14041 [==============================] - 5s 341us/step - loss: 0.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0622\n",
      "Precision/Recall/F-score: 0.8357740080228119 / 0.8635274143613303 / 0.8494240734828205\n",
      "14041/14041 [==============================] - 4s 304us/step - loss: 0.0622\n",
      "Epoch 32/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0610\n",
      "Precision/Recall/F-score: 0.8401530192242507 / 0.8663737141715769 / 0.8530619268874302\n",
      "14041/14041 [==============================] - 5s 362us/step - loss: 0.0610\n",
      "Epoch 33/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0605\n",
      "Precision/Recall/F-score: 0.829796325309785 / 0.8727654049735344 / 0.8507386405120592\n",
      "14041/14041 [==============================] - 4s 305us/step - loss: 0.0604\n",
      "Epoch 34/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0597\n",
      "Precision/Recall/F-score: 0.8773286285091588 / 0.8442524717866773 / 0.8604728096292338\n",
      "14041/14041 [==============================] - 4s 288us/step - loss: 0.0596\n",
      "Epoch 35/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0591\n",
      "Precision/Recall/F-score: 0.8282866277976303 / 0.8796564466193948 / 0.853199011962997\n",
      "14041/14041 [==============================] - 4s 309us/step - loss: 0.0591\n",
      "Epoch 36/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0587\n",
      "Precision/Recall/F-score: 0.8709660947712419 / 0.8517427344452212 / 0.8612471598081293\n",
      "14041/14041 [==============================] - 4s 314us/step - loss: 0.0585\n",
      "Epoch 37/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0575\n",
      "Precision/Recall/F-score: 0.8760606839804577 / 0.8506441625886347 / 0.8631653619112766\n",
      "14041/14041 [==============================] - 5s 343us/step - loss: 0.0578\n",
      "Epoch 38/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0571\n",
      "Precision/Recall/F-score: 0.8406619610835558 / 0.880205732547688 / 0.8599795091964678\n",
      "14041/14041 [==============================] - 5s 370us/step - loss: 0.0572\n",
      "Epoch 39/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0563\n",
      "Precision/Recall/F-score: 0.8481731282449415 / 0.872865275142315 / 0.8603420696443952\n",
      "14041/14041 [==============================] - 4s 310us/step - loss: 0.0563\n",
      "Epoch 40/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0555\n",
      "Precision/Recall/F-score: 0.8333802552552553 / 0.8868970338559872 / 0.8593062073636848\n",
      "14041/14041 [==============================] - 5s 359us/step - loss: 0.0559\n",
      "Epoch 41/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0554\n",
      "Precision/Recall/F-score: 0.8502910472891712 / 0.8826026165984221 / 0.8661455908656557\n",
      "14041/14041 [==============================] - 4s 290us/step - loss: 0.0554\n",
      "Epoch 42/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0545\n",
      "Precision/Recall/F-score: 0.8561724171635227 / 0.8748127434335364 / 0.8653922149772771\n",
      "14041/14041 [==============================] - 5s 323us/step - loss: 0.0546\n",
      "Epoch 43/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0540\n",
      "Precision/Recall/F-score: 0.8420778482810872 / 0.8879956057125736 / 0.8644273770173051\n",
      "14041/14041 [==============================] - 4s 295us/step - loss: 0.0540\n",
      "Epoch 44/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0535\n",
      "Precision/Recall/F-score: 0.8907943187795897 / 0.8456007190652152 / 0.8676093862076031\n",
      "14041/14041 [==============================] - 4s 319us/step - loss: 0.0534\n",
      "Epoch 45/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 0.0532\n",
      "Precision/Recall/F-score: 0.8597742707773489 / 0.8787076800159792 / 0.8691378756822166\n",
      "14041/14041 [==============================] - 5s 322us/step - loss: 0.0532\n",
      "Epoch 46/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0523\n",
      "Precision/Recall/F-score: 0.857702918542181 / 0.8848996304803756 / 0.8710890456411139\n",
      "14041/14041 [==============================] - 6s 427us/step - loss: 0.0524\n",
      "Epoch 47/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0520\n",
      "Precision/Recall/F-score: 0.8637675487942083 / 0.8817537201637871 / 0.8726679680743286\n",
      "14041/14041 [==============================] - 5s 375us/step - loss: 0.0519\n",
      "Epoch 48/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0518\n",
      "Precision/Recall/F-score: 0.8575984537327856 / 0.8862478777589134 / 0.8716878269197711\n",
      "14041/14041 [==============================] - 5s 349us/step - loss: 0.0517\n",
      "Epoch 49/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0511\n",
      "Precision/Recall/F-score: 0.8698025205914459 / 0.8753620293618296 / 0.8725734196117472\n",
      "14041/14041 [==============================] - 4s 316us/step - loss: 0.0511\n",
      "Epoch 50/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0509\n",
      "Precision/Recall/F-score: 0.8583413693346191 / 0.8889443723159892 / 0.8733748712162097\n",
      "14041/14041 [==============================] - 5s 361us/step - loss: 0.0507\n",
      "Epoch 51/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0503\n",
      "Precision/Recall/F-score: 0.8678059320369279 / 0.8824528113452512 / 0.8750680861599406\n",
      "14041/14041 [==============================] - 5s 369us/step - loss: 0.0501\n",
      "Epoch 52/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0497\n",
      "Precision/Recall/F-score: 0.8689301416707377 / 0.8881953460501348 / 0.8784571315685501\n",
      "14041/14041 [==============================] - 4s 311us/step - loss: 0.0498\n",
      "Epoch 53/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0495\n",
      "Precision/Recall/F-score: 0.8693737769080235 / 0.8873464496154998 / 0.8782681757524836\n",
      "14041/14041 [==============================] - 5s 374us/step - loss: 0.0495\n",
      "Epoch 54/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0489\n",
      "Precision/Recall/F-score: 0.8672889018521219 / 0.8908918406072106 / 0.8789319407838018\n",
      "14041/14041 [==============================] - 4s 305us/step - loss: 0.0488\n",
      "Epoch 55/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0484\n",
      "Precision/Recall/F-score: 0.8529844024083819 / 0.8984320383501448 / 0.8751185583306986\n",
      "14041/14041 [==============================] - 4s 315us/step - loss: 0.0485\n",
      "Epoch 56/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0487\n",
      "Precision/Recall/F-score: 0.8682271978288262 / 0.8945870368520923 / 0.8812100344318741\n",
      "14041/14041 [==============================] - 4s 311us/step - loss: 0.0486\n",
      "Epoch 57/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0476\n",
      "Precision/Recall/F-score: 0.8919591608675775 / 0.8768600818935384 / 0.8843451766424093\n",
      "14041/14041 [==============================] - 5s 329us/step - loss: 0.0476\n",
      "Epoch 58/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0472\n",
      "Precision/Recall/F-score: 0.8685667119829094 / 0.8932887246579446 / 0.880754271084634\n",
      "14041/14041 [==============================] - 5s 388us/step - loss: 0.0474\n",
      "Epoch 59/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0469\n",
      "Precision/Recall/F-score: 0.9070902107339355 / 0.8726655348047538 / 0.8895449455359871\n",
      "14041/14041 [==============================] - 5s 324us/step - loss: 0.0471\n",
      "Epoch 60/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0470\n",
      "Precision/Recall/F-score: 0.9064578641386896 / 0.8642265055427943 / 0.8848385694930853\n",
      "14041/14041 [==============================] - 5s 365us/step - loss: 0.0471\n",
      "Epoch 61/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0465\n",
      "Precision/Recall/F-score: 0.878441275500099 / 0.8858983321681814 / 0.8821540450499727\n",
      "14041/14041 [==============================] - 5s 370us/step - loss: 0.0463\n",
      "Epoch 62/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0460\n",
      "Precision/Recall/F-score: 0.8698027460839296 / 0.8983821032657545 / 0.8838614591009579\n",
      "14041/14041 [==============================] - 5s 335us/step - loss: 0.0462\n",
      "Epoch 63/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0459\n",
      "Precision/Recall/F-score: 0.8735855470836773 / 0.8982322980125836 / 0.8857374990767413\n",
      "14041/14041 [==============================] - 4s 311us/step - loss: 0.0458\n",
      "Epoch 64/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0457\n",
      "Precision/Recall/F-score: 0.844181826641232 / 0.9060221711774693 / 0.8740094896312531\n",
      "14041/14041 [==============================] - 5s 380us/step - loss: 0.0455\n",
      "Epoch 65/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0455\n",
      "Precision/Recall/F-score: 0.8750365247881562 / 0.8972335963247778 / 0.8859960552268243\n",
      "14041/14041 [==============================] - 4s 311us/step - loss: 0.0455\n",
      "Epoch 66/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0454\n",
      "Precision/Recall/F-score: 0.8611637604245641 / 0.9075202237091781 / 0.8837345003646973\n",
      "14041/14041 [==============================] - 5s 350us/step - loss: 0.0453\n",
      "Epoch 67/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0449\n",
      "Precision/Recall/F-score: 0.9146175578110286 / 0.8729651453110956 / 0.8933060807358202\n",
      "14041/14041 [==============================] - 5s 360us/step - loss: 0.0449\n",
      "Epoch 68/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0445\n",
      "Precision/Recall/F-score: 0.8735632183908046 / 0.9032258064516129 / 0.8881469115191987\n",
      "14041/14041 [==============================] - 5s 324us/step - loss: 0.0444\n",
      "Epoch 69/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0443\n",
      "Precision/Recall/F-score: 0.9152206763587099 / 0.8770598222310996 / 0.8957339929112376\n",
      "14041/14041 [==============================] - 5s 329us/step - loss: 0.0441\n",
      "Epoch 70/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0439\n",
      "Precision/Recall/F-score: 0.8783269961977186 / 0.8997303505442924 / 0.8888998519980265\n",
      "14041/14041 [==============================] - 5s 340us/step - loss: 0.0441\n",
      "Epoch 71/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0437\n",
      "Precision/Recall/F-score: 0.882970889990673 / 0.8981823629281933 / 0.8905116716587864\n",
      "14041/14041 [==============================] - 4s 313us/step - loss: 0.0438\n",
      "Epoch 72/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0434\n",
      "Precision/Recall/F-score: 0.8859090235590458 / 0.8956856087086787 / 0.8907704913962208\n",
      "14041/14041 [==============================] - 4s 311us/step - loss: 0.0434\n",
      "Epoch 73/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0430\n",
      "Precision/Recall/F-score: 0.9142042509072058 / 0.8806052132228104 / 0.8970902431580019\n",
      "14041/14041 [==============================] - 5s 323us/step - loss: 0.0431\n",
      "Epoch 74/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0424\n",
      "Precision/Recall/F-score: 0.9134655101197852 / 0.883451513033057 / 0.8982078489110017\n",
      "14041/14041 [==============================] - 4s 308us/step - loss: 0.0427\n",
      "Epoch 75/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0426\n",
      "Precision/Recall/F-score: 0.8857508995021933 / 0.8973834015779487 / 0.8915292074910083\n",
      "14041/14041 [==============================] - 4s 319us/step - loss: 0.0429\n",
      "Epoch 76/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0423\n",
      "Precision/Recall/F-score: 0.8787425149700598 / 0.9086687306501547 / 0.893455098934551\n",
      "14041/14041 [==============================] - 6s 423us/step - loss: 0.0425\n",
      "Epoch 77/100\n",
      "13300/14041 [===========================>..] - ETA: 0s - loss: 0.0424\n",
      "Precision/Recall/F-score: 0.882682112703711 / 0.8979326875062419 / 0.8902420911926333\n",
      "14041/14041 [==============================] - 4s 316us/step - loss: 0.0424\n",
      "Epoch 78/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0423\n",
      "Precision/Recall/F-score: 0.882464685825621 / 0.9046739238989314 / 0.8934313048624125\n",
      "14041/14041 [==============================] - 5s 337us/step - loss: 0.0423\n",
      "Epoch 79/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0419\n",
      "Precision/Recall/F-score: 0.8813962520633071 / 0.9065714571057625 / 0.8938066167782591\n",
      "14041/14041 [==============================] - 5s 328us/step - loss: 0.0419\n",
      "Epoch 80/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0415\n",
      "Precision/Recall/F-score: 0.8775097501083345 / 0.910066913013083 / 0.8934918494913593\n",
      "14041/14041 [==============================] - 4s 310us/step - loss: 0.0418\n",
      "Epoch 81/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0417\n",
      "Precision/Recall/F-score: 0.8733234690468236 / 0.9136622390891841 / 0.8930375576542939\n",
      "14041/14041 [==============================] - 4s 313us/step - loss: 0.0417\n",
      "Epoch 82/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0415\n",
      "Precision/Recall/F-score: 0.8781158092666699 / 0.9094676920003995 / 0.8935168150710133\n",
      "14041/14041 [==============================] - 4s 316us/step - loss: 0.0417\n",
      "Epoch 83/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0415\n",
      "Precision/Recall/F-score: 0.8822528914374574 / 0.9065714571057625 / 0.894246872229337\n",
      "14041/14041 [==============================] - 4s 312us/step - loss: 0.0415\n",
      "Epoch 84/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0414\n",
      "Precision/Recall/F-score: 0.8785425101214575 / 0.9102167182662538 / 0.8940991808505421\n",
      "14041/14041 [==============================] - 5s 324us/step - loss: 0.0413\n",
      "Epoch 85/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0410\n",
      "Precision/Recall/F-score: 0.8798586572438163 / 0.9076700289623489 / 0.8935479906599484\n",
      "14041/14041 [==============================] - 4s 308us/step - loss: 0.0410\n",
      "Epoch 86/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0412\n",
      "Precision/Recall/F-score: 0.9084209991360471 / 0.8925896334764806 / 0.9004357353348613\n",
      "14041/14041 [==============================] - 5s 333us/step - loss: 0.0412\n",
      "Epoch 87/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0410\n",
      "Precision/Recall/F-score: 0.9065847591003706 / 0.8916908019574553 / 0.8990761020063943\n",
      "14041/14041 [==============================] - 4s 303us/step - loss: 0.0409\n",
      "Epoch 88/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0409\n",
      "Precision/Recall/F-score: 0.8845854796384488 / 0.9089683411564966 / 0.8966111713131711\n",
      "14041/14041 [==============================] - 4s 308us/step - loss: 0.0410\n",
      "Epoch 89/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0405\n",
      "Precision/Recall/F-score: 0.9119815668202765 / 0.8893937880755018 / 0.9005460612802104\n",
      "14041/14041 [==============================] - 4s 301us/step - loss: 0.0405\n",
      "Epoch 90/100\n",
      "13200/14041 [===========================>..] - ETA: 0s - loss: 0.0402\n",
      "Precision/Recall/F-score: 0.9261170212765958 / 0.8694197543193848 / 0.8968732292793489\n",
      "14041/14041 [==============================] - 4s 312us/step - loss: 0.0404\n",
      "Epoch 91/100\n",
      "13600/14041 [============================>.] - ETA: 0s - loss: 0.0404\n",
      "Precision/Recall/F-score: 0.8894630641271716 / 0.9024767801857585 / 0.8959226669971496\n",
      "14041/14041 [==============================] - 5s 348us/step - loss: 0.0403\n",
      "Epoch 92/100\n",
      "13400/14041 [===========================>..] - ETA: 0s - loss: 0.0403\n",
      "Precision/Recall/F-score: 0.9033896600281633 / 0.8969839209028263 / 0.9001753946379354\n",
      "14041/14041 [==============================] - 4s 275us/step - loss: 0.0403\n",
      "Epoch 93/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0402\n",
      "Precision/Recall/F-score: 0.8888943272478097 / 0.9068710676121042 / 0.8977927181946265\n",
      "14041/14041 [==============================] - 4s 295us/step - loss: 0.0402\n",
      "Epoch 94/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0402\n",
      "Precision/Recall/F-score: 0.8813878299474014 / 0.9120643163886947 / 0.8964637169010283\n",
      "14041/14041 [==============================] - 5s 338us/step - loss: 0.0401\n",
      "Epoch 95/100\n",
      "13700/14041 [============================>.] - ETA: 0s - loss: 0.0395\n",
      "Precision/Recall/F-score: 0.9161170651277823 / 0.8878458004594028 / 0.9017599026221028\n",
      "14041/14041 [==============================] - 4s 292us/step - loss: 0.0395\n",
      "Epoch 96/100\n",
      "13900/14041 [============================>.] - ETA: 0s - loss: 0.0396\n",
      "Precision/Recall/F-score: 0.8811842993538432 / 0.9125137321482073 / 0.8965754096752037\n",
      "14041/14041 [==============================] - 4s 302us/step - loss: 0.0397\n",
      "Epoch 97/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0395\n",
      "Precision/Recall/F-score: 0.887058250060931 / 0.9087186657345451 / 0.897757825411312\n",
      "14041/14041 [==============================] - 5s 367us/step - loss: 0.0395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "13800/14041 [============================>.] - ETA: 0s - loss: 0.0395\n",
      "Precision/Recall/F-score: 0.8865501074428599 / 0.9064715869369819 / 0.896400177768999\n",
      "14041/14041 [==============================] - 5s 337us/step - loss: 0.0396\n",
      "Epoch 99/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.0396\n",
      "Precision/Recall/F-score: 0.8894181193223668 / 0.9044741835613702 / 0.8968829689782378\n",
      "14041/14041 [==============================] - 4s 317us/step - loss: 0.0396\n",
      "Epoch 100/100\n",
      "13500/14041 [===========================>..] - ETA: 0s - loss: 0.0395\n",
      "Precision/Recall/F-score: 0.9203599317370843 / 0.8886946968940378 / 0.9042501841831162\n",
      "14041/14041 [==============================] - 4s 308us/step - loss: 0.0393\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "cnn = Conv1D(100,3, activation='relu', padding='same')(embeddings)\n",
    "outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(cnn)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=1,epochs=100, callbacks=[EvaluateEntities()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 113, 100)          160400    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 113, 4)            404       \n",
      "=================================================================\n",
      "Total params: 15,161,404\n",
      "Trainable params: 160,804\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 4.3386\n",
      "Precision/Recall/F-score: 0.3169291338582677 / 0.03215819434734845 / 0.058391513283162576\n",
      "14041/14041 [==============================] - 155s 11ms/step - loss: 4.3347\n",
      "Epoch 2/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 2.1341\n",
      "Precision/Recall/F-score: 0.6756756756756757 / 0.49435733546389693 / 0.5709671838052945\n",
      "14041/14041 [==============================] - 156s 11ms/step - loss: 2.1341\n",
      "Epoch 3/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.4414\n",
      "Precision/Recall/F-score: 0.7088869205786832 / 0.5994706881054629 / 0.6496036362652525\n",
      "14041/14041 [==============================] - 155s 11ms/step - loss: 1.4415\n",
      "Epoch 4/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.2324\n",
      "Precision/Recall/F-score: 0.7116390584132519 / 0.6521522021372216 / 0.680598259419459\n",
      "14041/14041 [==============================] - 157s 11ms/step - loss: 1.2321\n",
      "Epoch 5/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.1217\n",
      "Precision/Recall/F-score: 0.7215482070002147 / 0.6711774692899231 / 0.6954519584001656\n",
      "14041/14041 [==============================] - 156s 11ms/step - loss: 1.1214\n",
      "Epoch 6/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.0569\n",
      "Precision/Recall/F-score: 0.7475792988313856 / 0.6708279236991911 / 0.7071270660069481\n",
      "14041/14041 [==============================] - 138s 10ms/step - loss: 1.0576\n",
      "Epoch 7/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.0066\n",
      "Precision/Recall/F-score: 0.7261073475768629 / 0.6957954658943374 / 0.7106283149734801\n",
      "14041/14041 [==============================] - 148s 11ms/step - loss: 1.0067\n",
      "Epoch 8/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.9638\n",
      "Precision/Recall/F-score: 0.7604375372285699 / 0.7012383900928792 / 0.7296391551710701\n",
      "14041/14041 [==============================] - 135s 10ms/step - loss: 0.9653\n",
      "Epoch 9/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.9249\n",
      "Precision/Recall/F-score: 0.765039302711085 / 0.7144212523719166 / 0.7388643581997056\n",
      "14041/14041 [==============================] - 142s 10ms/step - loss: 0.9253\n",
      "Epoch 10/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.9010\n",
      "Precision/Recall/F-score: 0.7800011085859986 / 0.7026865075401978 / 0.7393280268999397\n",
      "14041/14041 [==============================] - 138s 10ms/step - loss: 0.9006\n",
      "Epoch 11/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.8698\n",
      "Precision/Recall/F-score: 0.790445353328789 / 0.6948466992909218 / 0.7395694924262558\n",
      "14041/14041 [==============================] - 136s 10ms/step - loss: 0.8688\n",
      "Epoch 12/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.8526\n",
      "Precision/Recall/F-score: 0.787841124020535 / 0.7280035953260761 / 0.7567413251667489\n",
      "14041/14041 [==============================] - 125s 9ms/step - loss: 0.8520\n",
      "Epoch 13/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.8220\n",
      "Precision/Recall/F-score: 0.8061050477489768 / 0.7081294317387397 / 0.7539475782869902\n",
      "14041/14041 [==============================] - 123s 9ms/step - loss: 0.8217\n",
      "Epoch 14/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.8001\n",
      "Precision/Recall/F-score: 0.8108500306971033 / 0.7254569060221712 / 0.7657802493213505\n",
      "14041/14041 [==============================] - 125s 9ms/step - loss: 0.8009\n",
      "Epoch 15/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.7838\n",
      "Precision/Recall/F-score: 0.8133503637474315 / 0.7313492459802257 / 0.7701732705808113\n",
      "14041/14041 [==============================] - 126s 9ms/step - loss: 0.7832\n",
      "Epoch 16/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.7681\n",
      "Precision/Recall/F-score: 0.8124575695858791 / 0.7171177469289923 / 0.7618163492652908\n",
      "14041/14041 [==============================] - 123s 9ms/step - loss: 0.7682\n",
      "Epoch 17/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.7512\n",
      "Precision/Recall/F-score: 0.8140654052280433 / 0.743333666233896 / 0.7770933389016496\n",
      "14041/14041 [==============================] - 121s 9ms/step - loss: 0.7510\n",
      "Epoch 18/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.7353\n",
      "Precision/Recall/F-score: 0.8240074171029669 / 0.7544691900529312 / 0.787706584641051\n",
      "14041/14041 [==============================] - 127s 9ms/step - loss: 0.7357\n",
      "Epoch 19/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.7176\n",
      "Precision/Recall/F-score: 0.8176663840610429 / 0.7705482872266054 / 0.7934084014602293\n",
      "14041/14041 [==============================] - 124s 9ms/step - loss: 0.7172\n",
      "Epoch 20/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.7036\n",
      "Precision/Recall/F-score: 0.8422659430122117 / 0.7439328872465795 / 0.7900514397836348\n",
      "14041/14041 [==============================] - 125s 9ms/step - loss: 0.7031\n",
      "Epoch 21/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.6948\n",
      "Precision/Recall/F-score: 0.8248395821180463 / 0.7766903026066114 / 0.8000411490882905\n",
      "14041/14041 [==============================] - 128s 9ms/step - loss: 0.6939\n",
      "Epoch 22/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.6761\n",
      "Precision/Recall/F-score: 0.8413849958779884 / 0.7644562069309897 / 0.801077941445802\n",
      "14041/14041 [==============================] - 129s 9ms/step - loss: 0.6755\n",
      "Epoch 23/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.6610\n",
      "Precision/Recall/F-score: 0.838523395758001 / 0.7758414061719764 / 0.8059655038257035\n",
      "14041/14041 [==============================] - 141s 10ms/step - loss: 0.6614\n",
      "Epoch 24/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.6504\n",
      "Precision/Recall/F-score: 0.8604586902459243 / 0.7774892639568561 / 0.8168725899110727\n",
      "14041/14041 [==============================] - 145s 10ms/step - loss: 0.6500\n",
      "Epoch 25/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.6315\n",
      "Precision/Recall/F-score: 0.8633697527314549 / 0.7497253570358534 / 0.8025443660466111\n",
      "14041/14041 [==============================] - 135s 10ms/step - loss: 0.6322\n",
      "Epoch 26/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.6178\n",
      "Precision/Recall/F-score: 0.8670028011204481 / 0.7727953660241685 / 0.8171929454007815\n",
      "14041/14041 [==============================] - 151s 11ms/step - loss: 0.6179\n",
      "Epoch 27/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.6056\n",
      "Precision/Recall/F-score: 0.8688054682762802 / 0.787026865075402 / 0.8258967170592396\n",
      "14041/14041 [==============================] - 133s 9ms/step - loss: 0.6050\n",
      "Epoch 28/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5912\n",
      "Precision/Recall/F-score: 0.875075770099741 / 0.7929691401178468 / 0.8320016765776859\n",
      "14041/14041 [==============================] - 150s 11ms/step - loss: 0.5910\n",
      "Epoch 29/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5819\n",
      "Precision/Recall/F-score: 0.8759535655058043 / 0.7912713472485768 / 0.831461853289957\n",
      "14041/14041 [==============================] - 139s 10ms/step - loss: 0.5812\n",
      "Epoch 30/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5714\n",
      "Precision/Recall/F-score: 0.8849773054356249 / 0.7983621292319985 / 0.8394413525149638\n",
      "14041/14041 [==============================] - 136s 10ms/step - loss: 0.5722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5584\n",
      "Precision/Recall/F-score: 0.8812849921182802 / 0.8095975232198143 / 0.8439216094526715\n",
      "14041/14041 [==============================] - 137s 10ms/step - loss: 0.5584\n",
      "Epoch 32/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5413\n",
      "Precision/Recall/F-score: 0.8880721683929168 / 0.7963647258563867 / 0.8397219882055602\n",
      "14041/14041 [==============================] - 140s 10ms/step - loss: 0.5416\n",
      "Epoch 33/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5332\n",
      "Precision/Recall/F-score: 0.9040313549832026 / 0.8062518725656647 / 0.8523465132238821\n",
      "14041/14041 [==============================] - 149s 11ms/step - loss: 0.5330\n",
      "Epoch 34/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5196\n",
      "Precision/Recall/F-score: 0.8840437158469945 / 0.807849795266154 / 0.8442310702917079\n",
      "14041/14041 [==============================] - 140s 10ms/step - loss: 0.5201\n",
      "Epoch 35/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5147\n",
      "Precision/Recall/F-score: 0.903692614770459 / 0.8138919404773794 / 0.8564447480426672\n",
      "14041/14041 [==============================] - 138s 10ms/step - loss: 0.5144\n",
      "Epoch 36/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5011\n",
      "Precision/Recall/F-score: 0.9086206896551724 / 0.8157894736842105 / 0.8597063621533442\n",
      "14041/14041 [==============================] - 133s 9ms/step - loss: 0.5015\n",
      "Epoch 37/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4884\n",
      "Precision/Recall/F-score: 0.9129078962022497 / 0.8186357734944572 / 0.8632055602358888\n",
      "14041/14041 [==============================] - 122s 9ms/step - loss: 0.4893\n",
      "Epoch 38/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4803\n",
      "Precision/Recall/F-score: 0.9172647860679909 / 0.8232298012583641 / 0.8677070449222347\n",
      "14041/14041 [==============================] - 134s 10ms/step - loss: 0.4801\n",
      "Epoch 39/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4765\n",
      "Precision/Recall/F-score: 0.9182689643708095 / 0.8275242185159293 / 0.8705381766606256\n",
      "14041/14041 [==============================] - 122s 9ms/step - loss: 0.4768\n",
      "Epoch 40/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4673\n",
      "Precision/Recall/F-score: 0.9251327928431646 / 0.8262259063217817 / 0.8728864973226768\n",
      "14041/14041 [==============================] - 120s 9ms/step - loss: 0.4687\n",
      "Epoch 41/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4582\n",
      "Precision/Recall/F-score: 0.9240302322996554 / 0.8302706481573954 / 0.8746449237243556\n",
      "14041/14041 [==============================] - 126s 9ms/step - loss: 0.4588\n",
      "Epoch 42/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4511\n",
      "Precision/Recall/F-score: 0.931995976754582 / 0.8328672725456906 / 0.8796476979062287\n",
      "14041/14041 [==============================] - 121s 9ms/step - loss: 0.4507\n",
      "Epoch 43/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4531\n",
      "Precision/Recall/F-score: 0.926277250790481 / 0.8338160391491062 / 0.8776180590229417\n",
      "14041/14041 [==============================] - 124s 9ms/step - loss: 0.4538\n",
      "Epoch 44/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4440\n",
      "Precision/Recall/F-score: 0.9282560706401766 / 0.8399081194447219 / 0.8818749016934934\n",
      "14041/14041 [==============================] - 126s 9ms/step - loss: 0.4434\n",
      "Epoch 45/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4276\n",
      "Precision/Recall/F-score: 0.9316149102827621 / 0.8374113652252072 / 0.8820048912614721\n",
      "14041/14041 [==============================] - 123s 9ms/step - loss: 0.4278\n",
      "Epoch 46/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4253\n",
      "Precision/Recall/F-score: 0.9351990671331001 / 0.8410066913013083 / 0.8856053634810043\n",
      "14041/14041 [==============================] - 130s 9ms/step - loss: 0.4251\n",
      "Epoch 47/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4169\n",
      "Precision/Recall/F-score: 0.9385518590998043 / 0.8382103265754519 / 0.8855477302102293\n",
      "14041/14041 [==============================] - 145s 10ms/step - loss: 0.4168\n",
      "Epoch 48/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4123\n",
      "Precision/Recall/F-score: 0.9388461109251199 / 0.8402077299510636 / 0.8867924528301887\n",
      "14041/14041 [==============================] - 162s 12ms/step - loss: 0.4124\n",
      "Epoch 49/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4129\n",
      "Precision/Recall/F-score: 0.9355645744148552 / 0.8403076001198442 / 0.8853813169178966\n",
      "14041/14041 [==============================] - 151s 11ms/step - loss: 0.4136\n",
      "Epoch 50/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4126\n",
      "Precision/Recall/F-score: 0.9372712146422629 / 0.843852991111555 / 0.888112255623292\n",
      "14041/14041 [==============================] - 147s 10ms/step - loss: 0.4124\n",
      "Epoch 51/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4018\n",
      "Precision/Recall/F-score: 0.9389710361632608 / 0.8466493558374114 / 0.8904235485649765\n",
      "14041/14041 [==============================] - 146s 10ms/step - loss: 0.4028\n",
      "Epoch 52/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4024\n",
      "Precision/Recall/F-score: 0.9391241844520624 / 0.8481474083691202 / 0.8913203190596138\n",
      "14041/14041 [==============================] - 145s 10ms/step - loss: 0.4020\n",
      "Epoch 53/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3969\n",
      "Precision/Recall/F-score: 0.9405611821351486 / 0.848646759213023 / 0.8922430765192282\n",
      "14041/14041 [==============================] - 147s 11ms/step - loss: 0.3968\n",
      "Epoch 54/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3962\n",
      "Precision/Recall/F-score: 0.942880978865406 / 0.8465494856686308 / 0.8921222964795031\n",
      "14041/14041 [==============================] - 144s 10ms/step - loss: 0.3965\n",
      "Epoch 55/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3996\n",
      "Precision/Recall/F-score: 0.9394496434937611 / 0.842155198242285 / 0.8881457686028754\n",
      "14041/14041 [==============================] - 152s 11ms/step - loss: 0.3994\n",
      "Epoch 56/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3996\n",
      "Precision/Recall/F-score: 0.9413315385046834 / 0.838060521322281 / 0.8866992471271959\n",
      "14041/14041 [==============================] - 148s 11ms/step - loss: 0.3989\n",
      "Epoch 57/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3925\n",
      "Precision/Recall/F-score: 0.9314648565798117 / 0.8496953959852193 / 0.8887031911004335\n",
      "14041/14041 [==============================] - 148s 11ms/step - loss: 0.3933\n",
      "Epoch 58/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3850\n",
      "Precision/Recall/F-score: 0.9476033703476369 / 0.8479976031159493 / 0.8950378159011253\n",
      "14041/14041 [==============================] - 146s 10ms/step - loss: 0.3846\n",
      "Epoch 59/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3794\n",
      "Precision/Recall/F-score: 0.9521005104043974 / 0.8476480575252172 / 0.8968432175406156\n",
      "14041/14041 [==============================] - 144s 10ms/step - loss: 0.3797\n",
      "Epoch 60/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3802\n",
      "Precision/Recall/F-score: 0.950231804725465 / 0.849495655647658 / 0.897044477840175\n",
      "14041/14041 [==============================] - 143s 10ms/step - loss: 0.3796\n",
      "Epoch 61/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3766\n",
      "Precision/Recall/F-score: 0.9525279165030021 / 0.8476480575252172 / 0.8970327899172986\n",
      "14041/14041 [==============================] - 150s 11ms/step - loss: 0.3762\n",
      "Epoch 62/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3766\n",
      "Precision/Recall/F-score: 0.9442284190645484 / 0.8487965644661939 / 0.8939728621016093\n",
      "14041/14041 [==============================] - 146s 10ms/step - loss: 0.3763\n",
      "Epoch 63/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3802\n",
      "Precision/Recall/F-score: 0.9441904549509367 / 0.8456506541496055 / 0.8922079974711554\n",
      "14041/14041 [==============================] - 145s 10ms/step - loss: 0.3804\n",
      "Epoch 64/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3825\n",
      "Precision/Recall/F-score: 0.9505771601479324 / 0.847098771596924 / 0.8958597380650614\n",
      "14041/14041 [==============================] - 129s 9ms/step - loss: 0.3827\n",
      "Epoch 65/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4063\n",
      "Precision/Recall/F-score: 0.9330661102569734 / 0.8485468890442425 / 0.888801715570898\n",
      "14041/14041 [==============================] - 133s 9ms/step - loss: 0.4069\n",
      "Epoch 66/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3761\n",
      "Precision/Recall/F-score: 0.9525249132236032 / 0.8495955258164386 / 0.8981207770270271\n",
      "14041/14041 [==============================] - 131s 9ms/step - loss: 0.3760\n",
      "Epoch 67/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3672\n",
      "Precision/Recall/F-score: 0.9480757483200978 / 0.8524917607110756 / 0.8977466936607684\n",
      "14041/14041 [==============================] - 130s 9ms/step - loss: 0.3670\n",
      "Epoch 68/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3652\n",
      "Precision/Recall/F-score: 0.9520559440559441 / 0.8497952661539998 / 0.8980237988443577\n",
      "14041/14041 [==============================] - 118s 8ms/step - loss: 0.3652\n",
      "Epoch 69/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3679\n",
      "Precision/Recall/F-score: 0.9524261544474845 / 0.8517427344452212 / 0.8992750757875312\n",
      "14041/14041 [==============================] - 121s 9ms/step - loss: 0.3677\n",
      "Epoch 70/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3605\n",
      "Precision/Recall/F-score: 0.952743135171411 / 0.8506940976730251 / 0.8988313504102144\n",
      "14041/14041 [==============================] - 122s 9ms/step - loss: 0.3606\n",
      "Epoch 71/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3590\n",
      "Precision/Recall/F-score: 0.9534520287994642 / 0.8530410466393689 / 0.9004559470785125\n",
      "14041/14041 [==============================] - 117s 8ms/step - loss: 0.3586\n",
      "Epoch 72/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3610\n",
      "Precision/Recall/F-score: 0.9527554663096832 / 0.8529411764705882 / 0.9000895821257311\n",
      "14041/14041 [==============================] - 117s 8ms/step - loss: 0.3619\n",
      "Epoch 73/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3580\n",
      "Precision/Recall/F-score: 0.9527400703871292 / 0.8516428642764406 / 0.899359295488702\n",
      "14041/14041 [==============================] - 140s 10ms/step - loss: 0.3578\n",
      "Epoch 74/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3623\n",
      "Precision/Recall/F-score: 0.9252210480914385 / 0.8569359832218116 / 0.8897703116088558\n",
      "14041/14041 [==============================] - 143s 10ms/step - loss: 0.3622\n",
      "Epoch 75/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3600\n",
      "Precision/Recall/F-score: 0.9490022172949002 / 0.8548886447618097 / 0.8994903588504177\n",
      "14041/14041 [==============================] - 143s 10ms/step - loss: 0.3596\n",
      "Epoch 76/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3586\n",
      "Precision/Recall/F-score: 0.95321996097017 / 0.8536902027364426 / 0.9007138905719027\n",
      "14041/14041 [==============================] - 142s 10ms/step - loss: 0.3586\n",
      "Epoch 77/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3570\n",
      "Precision/Recall/F-score: 0.9519616584930896 / 0.8529911115549785 / 0.8997629707663944\n",
      "14041/14041 [==============================] - 150s 11ms/step - loss: 0.3566\n",
      "Epoch 78/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3547\n",
      "Precision/Recall/F-score: 0.9523862560561341 / 0.8539898132427843 / 0.9005081220546034\n",
      "14041/14041 [==============================] - 148s 11ms/step - loss: 0.3551\n",
      "Epoch 79/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3656\n",
      "Precision/Recall/F-score: 0.9400875685861553 / 0.8469989014281434 / 0.8911187580446032\n",
      "14041/14041 [==============================] - 151s 11ms/step - loss: 0.3652\n",
      "Epoch 80/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4138\n",
      "Precision/Recall/F-score: 0.9370551601423488 / 0.8415060421452112 / 0.8867140226256248\n",
      "14041/14041 [==============================] - 146s 10ms/step - loss: 0.4143\n",
      "Epoch 81/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3813\n",
      "Precision/Recall/F-score: 0.9533408197641774 / 0.8478477978627784 / 0.8975050216724813\n",
      "14041/14041 [==============================] - 144s 10ms/step - loss: 0.3812\n",
      "Epoch 82/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3605\n",
      "Precision/Recall/F-score: 0.9582047685834502 / 0.852891241386198 / 0.9024860637763863\n",
      "14041/14041 [==============================] - 145s 10ms/step - loss: 0.3606\n",
      "Epoch 83/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3513\n",
      "Precision/Recall/F-score: 0.9585390484739676 / 0.8531409168081494 / 0.902774108322325\n",
      "14041/14041 [==============================] - 148s 11ms/step - loss: 0.3512\n",
      "Epoch 84/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3524\n",
      "Precision/Recall/F-score: 0.9567576639069143 / 0.8540397483271747 / 0.9024853569732467\n",
      "14041/14041 [==============================] - 150s 11ms/step - loss: 0.3518\n",
      "Epoch 85/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3483\n",
      "Precision/Recall/F-score: 0.9538315883270216 / 0.8552381903525417 / 0.9018482439050076\n",
      "14041/14041 [==============================] - 151s 11ms/step - loss: 0.3479\n",
      "Epoch 86/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3438\n",
      "Precision/Recall/F-score: 0.9427460004383081 / 0.8592329971037651 / 0.89905428705784\n",
      "14041/14041 [==============================] - 146s 10ms/step - loss: 0.3437\n",
      "Epoch 87/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3479\n",
      "Precision/Recall/F-score: 0.9547581903276131 / 0.8556876061120543 / 0.902512245220414\n",
      "14041/14041 [==============================] - 147s 10ms/step - loss: 0.3478\n",
      "Epoch 88/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3474\n",
      "Precision/Recall/F-score: 0.9585948005378754 / 0.8543393588335164 / 0.9034693985319744\n",
      "14041/14041 [==============================] - 135s 10ms/step - loss: 0.3473\n",
      "Epoch 89/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3470\n",
      "Precision/Recall/F-score: 0.9590223667245922 / 0.8542894237491261 / 0.9036313218011356\n",
      "14041/14041 [==============================] - 135s 10ms/step - loss: 0.3469\n",
      "Epoch 90/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3463\n",
      "Precision/Recall/F-score: 0.9565314810665472 / 0.8526915010486368 / 0.9016315539363219\n",
      "14041/14041 [==============================] - 138s 10ms/step - loss: 0.3461\n",
      "Epoch 91/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3488\n",
      "Precision/Recall/F-score: 0.9587461516932549 / 0.855288125436932 / 0.9040669288221478\n",
      "14041/14041 [==============================] - 139s 10ms/step - loss: 0.3486\n",
      "Epoch 92/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3451\n",
      "Precision/Recall/F-score: 0.958200436461306 / 0.8550883850993708 / 0.9037126949362747\n",
      "14041/14041 [==============================] - 138s 10ms/step - loss: 0.3452\n",
      "Epoch 93/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3447\n",
      "Precision/Recall/F-score: 0.9580623458174479 / 0.8532907220613203 / 0.9026464529079289\n",
      "14041/14041 [==============================] - 134s 10ms/step - loss: 0.3453\n",
      "Epoch 94/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3457\n",
      "Precision/Recall/F-score: 0.954279342068581 / 0.8546389693398582 / 0.9017149179420983\n",
      "14041/14041 [==============================] - 136s 10ms/step - loss: 0.3460\n",
      "Epoch 95/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3434\n",
      "Precision/Recall/F-score: 0.9594837261503928 / 0.8537900729052232 / 0.903556518522433\n",
      "14041/14041 [==============================] - 138s 10ms/step - loss: 0.3435\n",
      "Epoch 96/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3438\n",
      "Precision/Recall/F-score: 0.9576456106977005 / 0.8546889044242485 / 0.9032428296261116\n",
      "14041/14041 [==============================] - 147s 10ms/step - loss: 0.3433\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3415\n",
      "Precision/Recall/F-score: 0.9560114023810855 / 0.854089683411565 / 0.9021810797267718\n",
      "14041/14041 [==============================] - 148s 11ms/step - loss: 0.3419\n",
      "Epoch 98/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.4030\n",
      "Precision/Recall/F-score: 0.9419622810956444 / 0.8380105862378907 / 0.8869510068178215\n",
      "14041/14041 [==============================] - 148s 11ms/step - loss: 0.4025\n",
      "Epoch 99/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3613\n",
      "Precision/Recall/F-score: 0.9579342407438526 / 0.8539898132427843 / 0.9029805433089575\n",
      "14041/14041 [==============================] - 147s 10ms/step - loss: 0.3616\n",
      "Epoch 100/100\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.3438\n",
      "Precision/Recall/F-score: 0.9592362394310991 / 0.8554379306901029 / 0.9043684835686946\n",
      "14041/14041 [==============================] - 146s 10ms/step - loss: 0.3439\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "rnn = LSTM(100, activation='tanh', return_sequences=True)(embeddings)\n",
    "outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(rnn)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=1,epochs=100, callbacks=[EvaluateEntities()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 113, 200)          320800    \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 113, 4)            804       \n",
      "=================================================================\n",
      "Total params: 15,322,204\n",
      "Trainable params: 321,604\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 3.9394\n",
      "Precision/Recall/F-score: 0.553005284015852 / 0.16723259762308998 / 0.2568054597040104\n",
      "14041/14041 [==============================] - 307s 22ms/step - loss: 3.9332\n",
      "Epoch 2/10\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.6082\n",
      "Precision/Recall/F-score: 0.7410719371190978 / 0.6497053830020972 / 0.6923875156320677\n",
      "14041/14041 [==============================] - 302s 22ms/step - loss: 1.6067\n",
      "Epoch 3/10\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 1.0547\n",
      "Precision/Recall/F-score: 0.7721611339830111 / 0.7398881454109657 / 0.755680224403927\n",
      "14041/14041 [==============================] - 313s 22ms/step - loss: 1.0544\n",
      "Epoch 4/10\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.8725\n",
      "Precision/Recall/F-score: 0.8005074039556798 / 0.7720463397583142 / 0.7860193187595322\n",
      "14041/14041 [==============================] - 309s 22ms/step - loss: 0.8726\n",
      "Epoch 5/10\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.7830\n",
      "Precision/Recall/F-score: 0.8390600753713146 / 0.7560171776690303 / 0.7953769372209087\n",
      "14041/14041 [==============================] - 303s 22ms/step - loss: 0.7830\n",
      "Epoch 6/10\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.7187\n",
      "Precision/Recall/F-score: 0.8150225884922802 / 0.8197842804354339 / 0.817396499788394\n",
      "14041/14041 [==============================] - 310s 22ms/step - loss: 0.7187\n",
      "Epoch 7/10\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.6454\n",
      "Precision/Recall/F-score: 0.8186697335633506 / 0.8530909817237591 / 0.8355259940333545\n",
      "14041/14041 [==============================] - 294s 21ms/step - loss: 0.6451\n",
      "Epoch 8/10\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.6082\n",
      "Precision/Recall/F-score: 0.846301965043942 / 0.8559372815340058 / 0.8510923535253228\n",
      "14041/14041 [==============================] - 293s 21ms/step - loss: 0.6080\n",
      "Epoch 9/10\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5637\n",
      "Precision/Recall/F-score: 0.8426138304621541 / 0.8621791670827924 / 0.8522842263741146\n",
      "14041/14041 [==============================] - 287s 20ms/step - loss: 0.5635\n",
      "Epoch 10/10\n",
      "14000/14041 [============================>.] - ETA: 0s - loss: 0.5339\n",
      "Precision/Recall/F-score: 0.8661366459627329 / 0.8704184560071907 / 0.8682722721725485\n",
      "14041/14041 [==============================] - 288s 21ms/step - loss: 0.5340\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "rnn = Bidirectional(LSTM(100, activation='tanh', return_sequences=True))(embeddings)\n",
    "outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(rnn)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=1,epochs=10, callbacks=[EvaluateEntities()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
