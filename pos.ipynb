{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data, vocabulary and pretrained embeddings\n",
    "\n",
    "Use the [pos reader](read_pos.ipynb) to convert the data to json format.\n",
    "\n",
    "These parts are similar to the previous examples. Things to note though:\n",
    "* Our data has already been tokenized and divided into sentences\n",
    "* We _cannot_ skip tokens\n",
    "* We are using a specific OOV (out-of-vocabulary) embedding for all words which are not present in our vocab\n",
    "* We now have one label for each token, not for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.'], 'tags': ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']}\n",
      "[['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.'], ['[', 'This', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']]\n",
      "[['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], ['PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'AUX', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADP', 'NOUN', 'PART', 'VERB', 'PUNCT', 'PUNCT']]\n",
      "Words from embedding model: 50000\n",
      "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
      "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
      " -0.0063]\n",
      "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n",
      "Words in vocabulary: 50002\n",
      "Found pretrained vectors for 50000 words.\n"
     ]
    }
   ],
   "source": [
    "# Load our training data\n",
    "import json\n",
    "import random\n",
    "import numpy\n",
    "with open(\"data/pos_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "print(data[0])\n",
    "\n",
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"tags\"] for one_example in data] # This is now a list of lists just like the texts variable\n",
    "print(texts[:2])\n",
    "print(labels[:2])\n",
    "\n",
    "# Lets do the same thing for the validation data\n",
    "# We use a separate validation set, since generally using sentences from the same documents as train/validation results in overly optimistic scores\n",
    "with open(\"data/pos_devel.json\") as f:\n",
    "    validation_data=json.load(f)\n",
    "validation_texts=[one_example[\"text\"] for one_example in validation_data]\n",
    "validation_labels=[one_example[\"tags\"] for one_example in validation_data]\n",
    "\n",
    "# Use gensim to read the embedding model\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model=KeyedVectors.load_word2vec_format(\"data/wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words=[k for k,v in sorted(vector_model.vocab.items(), key=lambda x:x[1].index)]\n",
    "print(\"Words from embedding model:\",len(words))\n",
    "print(\"First 50 words:\",words[:50])\n",
    "\n",
    "# Normalize the vectors\n",
    "\n",
    "print(\"Before normalization:\",vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\",vector_model.get_vector(\"in\")[:10])\n",
    "\n",
    "# Build vocabulary mappings\n",
    "\n",
    "vocabulary={\"<SPECIAL>\": 0, \"<OOV>\": 1} # zero has a special meaning in sequence models, prevent using it for a normal word\n",
    "for word in words:\n",
    "    vocabulary.setdefault(word, len(vocabulary))\n",
    "\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "inversed_vocabulary={value:key for key, value in vocabulary.items()} # inverse the dictionary\n",
    "\n",
    "# Label mappings\n",
    "label_set = set([label for sentence_labels in labels for label in sentence_labels])\n",
    "label_map = {label: index for index, label in enumerate(label_set)}\n",
    "                \n",
    "# Embedding matrix\n",
    "\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings=numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
    "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing data\n",
    "If we want to consider the task as sequence labeling, we should feed the input data as word sequences and outputs as label sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def vectorizer(vocab, texts, label_map, labels=None):\n",
    "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
    "    vectorized_labels = [] # same thing for the labels\n",
    "    sentence_lengths = [] # Number of tokens in each sentence\n",
    "    \n",
    "    for i, one_example in enumerate(texts):\n",
    "        vectorized_example = []\n",
    "        vectorized_example_labels = []\n",
    "        for word in one_example:\n",
    "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
    "        \n",
    "        if labels:\n",
    "            for label in labels[i]:\n",
    "                vectorized_example_labels.append(label_map[label])\n",
    "\n",
    "        vectorized_data.append(vectorized_example)\n",
    "        vectorized_labels.append(vectorized_example_labels)\n",
    "        \n",
    "        sentence_lengths.append(len(one_example))\n",
    "        \n",
    "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy matrix\n",
    "    vectorized_labels = numpy.array(vectorized_labels)\n",
    "    \n",
    "    return vectorized_data, vectorized_labels, sentence_lengths\n",
    "\n",
    "vectorized_data, vectorized_labels, lengths=vectorizer(vocabulary, texts, label_map, labels)\n",
    "validation_vectorized_data, validation_vectorized_labels, validation_lengths=vectorizer(vocabulary, validation_texts, label_map, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "We add padding to the label sequences as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (12543,)\n",
      "New shape: (12543, 159)\n",
      "First example: [ 3424    37     1    11   285  1084   974 34462 10554  4733    37 43264\n",
      "     2     3 16500    29     3  8683     8     3   754     6     1     2\n",
      "   504     3  4761  1757     4     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "Padded labels shape: (12543, 159, 1)\n",
      "{'ADP': 0, 'INTJ': 1, 'DET': 2, 'PART': 3, 'NUM': 13, 'ADJ': 5, 'PROPN': 8, 'SCONJ': 9, 'PUNCT': 7, 'NOUN': 10, 'VERB': 11, 'PRON': 12, 'SYM': 15, 'ADV': 4, 'X': 14, 'AUX': 16, 'CCONJ': 6}\n",
      "First example labels: [[ 8]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [10]\n",
      " [11]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]]\n",
      "First weight vector: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='post', maxlen=max(lengths))\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\", vectorized_data_padded[0])\n",
    "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
    "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='post', maxlen=max(lengths)), -1)\n",
    "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
    "print(label_map)\n",
    "print(\"First example labels:\", vectorized_labels_padded[0])\n",
    "\n",
    "weights = numpy.copy(vectorized_data_padded)\n",
    "weights[weights > 0] = 1\n",
    "print(\"First weight vector:\", weights[0])\n",
    "\n",
    "# Same stuff for the validation data\n",
    "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='post', maxlen=max(lengths))\n",
    "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='post',maxlen=max(lengths)), -1)\n",
    "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
    "validation_weights[validation_weights > 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating POS tags\n",
    "Keras does not use sample weighting in metrics (only for losses) (correct me if I'm wrong), so we have to create our own evaluation if we want to ignore padding in models which do not support masking (e.g. convolution).\n",
    "Thus, to have evaluation that is identical for all models, we have to create our own script, which will ignore padded parts of the sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(predictions, gold, lengths):\n",
    "    pred_tags = numpy.concatenate([labels[:lengths[i]] for i, labels in enumerate(predictions)]).ravel()\n",
    "    \n",
    "    gold_tags = numpy.concatenate([labels[:lengths[i], 0] for i, labels in enumerate(gold)]).ravel()\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(gold_tags, pred_tags))\n",
    "\n",
    "class EvaluateTags(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
    "        accuracy(pred, validation_vectorized_labels_padded, validation_lengths) # FIXME: Using global variables here, not good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent classification\n",
    "Time-distributed means that the same dense layer is applied to each time step. This means that we are now simply using a normal feedforward network to classify each word/token separately.\n",
    "\n",
    "Why didn't we one-hot encode our labels? :S\n",
    "\n",
    "It's because the sparse loss is doing it for us implicitly! Neat, right!\n",
    "\n",
    "__Also, word embeddings are frozen!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, Conv1D, TimeDistributed, LSTM, Bidirectional\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "\n",
    "vector_size= pretrained.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 159)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 159, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 159, 100)          30100     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 159, 17)           1717      \n",
      "=================================================================\n",
      "Total params: 15,032,417\n",
      "Trainable params: 31,817\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "Accuracy: 0.2624254473161034\n",
      " - 2s - loss: 27.0499\n",
      "Epoch 2/10\n",
      "Accuracy: 0.3205168986083499\n",
      " - 1s - loss: 25.7245\n",
      "Epoch 3/10\n",
      "Accuracy: 0.4422266401590457\n",
      " - 1s - loss: 24.0143\n",
      "Epoch 4/10\n",
      "Accuracy: 0.5137574552683897\n",
      " - 1s - loss: 21.4225\n",
      "Epoch 5/10\n",
      "Accuracy: 0.5470775347912525\n",
      " - 1s - loss: 18.7635\n",
      "Epoch 6/10\n",
      "Accuracy: 0.5760238568588469\n",
      " - 1s - loss: 16.5152\n",
      "Epoch 7/10\n",
      "Accuracy: 0.5933598409542744\n",
      " - 1s - loss: 14.7772\n",
      "Epoch 8/10\n",
      "Accuracy: 0.6191252485089463\n",
      " - 1s - loss: 13.3808\n",
      "Epoch 9/10\n",
      "Accuracy: 0.691610337972167\n",
      " - 1s - loss: 12.1142\n",
      "Epoch 10/10\n",
      "Accuracy: 0.7186083499005964\n",
      " - 1s - loss: 11.0328\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "hidden = TimeDistributed(Dense(100, activation=\"softmax\"))(embeddings)\n",
    "outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[EvaluateTags()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding context with convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 159)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 159, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 159, 100)          90100     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 159, 17)           1717      \n",
      "=================================================================\n",
      "Total params: 15,092,417\n",
      "Trainable params: 91,817\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "Accuracy: 0.8067196819085487\n",
      " - 3s - loss: 1.5569\n",
      "Epoch 2/10\n",
      "Accuracy: 0.875427435387674\n",
      " - 1s - loss: 0.5405\n",
      "Epoch 3/10\n",
      "Accuracy: 0.894831013916501\n",
      " - 1s - loss: 0.3591\n",
      "Epoch 4/10\n",
      "Accuracy: 0.905248508946322\n",
      " - 1s - loss: 0.2945\n",
      "Epoch 5/10\n",
      "Accuracy: 0.9093041749502982\n",
      " - 1s - loss: 0.2624\n",
      "Epoch 6/10\n",
      "Accuracy: 0.9130019880715706\n",
      " - 1s - loss: 0.2429\n",
      "Epoch 7/10\n",
      "Accuracy: 0.9147514910536779\n",
      " - 1s - loss: 0.2297\n",
      "Epoch 8/10\n",
      "Accuracy: 0.9170178926441351\n",
      " - 2s - loss: 0.2197\n",
      "Epoch 9/10\n",
      "Accuracy: 0.9164214711729622\n",
      " - 1s - loss: 0.2120\n",
      "Epoch 10/10\n",
      "Accuracy: 0.9182107355864811\n",
      " - 1s - loss: 0.2057\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "cnn = Conv1D(100,3, activation='relu', padding='same')(embeddings)\n",
    "outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(cnn)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[EvaluateTags()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "\n",
    "If you want to see the training progress, use verbose=1 instead of verbose=2.\n",
    "\n",
    "Warning: training this on on class room computers on the full data set will take for a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 159)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 159, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 159, 200)          320800    \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 159, 17)           3417      \n",
      "=================================================================\n",
      "Total params: 15,324,817\n",
      "Trainable params: 324,217\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "Accuracy: 0.7654075546719682\n",
      " - 253s - loss: 18.2768\n",
      "Epoch 2/10\n",
      "Accuracy: 0.8652882703777336\n",
      " - 240s - loss: 5.8403\n",
      "Epoch 3/10\n",
      "Accuracy: 0.8924055666003976\n",
      " - 242s - loss: 3.6612\n",
      "Epoch 4/10\n",
      "Accuracy: 0.9033001988071571\n",
      " - 244s - loss: 2.9337\n",
      "Epoch 5/10\n",
      "Accuracy: 0.910258449304175\n",
      " - 243s - loss: 2.5724\n",
      "Epoch 6/10\n",
      "Accuracy: 0.9155864811133201\n",
      " - 246s - loss: 2.3429\n",
      "Epoch 7/10\n",
      "Accuracy: 0.9165009940357853\n",
      " - 269s - loss: 2.1785\n",
      "Epoch 8/10\n",
      "Accuracy: 0.9180914512922466\n",
      " - 264s - loss: 2.0448\n",
      "Epoch 9/10\n",
      "Accuracy: 0.9201988071570577\n",
      " - 260s - loss: 1.9224\n",
      "Epoch 10/10\n",
      "Accuracy: 0.9247713717693837\n",
      " - 273s - loss: 1.8349\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "rnn = Bidirectional(LSTM(100, activation='tanh', return_sequences=True))(embeddings)\n",
    "outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(rnn)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[EvaluateTags()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skeleton for tagging a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence> What a great sentence this is !\n",
      "What PRON\n",
      "a DET\n",
      "great ADJ\n",
      "sentence NOUN\n",
      "this PRON\n",
      "is AUX\n",
      "! PUNCT\n",
      "sentence> Another one could be even better , but it has too many words .\n",
      "Another DET\n",
      "one NOUN\n",
      "could AUX\n",
      "be AUX\n",
      "even ADV\n",
      "better ADJ\n",
      ", PUNCT\n",
      "but CCONJ\n",
      "it PRON\n",
      "has VERB\n",
      "too ADV\n",
      "many ADJ\n",
      "words NOUN\n",
      ". PUNCT\n",
      "sentence> end\n"
     ]
    }
   ],
   "source": [
    "inverse_label_map = {value: key for key, value in label_map.items()}\n",
    "\n",
    "def tag_sentence(sentence):\n",
    "    tokens = sentence.split() # Stupid whitespace tokenization\n",
    "    vectorized_sentence, _, sentence_length=vectorizer(vocabulary, [tokens], label_map) # Using our global variables again...\n",
    "    vectorized_sentence_padded = pad_sequences(vectorized_sentence, padding='post', maxlen=max(lengths)) # Pad the sequence\n",
    "\n",
    "    predictions = model.predict(vectorized_sentence_padded)[0] # Everything so far has been a 'list' of sentences with a single sentence, so we only take index 0\n",
    "    predictions = numpy.argmax(predictions, axis=-1) # Take the tag index with the highest value for each token\n",
    "    \n",
    "    tags = [inverse_label_map[label_index] for label_index in predictions[:len(tokens)]] # Ignore padded region\n",
    "    return tags, tokens\n",
    "    \n",
    "while True:\n",
    "    sentence=input(\"sentence> \")\n",
    "    if sentence==\"end\":\n",
    "        break\n",
    "    tags, tokens = tag_sentence(sentence)\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        print(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
